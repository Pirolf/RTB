{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing a metric\n",
    "\n",
    "I first looked at class ditribution. There are two classes. The dataset is highly imbalanced because the positive class makes up only ~0.19% of all labels.\n",
    "\n",
    "I suspected if I just run a simple logistic regression on the dataset, I could achieve 99.81% accuracy, because the model can simply cheat by classifying every sample as 0. I ran logistic regression using sklearn with default params, and the result confirmed my hypothesis - the model predicted every sample as 0.\n",
    "\n",
    "Accuracy is a bad metric for an imbalanced dataset. If the model predicts the majority label for every sample, precision will be perfect but recall will be 0. If the model predicts the minority label for every sample, the recall will be perfect but precision will be 1. One potential metric I can use is the f1 score, but it assumes that precision and recall are equally important, which in reality may not be true. If capturing most of the click-throughs are more important than serving efficiency (most of the served impressions result in click-throughs, in other words, there is no infrastructure bottlenck), then recall is more important than precision. On the other hand, if serving efficiency is more important than capturing most of click-throughs, which may be true when the scale is large, then precision is more important.\n",
    "\n",
    "Without making too much an assumption, ROC AUC is a better metric to use because it shows the tradeoff between specificity and senstivity. A large ROC AUC value indicates the model is more robust, and the optimal value should approach 1.\n",
    "\n",
    "### Baseline models\n",
    "\n",
    "#### Logistic Regression\n",
    "I reran the default logistic regression model (no penalty, all params left as default set by sklearn) and printed ROC AUC, f1 score and the confusion matrix. I used the binary labels as predicted values rather than scores, because I didn't want ROC AUC to be skewed by precision, and I care more about the repdiced labels than probablities (although this is likely not the best idea in real life, if probablities are used for calculating the cost per click).\n",
    "```\n",
    "================================\n",
    "Confusion Matrix:\n",
    "True Negative = 199623\n",
    "False Negative = 377\n",
    "True Positive = 0\n",
    "False Positive = 0\n",
    "================================\n",
    "f1 score = 0.000\n",
    "================================\n",
    "ROC AUC Score = 0.500\n",
    "```\n",
    "F1 score is forced to 0 because there are no predicted positives at all. ROC AUC score is low - only as good as a random classifier. Not expecting much to change, I added l2 regularization `penalty='l2'`, the result is the same. The imbalance is to0 strong for the model to try to predict positives.\n",
    "\n",
    "#### Neural Net\n",
    "The baseline neural net has 1 hidden layer that outputs 88 weights for all features, and an output layer with 2 outputs, one probability for each class, with a softmax activation. Categorical crossentropy is used as loss.\n",
    "\n",
    "The model was fit with 3 epochs, and a batch size of 32. The baseline neural net should be equivalent of a logisc regression, except that I used the SGD optimizer. I picked a learning rate of 0.01 (it usually worked well for me). I set the inital weights as `Ones`, because Keras defaults the inital weights to `glorot_uniform`, which generally works really well, and too well for a baseline model).\n",
    "\n",
    "I used set split to 0.2 for validation after each epoch.\n",
    "Both training and validation error went down, and validation error was slightly smaller than training error in all epochs, indicating no overfitting. \n",
    "\n",
    "Unsurprisingly, the accuracy was still 99.81%, with the vast majority of samples predicted negative. ROC AUC score was still 0.5.\n",
    "\n",
    "The result:\n",
    "```\n",
    "================================\n",
    "Confusion Matrix:\n",
    "True Negative = 199619\n",
    "False Negative = 377\n",
    "True Positive = 0\n",
    "False Positive = 4\n",
    "================================\n",
    "f1 score = 0.000\n",
    "================================\n",
    "ROC AUC Score = 0.500\n",
    "```\n",
    "\n",
    "Because I set the initial weights to `Ones`, I wanted to eliminate the possibility that my weights were either too big or too samll. I checked the hidden layer's output weights, they were all between -1 and 1. I removed the initial weights so that they were set to `glorot_uniform`, the result was similar, there was one true positive and more false positives\n",
    "```\n",
    "================================\n",
    "Confusion Matrix:\n",
    "True Negative = 199418\n",
    "False Negative = 376\n",
    "True Positive = 1\n",
    "False Positive = 205\n",
    "================================\n",
    "f1 score = 0.003\n",
    "================================\n",
    "ROC AUC Score = 0.501\n",
    "```\n",
    "\n",
    "### Model improvements\n",
    "I thought the main obstacle to achieving higher ROC AUC is the severe data imbalance. I could try\n",
    "\n",
    "- Deepen the neural network model\n",
    "  <p>\n",
    "    If add more layers, the model will have a chance to explore nonlinearty, which might cover the positive space.\n",
    "  </p>\n",
    "- Down sampling\n",
    "  <p>\n",
    "    The negative class can be overpowering. I can find a good class ratio so that the model can be trained for positive classes without underfitting the negative class. A side benefit is the model will take less time to train.\n",
    "  </p>\n",
    "- Over sampling\n",
    "  <p>\n",
    "    Like down sampling, the goal is to make the data more balanced. If there is underfitting for the negative class, I can try oversampling the positive class and find a good class ratio. Oversampling can be done with random selection and duplication. This can cause overfitting for the positive class, and slow down training and can increase model size.\n",
    "  </p>\n",
    "- SMOTE\n",
    "  <p>\n",
    "    I can also try a hybrid approach, down sample negative class while generating synthetic samples for the positive class eith nearest neighbours. This could help prevent positive class overfitting. But just like oversampling, it causes the model to train slower and the size to increase.\n",
    "  </p>\n",
    "  \n",
    "  \n",
    "#### Deepen the neural network model\n",
    "I added one more hidden layer with 64 outputs, without any regularization, and left the initial weights to `glorot_uniform`. Both training and validation lossses are smaller (around 0.015 v.s. 0.030 in the baseline neural net). However, there is no improvement in metrics. The vast majority of predicted clases are still negative, and ROC AUC remains 0.5. The validation error also failed to decrease while training error did. To reduce overfitting, so I added `l2(0.01)` to the first hidden layer. The validation error decreased with training error, but there was no improvement in metrics. In fact, the model predicted all negatives again. It was likely the extra layer only allowed the model to explore some nonlinearity and as a result the error declined, but it heavily benefited the negative samples, which severly outweighed the positive class.\n",
    "\n",
    "I tried to tune learning rate (0.1, 0.001, 0.0001) and batch size (64, 128, 256) on different scales and different weight initializers (random normal, he_normal, etc). There was no improvement. Adding another layer didn't help either.\n",
    "\n",
    "Since the imbalance was preventing the model from predicting positives, I used weighted loss to balance the classes by setting `class_weight` (roughly 1:526) in the `fit` function. The losses were large, and validation error plateaued, and the model again tried to predict all negatives. I probably overshot the class weight, so I adjusted to 1:100, and added l2(0.01). There were more false positives, but there was still overfitting, so I changed l2 to l1 in the first layer, in an attempt to reduce features. This helped. I have more true positives, but also more false positives, but the ROC AUC reached 0.541\n",
    "\n",
    "```\n",
    "================================\n",
    "Confusion Matrix:\n",
    "True Negative = 190136\n",
    "False Negative = 328\n",
    "True Positive = 49\n",
    "False Positive = 9487\n",
    "================================\n",
    "f1 score = 0.010\n",
    "================================\n",
    "ROC AUC Score = 0.541\n",
    "```\n",
    "\n",
    "There was still a little overfitting, so I added a dropout layer between the two hidden layers at 0.1. However, this pushed the model back to all negatives again. I tried to tune the dropout rate and although it decreases losses when the rate is 0.05, it also prevented the mode from predicting positives, probably because most of the loss it regulated is from negatives. I also tuned the learn rate, decay, and tried out RMSprop. The loss went down but ROC AUC was stagnent.\n",
    "\n",
    "I could keep adding layers and tuning the extra hyper parameters, but I decided to explore sampling techiniques, otherwise the model would be exponentially slower and harder to tune as trained parameters hyper parameters increase.\n",
    "\n",
    "#### Undersampling/Oversampling\n",
    "I used logistic regression to evaluate different sampling methods.\n",
    "\n",
    "Initially I tried just random undersampling with a negative-positive ratio of 10:1. I tried 1:1 but this lead to overfitting due to extremely small positive size. 10:1 resulted in an ROC AUC score of 0.501. I used `class_weight` to counter balance so that to total weight of the two classes were equal (i.e. setting it to 1:10). ROC AUC score improved to 0.663.\n",
    "\n",
    "```\n",
    "================================\n",
    "Confusion Matrix:\n",
    "True Negative = 129499\n",
    "False Negative = 122\n",
    "True Positive = 255\n",
    "False Positive = 70124\n",
    "================================\n",
    "f1 score = 0.007\n",
    "================================\n",
    "ROC AUC Score = 0.663\n",
    "```\n",
    "\n",
    "I tried random oversampling next. However, the sampling process was very slow this time, because I was upsampling the minority class to have an equal size with the majority class. So I put the undersampler before the oversampler, and built a pipeline.\n",
    "\n",
    "```\n",
    "Undersample majority to 10:1 -> Oversample minority to 1:1\n",
    "```\n",
    "\n",
    "Due to lack of positive samples, I worried simple oversampling would cause overfitting, as the classifier would memorize them. I built another pipeline with SMOTE as the second step.\n",
    "\n",
    "```\n",
    "Undersample majority to 10:1 -> SMOTE 1:1\n",
    "```\n",
    "\n",
    "The results:\n",
    "```\n",
    "================================\n",
    "Confusion Matrix:\n",
    "True Negative = 129713\n",
    "False Negative = 123\n",
    "True Positive = 254\n",
    "False Positive = 69910\n",
    "================================\n",
    "f1 score = 0.007\n",
    "================================\n",
    "ROC AUC Score = 0.662\n",
    "```\n",
    "\n",
    "```\n",
    "================================\n",
    "Confusion Matrix:\n",
    "True Negative = 130028\n",
    "False Negative = 125\n",
    "True Positive = 252\n",
    "False Positive = 69595\n",
    "================================\n",
    "f1 score = 0.007\n",
    "================================\n",
    "ROC AUC Score = 0.660\n",
    "```\n",
    "\n",
    "There was little difference between oversampling vs SMOTE, and having an oversampling step didn't yield better results than only undersampling.\n",
    "\n",
    "Because of undersampling, the overall size was still small. To prove I wasn't lucky, I removed the random seed and repeated the tests above 10 times and the results were similar.\n",
    "\n",
    "I tuned the initial undersampling ratio higher, 20:1, and 40:1, the results were not better, but slightly worse. Maybe  higher undersampling ratio lead to more noisy oversampled data down the pipeline, as the minority class had to be sampled/synthesizes to a large size.\n",
    "\n",
    "After a closer look at the sampling results above, SMOTE actually resulted in fewer false positives and more true negatives. I played around with `k_neighbors` and `m_neighbors`, and was able to reduce the false positives a little  further. The best config is `k_neighbors=1`, `m_neighbors=3`\n",
    "\n",
    "```\n",
    "================================\n",
    "Confusion Matrix:\n",
    "True Negative = 130268\n",
    "False Negative = 125\n",
    "True Positive = 252\n",
    "False Positive = 69355\n",
    "================================\n",
    "f1 score = 0.007\n",
    "================================\n",
    "ROC AUC Score = 0.661\n",
    "```\n",
    "\n",
    "#### Ensembling\n",
    "So far I hadn't spent much time on improving the undersampling. I saw decent improvement in ROC AUC score from 0.5 to 0.66, but false positives were still high. I stumbled upon `BalanceCascade` and wondered if more sophisticated estimators with iterative undersampling could help reduce potential noise in the features and negative samples.\n",
    "\n",
    "I still had to undersample as the first step so that ensembling didn't take forever.\n",
    "\n",
    "__EasyEnsembler__\n",
    "No estimator, just iterative random undersampling\n",
    "Results are in line with random undersampling\n",
    "```\n",
    "================================\n",
    "Confusion Matrix:\n",
    "True Negative = 126066\n",
    "False Negative = 124\n",
    "True Positive = 253\n",
    "False Positive = 73557\n",
    "================================\n",
    "f1 score = 0.007\n",
    "================================\n",
    "ROC AUC Score = 0.651\n",
    "```\n",
    "__BalanceCascade__\n",
    "Without an estimator, it uses KNN\n",
    "```\n",
    "================================\n",
    "Confusion Matrix:\n",
    "True Negative = 115446\n",
    "False Negative = 149\n",
    "True Positive = 228\n",
    "False Positive = 84177\n",
    "================================\n",
    "f1 score = 0.005\n",
    "================================\n",
    "ROC AUC Score = 0.592\n",
    "```\n",
    "\n",
    "__BalanceCascade + DecisionTreeClassifier__\n",
    "This is not bad as I left all params default.\n",
    "```\n",
    "================================\n",
    "Confusion Matrix:\n",
    "True Negative = 128417\n",
    "False Negative = 152\n",
    "True Positive = 225\n",
    "False Positive = 71206\n",
    "================================\n",
    "f1 score = 0.006\n",
    "================================\n",
    "ROC AUC Score = 0.620\n",
    "```\n",
    "Since I didn't limit max subsets. I tried 20, 10, 5.\n",
    "5 had the best result, with ROC AUC = 0.674, where as 20 -> 0.636, 10 -> 0.644.\n",
    "```\n",
    "================================\n",
    "Confusion Matrix:\n",
    "True Negative = 127183\n",
    "False Negative = 109\n",
    "True Positive = 268\n",
    "False Positive = 72440\n",
    "================================\n",
    "f1 score = 0.007\n",
    "================================\n",
    "ROC AUC Score = 0.674\n",
    "```\n",
    "Then I tried to limit `max_features`, with the assumption that maybe there was still too much feature noise. Unfortunately, as the `max_features` ratio went down from 0.8 to 0.2, the ROC AUC also declined.\n",
    "\n",
    "I tried to tune `min_sample_leaves` as well but didn't see improvement.\n",
    "Perhaps it was because I already severly limited max subsets. I increased max subsets to 10, and the best max feature ratio was 0.8. ROC AUC was better than when max subsets was 5, but it still didn't beat 0.674. \n",
    "\n",
    "__BalanceCascade + RandomForestClassifier__\n",
    "I could have tried to tune this. But since the baseline result below wasn't better then decision tree, I just focused on decision tree instead. Random forest sampling also was very time consuming, as there were more full grown trees than one decision tree.\n",
    "```\n",
    "================================\n",
    "Confusion Matrix:\n",
    "True Negative = 132453\n",
    "False Negative = 161\n",
    "True Positive = 216\n",
    "False Positive = 67170\n",
    "================================\n",
    "f1 score = 0.006\n",
    "================================\n",
    "ROC AUC Score = 0.618\n",
    "```\n",
    "\n",
    "__BalanceCascade + XGBClassifier__\n",
    "I was going to try XGB sampling, but random forest didn't perform quite well, and XGB was taking a very long time.\n",
    "\n",
    "With the best sampling config (__BalanceCascade + DecisionTreeClassifier__ with a max subset of 5), I tried to tune logistic regression with different params on log scale: l2/l1, regularization strength (C from 0.001 to 2). Nothing beat l2 with C=1.\n",
    "\n",
    "\n",
    "#### Ensembling with Decision Tree + Deep Neural Net\n",
    "\n",
    "I tried the decision tree ensembler with a deep neural net and hoped that with decent feature selection, the model would be able to explore the nonlinearity. I added a second hidden layer with output size = 32. I had to add an l2 regularizer on both hidden layers to reduce overfitting. However, I didn't seem improvement in ROC AUC.\n",
    "\n",
    "I experimented with `relu` or `tanh` as activation on the first or both layers. The results had similar ROC AUC scores of 0.650. However, compared to linear activation, the true positive rate was much higher, and after I experimentally set the `class_weight` to `{0:1, 1:1.2}` to true positive rate was even higher, although false negative rate also rose, while ROC AUC remains the same.\n",
    "\n",
    "```\n",
    "================================\n",
    "Confusion Matrix:\n",
    "True Negative = 110810\n",
    "False Negative = 96\n",
    "True Positive = 281\n",
    "False Positive = 88813\n",
    "================================\n",
    "f1 score = 0.006\n",
    "================================\n",
    "ROC AUC Score = 0.650\n",
    "```\n",
    "\n",
    "I only slightly tuned the learning rate, and did not set a decay or try different optimizers, because both training and validation errors went down steadily. I also didn't add dropout, since the total training set was already small, and two l2 regularizers were enough to avoid overfitting.\n",
    "\n",
    "I looked at the score breakdown:\n",
    "```\n",
    "Median false positive scores: 0.613\n",
    "Median false negative scores: 0.411\n",
    "Median true positive scores: 0.681\n",
    "Median true negative scores: 0.374\n",
    "```\n",
    "The separation between false positives and true negatives were small: the median score for false positives was 1-0.613 = 0.387 for class 0, while true negative scores had a median of 0.374. But the separation between false negatives and true positives was much better: 1-0.411 = 0.589 v.s. 0.681. As a result there were a lot more true positives than false positives.\n",
    "\n",
    "This led me to believe the key to improving the model was sampling and feature selection.\n",
    "\n",
    "#### XGBoost\n",
    "I stumbled upon XGBoost when researching for sampling and feature selection. It seems to be more 'greedy' than random forest, as it adds one tree at a time instead of growing them in parallel. It's also less prone to overfitting, as trees are shallow compared to the fully grown trees in a random forest. Plus, it's faster to experiment with.\n",
    "\n",
    "The results are amazing. I didn't tune too many params, only reduced `max_depth` from the default 6 to 3 to avoid overfitting. I tuned `eta`, and 0.1 worked well. At first I didn't know there was a class weight param `scale_pos_weight` (same as `class_weight` in sklearn and keras), so the model predicted all negatives. My final params:\n",
    "\n",
    "```\n",
    "params = {'max_depth': 3, 'eta':0.1, 'objective':'binary:logistic',\n",
    "         'nthread': 4, 'eval_metric':'auc', 'scale_pos_weight': 521, 'n_estimators': 10}\n",
    "```\n",
    "\n",
    "I trained for 20 rounds. The result:\n",
    "\n",
    "```\n",
    "================================\n",
    "Confusion Matrix:\n",
    "True Negative = 137863\n",
    "False Negative = 149\n",
    "True Positive = 228\n",
    "False Positive = 61760\n",
    "================================\n",
    "f1 score = 0.007\n",
    "ROC Score = 0.648\n",
    "```\n",
    "\n",
    "Given I didn't do any sampling for feature selection, the result was amazing. The model captured 60.4% of true positives and 31% of negatives lurked in as positives.\n",
    "\n",
    "\n",
    "#### Autoencoder\n",
    "It had been very challenging to increase true positives while reducing false positives. What if I have model that learns the negative class really well, and when exposed to positive class, maybe it can tell it's different than what it knows, and flag it as positive? In other words, can I treat the problem as anomaly detection?\n",
    "\n",
    "```\n",
    "================================\n",
    "Confusion Matrix:\n",
    "True Negative = 184891\n",
    "False Negative = 326\n",
    "True Positive = 51\n",
    "False Positive = 14732\n",
    "f1 = 0.007\n",
    "ROC AUC score 0.531\n",
    "```\n",
    "I tried training with all features and undersampling. Undersampling performs slightly better (above) with fewer false positives, both ROC AUC score remains low. The model was very hesitant to predict positives compared to other approaches above. It's possible that when I trained with all features, the model memorized too much about the negative features, but the model still didn't predict many positives with undersampling. I think it's because of fuzzy separation between positives and negatives, that even if the autoencoder learns the negatives well, it was not sure about the true positives.\n",
    "\n",
    "## Conclusion\n",
    "Decision tree based balance cascade ensembler + logistic regression results in the highest ROC AUC = 0.674.\n",
    "Sampling plays a big role in improving ROC AUC. SMOTE and random undersampling yield similar results, but undersampling is obviously faster. \n",
    "\n",
    "Compared to sampling, the model architecture itself has very little impact. I think it's possible to build a deep neural net without any sampling or feature selection that yields similar or better ROC AUC score. However, given the severe imbalance and more hyper parameters, the model will be extremely hard and slow to train. [tflearn](http://tflearn.org/objectives/) supports a proxy `roc_auc_score` objective that's differentiable. It would be worth a try.\n",
    "\n",
    "XGBoost has very good result (ROC AUC = 0.648) with a relatively simpler process, and it was very fast to train. I would consider XGBoost if I were to build a model for production.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from imblearn.under_sampling import NearMiss, RandomUnderSampler\n",
    "from imblearn.combine import SMOTEENN,SMOTETomek\n",
    "from imblearn.ensemble import BalanceCascade, EasyEnsemble\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import h5py\n",
    "import keras\n",
    "from sklearn.utils import class_weight\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import *\n",
    "from keras.regularizers import *\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, f1_score, precision_recall_fscore_support\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "KFOLD_SEED = 42\n",
    "\n",
    "\n",
    "def shuffle(features, labels):\n",
    "    p = np.random.permutation(len(features))\n",
    "    return features[p], labels[p]\n",
    "\n",
    "\n",
    "def rtb_confusion_matrix(test_labels, y_preds):\n",
    "    m = confusion_matrix(test_labels, y_preds)\n",
    "    \n",
    "    print(\"================================\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(\"True Negative = %d\" % m[0][0])\n",
    "    print(\"False Negative = %d\" % m[1][0])\n",
    "    print(\"True Positive = %d\" % m[1][1])\n",
    "    print(\"False Positive = %d\" % m[0][1])\n",
    "\n",
    "\n",
    "def rtb_f1_score(test_labels, y_preds):\n",
    "    f = f1_score(test_labels, y_preds)\n",
    "    print(\"================================\")\n",
    "    print(\"f1 score = %0.3f\" % f)\n",
    "\n",
    "\n",
    "def print_metrics(true_labels, y_preds, y_scores, is_train=True):\n",
    "    if is_train:\n",
    "        print(\"--------train---------\")\n",
    "    else:\n",
    "        print(\"--------test---------\")\n",
    "    \n",
    "    rtb_confusion_matrix(true_labels, y_preds)\n",
    "    rtb_f1_score(true_labels, y_preds)\n",
    "    print(\"================================\")\n",
    "    print(\"ROC AUC Score = %0.3f\" % roc_auc_score(true_labels, y_scores.argmax(axis=-1)))\n",
    "\n",
    "    \n",
    "def keras_print_metrics(true_labels, y_scores, is_train=True):\n",
    "    y_preds = y_scores.argmax(axis=-1)\n",
    "    \n",
    "    if is_train:\n",
    "        print(\"--------train---------\")\n",
    "    else:\n",
    "        print(\"--------test---------\")\n",
    "    \n",
    "    rtb_confusion_matrix(true_labels, y_preds)\n",
    "    rtb_f1_score(true_labels, y_preds)\n",
    "    print(\"================================\")\n",
    "    print(\"ROC AUC Score = %0.3f\" % roc_auc_score(true_labels, y_preds))\n",
    "\n",
    "    \n",
    "def print_xgb_metrics(test_labels_1d, y_scores, score_to_label_threshold=0.5):\n",
    "    predicted_labels = np.array([])\n",
    "    for s in y_scores:\n",
    "        if s > score_to_label_threshold:\n",
    "            predicted_labels = np.append(predicted_labels, 1)\n",
    "        else:\n",
    "            predicted_labels = np.append(predicted_labels, 0)\n",
    "\n",
    "    print(predicted_labels.shape)\n",
    "    \n",
    "    rtb_confusion_matrix(test_labels_1d, predicted_labels)\n",
    "    rtb_f1_score(test_labels_1d, predicted_labels)\n",
    "    print(\"ROC Score = %0.3f\" % roc_auc_score(test_labels_1d, predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000000, 89)\n"
     ]
    }
   ],
   "source": [
    "input_path = '~/data/biddings.csv'\n",
    "data = pd.read_csv(input_path)\n",
    "print(data.shape)\n",
    "\n",
    "train = data[:800000]\n",
    "test = data[800000:]\n",
    "\n",
    "sample = train.sample(frac=1)\n",
    "features = sample.drop('convert', axis=1).values\n",
    "labels = sample.convert.ravel()\n",
    "categorical_labels = to_categorical(labels, 2)\n",
    "\n",
    "test_features = test.drop('convert', axis=1).values\n",
    "test_labels = test.convert.ravel()\n",
    "categorical_test_labels = to_categorical(test_labels, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear](200000, 2) (200000,)\n",
      "--------test---------\n",
      "================================\n",
      "Confusion Matrix:\n",
      "True Negative = 199623\n",
      "False Negative = 377\n",
      "True Positive = 0\n",
      "False Positive = 0\n",
      "================================\n",
      "f1 score = 0.000\n",
      "================================\n",
      "ROC AUC Score = 0.500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.local/share/virtualenvs/RTB-V2Lvgo6A/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(penalty='l2', random_state=KFOLD_SEED, verbose=2)\n",
    "\n",
    "model = lr.fit(features, labels)\n",
    "predicted_scores = model.predict_proba(test_features)\n",
    "predicted_labels = model.predict(test_features)\n",
    "print(predicted_scores.shape, predicted_labels.shape)\n",
    "\n",
    "print_metrics(test_labels, predicted_labels, predicted_scores, is_train=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_11 (Dense)             (None, 88)                7832      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 2)                 178       \n",
      "=================================================================\n",
      "Total params: 8,010\n",
      "Trainable params: 8,010\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 640000 samples, validate on 160000 samples\n",
      "Epoch 1/3\n",
      "640000/640000 [==============================] - 39s 61us/step - loss: 0.0321 - acc: 0.9964 - val_loss: 0.0300 - val_acc: 0.9970\n",
      "Epoch 2/3\n",
      "640000/640000 [==============================] - 39s 61us/step - loss: 0.0279 - acc: 0.9972 - val_loss: 0.0300 - val_acc: 0.9970\n",
      "Epoch 3/3\n",
      "640000/640000 [==============================] - 40s 62us/step - loss: 0.0279 - acc: 0.9972 - val_loss: 0.0300 - val_acc: 0.9970\n",
      "200000/200000 [==============================] - 4s 19us/step\n",
      "--------train---------\n",
      "================================\n",
      "Confusion Matrix:\n",
      "True Negative = 199418\n",
      "False Negative = 376\n",
      "True Positive = 1\n",
      "False Positive = 205\n",
      "================================\n",
      "f1 score = 0.003\n",
      "================================\n",
      "ROC AUC Score = 0.501\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(88, input_shape=(88,)))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=SGD(lr=0.01),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(features, categorical_labels,\n",
    "                    batch_size=32,\n",
    "                    epochs=3,\n",
    "                    callbacks=[keras.callbacks.EarlyStopping()],\n",
    "                    validation_split=0.2,\n",
    "                    verbose=1)\n",
    "\n",
    "y_scores = model.predict(test_features, verbose=1)\n",
    "\n",
    "keras_print_metrics(test_labels, y_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deeper neural nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_28 (Dense)             (None, 88)                7832      \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 64)                5696      \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 13,658\n",
      "Trainable params: 13,658\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 640000 samples, validate on 160000 samples\n",
      "Epoch 1/3\n",
      "640000/640000 [==============================] - 46s 72us/step - loss: 0.8766 - acc: 0.9896 - val_loss: 0.5066 - val_acc: 0.9982\n",
      "Epoch 2/3\n",
      "640000/640000 [==============================] - 45s 70us/step - loss: 0.5423 - acc: 0.9918 - val_loss: 0.5224 - val_acc: 0.9972\n",
      "200000/200000 [==============================] - 5s 23us/step\n",
      "--------train---------\n",
      "================================\n",
      "Confusion Matrix:\n",
      "True Negative = 199437\n",
      "False Negative = 375\n",
      "True Positive = 2\n",
      "False Positive = 186\n",
      "================================\n",
      "f1 score = 0.007\n",
      "================================\n",
      "ROC AUC Score = 0.502\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(88, kernel_regularizer=l1(0.01), input_shape=(88,)))\n",
    "model.add(Dense(64, kernel_regularizer=l2(0.01)))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=SGD(lr=0.01),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(features, categorical_labels,\n",
    "                    batch_size=32,\n",
    "                    class_weight={0:1, 1:100},\n",
    "                    epochs=3,\n",
    "                    callbacks=[keras.callbacks.EarlyStopping()],\n",
    "                    validation_split=0.2,\n",
    "                    verbose=1)\n",
    "\n",
    "y_scores = model.predict(test_features, verbose=1)\n",
    "\n",
    "keras_print_metrics(test_labels, y_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = model.get_layer(name='dense_10').get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression():\n",
    "    lr = LogisticRegression(penalty='l2', random_state=KFOLD_SEED, verbose=2)\n",
    "    return lr\n",
    "\n",
    "\n",
    "def pipeline_test(pipeline, features, labels):\n",
    "    pipeline.fit(features, labels)\n",
    "    \n",
    "    predicted_scores = pipeline.predict_proba(test_features)\n",
    "    predicted_labels = pipeline.predict(test_features)\n",
    "        \n",
    "    print_metrics(test_labels, predicted_labels, predicted_scores, is_train=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Undersampling / Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]--------test---------\n",
      "================================\n",
      "Confusion Matrix:\n",
      "True Negative = 128714\n",
      "False Negative = 123\n",
      "True Positive = 254\n",
      "False Positive = 70909\n",
      "================================\n",
      "f1 score = 0.007\n",
      "================================\n",
      "ROC AUC Score = 0.659\n",
      "[LibLinear]--------test---------\n",
      "================================\n",
      "Confusion Matrix:\n",
      "True Negative = 129604\n",
      "False Negative = 124\n",
      "True Positive = 253\n",
      "False Positive = 70019\n",
      "================================\n",
      "f1 score = 0.007\n",
      "================================\n",
      "ROC AUC Score = 0.660\n",
      "[LibLinear]--------test---------\n",
      "================================\n",
      "Confusion Matrix:\n",
      "True Negative = 130225\n",
      "False Negative = 121\n",
      "True Positive = 256\n",
      "False Positive = 69398\n",
      "================================\n",
      "f1 score = 0.007\n",
      "================================\n",
      "ROC AUC Score = 0.666\n"
     ]
    }
   ],
   "source": [
    "def sample_pipelines_test(pipeline_test_fn=pipeline_test):\n",
    "    rus = RandomUnderSampler(ratio={0: 1531*10, 1: 1531}, random_state=KFOLD_SEED)\n",
    "    ros = RandomOverSampler(random_state=KFOLD_SEED)\n",
    "    smote = SMOTE(n_jobs=-1, k_neighbors=1, m_neighbors = 3, random_state=KFOLD_SEED)\n",
    "    \n",
    "    lr1 = LogisticRegression(penalty='l2', class_weight={0:1, 1:10}, random_state=KFOLD_SEED, verbose=2)\n",
    "    p1 = Pipeline([('rus', rus), ('lr', lr1)])\n",
    "    pipeline_test_fn(p1, features, labels)\n",
    "    \n",
    "    p2 = Pipeline([('rus', rus), ('ros', ros), ('lr', logistic_regression())])\n",
    "    pipeline_test_fn(p2, features, labels)\n",
    "    \n",
    "    p3 = Pipeline([('rus', rus), ('smote', smote), ('lr', logistic_regression())])\n",
    "    pipeline_test_fn(p3, features, labels)\n",
    "    \n",
    "    \n",
    "\n",
    "sample_pipelines_test(pipeline_test_fn=pipeline_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensembler_test(classifier_fn, ensemblers):\n",
    "    rus = RandomUnderSampler(ratio={0: 1531*10, 1: 1531}, random_state=KFOLD_SEED)\n",
    "    X_us, y_us = rus.fit_sample(features, labels)\n",
    "    \n",
    "    for i, e in enumerate(ensemblers):\n",
    "        print(\"fitting sample\")\n",
    "        X_res, y_res = e.fit_sample(X_us, y_us)\n",
    "        print(X_res.shape, y_res.shape)\n",
    "        clf = classifier_fn()\n",
    "        print(\"training\")\n",
    "        \n",
    "        for j, X_train in enumerate(X_res):\n",
    "            model = clf.fit(X_train, y_res[j])\n",
    "        \n",
    "        predicted_scores = model.predict_proba(test_features)\n",
    "        predicted_labels = model.predict(test_features)\n",
    "        \n",
    "        print(\"Ensembler %d\" % i)\n",
    "        print_metrics(test_labels, predicted_labels, predicted_scores, is_train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EasyEnsemble and decision tree are consistenly the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting sample\n",
      "(10, 3062, 88) (10, 3062)\n",
      "training\n",
      "[LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear]Ensembler 0\n",
      "--------test---------\n",
      "================================\n",
      "Confusion Matrix:\n",
      "True Negative = 126771\n",
      "False Negative = 119\n",
      "True Positive = 258\n",
      "False Positive = 72852\n",
      "================================\n",
      "f1 score = 0.007\n",
      "================================\n",
      "ROC AUC Score = 0.660\n",
      "fitting sample\n",
      "(18, 3062, 88) (18, 3062)\n",
      "training\n",
      "[LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear]Ensembler 1\n",
      "--------test---------\n",
      "================================\n",
      "Confusion Matrix:\n",
      "True Negative = 109272\n",
      "False Negative = 146\n",
      "True Positive = 231\n",
      "False Positive = 90351\n",
      "================================\n",
      "f1 score = 0.005\n",
      "================================\n",
      "ROC AUC Score = 0.580\n",
      "fitting sample\n",
      "(17, 3062, 88) (17, 3062)\n",
      "training\n",
      "[LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear]Ensembler 2\n",
      "--------test---------\n",
      "================================\n",
      "Confusion Matrix:\n",
      "True Negative = 134970\n",
      "False Negative = 152\n",
      "True Positive = 225\n",
      "False Positive = 64653\n",
      "================================\n",
      "f1 score = 0.007\n",
      "================================\n",
      "ROC AUC Score = 0.636\n",
      "fitting sample\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 3062, 88) (15, 3062)\n",
      "training\n",
      "[LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear]Ensembler 3\n",
      "--------test---------\n",
      "================================\n",
      "Confusion Matrix:\n",
      "True Negative = 143210\n",
      "False Negative = 172\n",
      "True Positive = 205\n",
      "False Positive = 56413\n",
      "================================\n",
      "f1 score = 0.007\n",
      "================================\n",
      "ROC AUC Score = 0.631\n"
     ]
    }
   ],
   "source": [
    "ee = EasyEnsemble(random_state=KFOLD_SEED)\n",
    "bc = BalanceCascade(random_state=KFOLD_SEED)\n",
    "\n",
    "dt = DecisionTreeClassifier(class_weight='balanced', random_state=KFOLD_SEED)\n",
    "bc_dt = BalanceCascade(estimator=dt, random_state=KFOLD_SEED)\n",
    "\n",
    "rf = RandomForestClassifier(class_weight='balanced', n_jobs=-1, random_state=KFOLD_SEED, verbose=1)\n",
    "bc_rf = BalanceCascade(estimator=rf, random_state=KFOLD_SEED)\n",
    "\n",
    "# xgbc = XGBClassifier(n_jobs=-1, n_estimators=10, scale_pos_weight=10)\n",
    "# bc_xgbc = BalanceCascade(estimator=xgbc, random_state=KFOLD_SEED)\n",
    "\n",
    "\n",
    "ensemblers = [ee, bc, bc_dt, bc_rf]\n",
    "ensembler_test(logistic_regression, ensemblers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to tune EasyEnsemble by adjusting subsets. It does not affect f1-score or ROC AUC socre\n",
    "```\n",
    "ee = EasyEnsemble(n_subsets = 100, random_state=KFOLD_SEED)\n",
    "ensembler_test(logistic_regression, [ee])\n",
    "\n",
    "ee = EasyEnsemble(n_subsets = 4, random_state=KFOLD_SEED)\n",
    "ensembler_test(logistic_regression, [ee])\n",
    "```\n",
    "Both result in:\n",
    "```\n",
    "================================\n",
    "Confusion Matrix:\n",
    "True Negative = 127470\n",
    "False Negative = 132\n",
    "True Positive = 245\n",
    "False Positive = 72153\n",
    "================================\n",
    "f1 score = 0.007\n",
    "================================\n",
    "ROC AUC Score = 0.644\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tune max subset for BalanceCascade with DecisionTreeClassifier\n",
    "\n",
    "5 is the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting sample\n",
      "(17, 3062, 88) (17, 3062)\n",
      "training\n",
      "[LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear]Ensembler 0\n",
      "--------test---------\n",
      "================================\n",
      "Confusion Matrix:\n",
      "True Negative = 134970\n",
      "False Negative = 152\n",
      "True Positive = 225\n",
      "False Positive = 64653\n",
      "================================\n",
      "f1 score = 0.007\n",
      "================================\n",
      "ROC AUC Score = 0.636\n",
      "fitting sample\n",
      "(10, 3062, 88) (10, 3062)\n",
      "training\n",
      "[LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear]Ensembler 1\n",
      "--------test---------\n",
      "================================\n",
      "Confusion Matrix:\n",
      "True Negative = 130663\n",
      "False Negative = 138\n",
      "True Positive = 239\n",
      "False Positive = 68960\n",
      "================================\n",
      "f1 score = 0.007\n",
      "================================\n",
      "ROC AUC Score = 0.644\n",
      "fitting sample\n",
      "(5, 3062, 88) (5, 3062)\n",
      "training\n",
      "[LibLinear][LibLinear][LibLinear][LibLinear][LibLinear]Ensembler 2\n",
      "--------test---------\n",
      "================================\n",
      "Confusion Matrix:\n",
      "True Negative = 127183\n",
      "False Negative = 109\n",
      "True Positive = 268\n",
      "False Positive = 72440\n",
      "================================\n",
      "f1 score = 0.007\n",
      "================================\n",
      "ROC AUC Score = 0.674\n"
     ]
    }
   ],
   "source": [
    "dt_20 = DecisionTreeClassifier(random_state=KFOLD_SEED)\n",
    "bc_dt_20 = BalanceCascade(estimator=dt_20, n_max_subset=20, random_state=KFOLD_SEED)\n",
    "\n",
    "dt_10 = DecisionTreeClassifier(random_state=KFOLD_SEED)\n",
    "bc_dt_10 = BalanceCascade(estimator=dt_10, n_max_subset=10, random_state=KFOLD_SEED)\n",
    "\n",
    "dt_5 = DecisionTreeClassifier(random_state=KFOLD_SEED)\n",
    "bc_dt_5 = BalanceCascade(estimator=dt_5, n_max_subset=5, random_state=KFOLD_SEED)\n",
    "\n",
    "ensemblers = [bc_dt_20, bc_dt_10, bc_dt_5]\n",
    "ensembler_test(logistic_regression, ensemblers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tune max features, overall it doesn't affect ROC AUC much, but 17 features is slightly higher than others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting sample\n",
      "(5, 3062, 88) (5, 3062)\n",
      "training\n",
      "[LibLinear][LibLinear][LibLinear][LibLinear][LibLinear]Ensembler 0\n",
      "--------test---------\n",
      "================================\n",
      "Confusion Matrix:\n",
      "True Negative = 127258\n",
      "False Negative = 121\n",
      "True Positive = 256\n",
      "False Positive = 72365\n",
      "================================\n",
      "f1 score = 0.007\n",
      "================================\n",
      "ROC AUC Score = 0.658\n",
      "fitting sample\n",
      "(5, 3062, 88) (5, 3062)\n",
      "training\n",
      "[LibLinear][LibLinear][LibLinear][LibLinear][LibLinear]Ensembler 1\n",
      "--------test---------\n",
      "================================\n",
      "Confusion Matrix:\n",
      "True Negative = 128352\n",
      "False Negative = 125\n",
      "True Positive = 252\n",
      "False Positive = 71271\n",
      "================================\n",
      "f1 score = 0.007\n",
      "================================\n",
      "ROC AUC Score = 0.656\n",
      "fitting sample\n",
      "(5, 3062, 88) (5, 3062)\n",
      "training\n",
      "[LibLinear][LibLinear][LibLinear][LibLinear][LibLinear]Ensembler 2\n",
      "--------test---------\n",
      "================================\n",
      "Confusion Matrix:\n",
      "True Negative = 131504\n",
      "False Negative = 141\n",
      "True Positive = 236\n",
      "False Positive = 68119\n",
      "================================\n",
      "f1 score = 0.007\n",
      "================================\n",
      "ROC AUC Score = 0.642\n",
      "fitting sample\n",
      "(5, 3062, 88) (5, 3062)\n",
      "training\n",
      "[LibLinear][LibLinear][LibLinear][LibLinear][LibLinear]Ensembler 3\n",
      "--------test---------\n",
      "================================\n",
      "Confusion Matrix:\n",
      "True Negative = 128496\n",
      "False Negative = 132\n",
      "True Positive = 245\n",
      "False Positive = 71127\n",
      "================================\n",
      "f1 score = 0.007\n",
      "================================\n",
      "ROC AUC Score = 0.647\n"
     ]
    }
   ],
   "source": [
    "# 70 features\n",
    "dt_08 = DecisionTreeClassifier(max_features=0.8, random_state=KFOLD_SEED)\n",
    "bc_dt_08 = BalanceCascade(estimator=dt_08, n_max_subset=5, random_state=KFOLD_SEED)\n",
    "\n",
    "# 35 features\n",
    "dt_04 = DecisionTreeClassifier(max_features=0.4, random_state=KFOLD_SEED)\n",
    "bc_dt_04 = BalanceCascade(estimator=dt_04, n_max_subset=5, random_state=KFOLD_SEED)\n",
    "\n",
    "# 17 features\n",
    "dt_02 = DecisionTreeClassifier(max_features=0.2, random_state=KFOLD_SEED)\n",
    "bc_dt_02 = BalanceCascade(estimator=dt_02, n_max_subset=5, random_state=KFOLD_SEED)\n",
    "\n",
    "# Auto is sqrt(n_features) ~= 9\n",
    "dt_auto = DecisionTreeClassifier(max_features='auto', random_state=KFOLD_SEED)\n",
    "bc_dt_auto = BalanceCascade(estimator=dt_auto, n_max_subset=5, random_state=KFOLD_SEED)\n",
    "\n",
    "ensemblers = [bc_dt_08, bc_dt_04, bc_dt_02, bc_dt_auto]\n",
    "ensembler_test(logistic_regression, ensemblers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No need to tune decision tree's class weight, since ensembler already ensures both classes have equal samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Min samples at leaves do not seem to matter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting sample\n",
      "(5, 3062, 88) (5, 3062)\n",
      "training\n",
      "[LibLinear][LibLinear][LibLinear][LibLinear][LibLinear]Ensembler 0\n",
      "--------test---------\n",
      "================================\n",
      "Confusion Matrix:\n",
      "True Negative = 130197\n",
      "False Negative = 134\n",
      "True Positive = 243\n",
      "False Positive = 69426\n",
      "================================\n",
      "f1 score = 0.007\n",
      "================================\n",
      "ROC AUC Score = 0.648\n",
      "fitting sample\n",
      "(5, 3062, 88) (5, 3062)\n",
      "training\n",
      "[LibLinear][LibLinear][LibLinear][LibLinear][LibLinear]Ensembler 1\n",
      "--------test---------\n",
      "================================\n",
      "Confusion Matrix:\n",
      "True Negative = 129656\n",
      "False Negative = 138\n",
      "True Positive = 239\n",
      "False Positive = 69967\n",
      "================================\n",
      "f1 score = 0.007\n",
      "================================\n",
      "ROC AUC Score = 0.642\n",
      "fitting sample\n",
      "(5, 3062, 88) (5, 3062)\n",
      "training\n",
      "[LibLinear][LibLinear][LibLinear][LibLinear][LibLinear]Ensembler 2\n",
      "--------test---------\n",
      "================================\n",
      "Confusion Matrix:\n",
      "True Negative = 129531\n",
      "False Negative = 140\n",
      "True Positive = 237\n",
      "False Positive = 70092\n",
      "================================\n",
      "f1 score = 0.007\n",
      "================================\n",
      "ROC AUC Score = 0.639\n",
      "fitting sample\n",
      "(5, 3062, 88) (5, 3062)\n",
      "training\n",
      "[LibLinear][LibLinear][LibLinear][LibLinear][LibLinear]Ensembler 3\n",
      "--------test---------\n",
      "================================\n",
      "Confusion Matrix:\n",
      "True Negative = 130548\n",
      "False Negative = 141\n",
      "True Positive = 236\n",
      "False Positive = 69075\n",
      "================================\n",
      "f1 score = 0.007\n",
      "================================\n",
      "ROC AUC Score = 0.640\n"
     ]
    }
   ],
   "source": [
    "dt_min_samples_50 = DecisionTreeClassifier(min_samples_leaf=50, random_state=KFOLD_SEED)\n",
    "bc_dt_min_samples_50 = BalanceCascade(estimator=dt_min_samples_50, n_max_subset=5, random_state=KFOLD_SEED)\n",
    "\n",
    "dt_min_samples_20 = DecisionTreeClassifier(min_samples_leaf=20, random_state=KFOLD_SEED)\n",
    "bc_dt_min_samples_20 = BalanceCascade(estimator=dt_min_samples_20, n_max_subset=5, random_state=KFOLD_SEED)\n",
    "\n",
    "dt_min_samples_10 = DecisionTreeClassifier(min_samples_leaf=10, random_state=KFOLD_SEED)\n",
    "bc_dt_min_samples_10 = BalanceCascade(estimator=dt_min_samples_10, n_max_subset=5, random_state=KFOLD_SEED)\n",
    "\n",
    "dt_min_samples_5 = DecisionTreeClassifier(min_samples_leaf=5, random_state=KFOLD_SEED)\n",
    "bc_dt_min_samples_5 = BalanceCascade(estimator=dt_min_samples_5, n_max_subset=5, random_state=KFOLD_SEED)\n",
    "\n",
    "ensemblers = [bc_dt_min_samples_50, bc_dt_min_samples_20, bc_dt_min_samples_10, bc_dt_min_samples_5]\n",
    "ensembler_test(logistic_regression, ensemblers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class weight has to be balanced!\n",
    "If negative class is heavier, both TPR and FNR decrease, but TPR decrease causes more harm to ROC AUC.\n",
    "If positive class is heavier, the observation is the opposite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting sample\n",
      "(5, 3062, 88) (5, 3062)\n",
      "training\n",
      "Ensembler 0\n",
      "--------test---------\n",
      "================================\n",
      "Confusion Matrix:\n",
      "True Negative = 181161\n",
      "False Negative = 271\n",
      "True Positive = 106\n",
      "False Positive = 18462\n",
      "================================\n",
      "f1 score = 0.011\n",
      "================================\n",
      "ROC AUC Score = 0.594\n",
      "fitting sample\n",
      "(5, 3062, 88) (5, 3062)\n",
      "training\n",
      "Ensembler 0\n",
      "--------test---------\n",
      "================================\n",
      "Confusion Matrix:\n",
      "True Negative = 127183\n",
      "False Negative = 109\n",
      "True Positive = 268\n",
      "False Positive = 72440\n",
      "================================\n",
      "f1 score = 0.007\n",
      "================================\n",
      "ROC AUC Score = 0.674\n",
      "fitting sample\n",
      "(5, 3062, 88) (5, 3062)\n",
      "training\n",
      "Ensembler 0\n",
      "--------test---------\n",
      "================================\n",
      "Confusion Matrix:\n",
      "True Negative = 50431\n",
      "False Negative = 30\n",
      "True Positive = 347\n",
      "False Positive = 149192\n",
      "================================\n",
      "f1 score = 0.005\n",
      "================================\n",
      "ROC AUC Score = 0.587\n",
      "fitting sample\n",
      "(5, 3062, 88) (5, 3062)\n",
      "training\n",
      "Ensembler 0\n",
      "--------test---------\n",
      "================================\n",
      "Confusion Matrix:\n",
      "True Negative = 14369\n",
      "False Negative = 11\n",
      "True Positive = 366\n",
      "False Positive = 185254\n",
      "================================\n",
      "f1 score = 0.004\n",
      "================================\n",
      "ROC AUC Score = 0.521\n"
     ]
    }
   ],
   "source": [
    "def create_lr_proxy(class_weight='balanced'):\n",
    "    def create_lr():\n",
    "        return LogisticRegression(penalty='l2', class_weight=class_weight)\n",
    "    return create_lr\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state=KFOLD_SEED)\n",
    "bc = BalanceCascade(estimator=dt, n_max_subset=5, random_state=KFOLD_SEED)\n",
    "\n",
    "ensembler_test(create_lr_proxy({0: 2, 1: 1}), [bc])\n",
    "ensembler_test(create_lr_proxy(), [bc])\n",
    "ensembler_test(create_lr_proxy({0: 1, 1: 2}), [bc])\n",
    "ensembler_test(create_lr_proxy({0: 1, 1: 4}), [bc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try L1 regularization, C=1.0 is just right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting sample\n",
      "(5, 3062, 88) (5, 3062)\n",
      "training\n",
      "Ensembler 0\n",
      "--------test---------\n",
      "================================\n",
      "Confusion Matrix:\n",
      "True Negative = 131468\n",
      "False Negative = 140\n",
      "True Positive = 237\n",
      "False Positive = 68155\n",
      "================================\n",
      "f1 score = 0.007\n",
      "================================\n",
      "ROC AUC Score = 0.644\n",
      "fitting sample\n",
      "(5, 3062, 88) (5, 3062)\n",
      "training\n",
      "Ensembler 0\n",
      "--------test---------\n",
      "================================\n",
      "Confusion Matrix:\n",
      "True Negative = 131521\n",
      "False Negative = 140\n",
      "True Positive = 237\n",
      "False Positive = 68102\n",
      "================================\n",
      "f1 score = 0.007\n",
      "================================\n",
      "ROC AUC Score = 0.644\n",
      "fitting sample\n",
      "(5, 3062, 88) (5, 3062)\n",
      "training\n",
      "Ensembler 0\n",
      "--------test---------\n",
      "================================\n",
      "Confusion Matrix:\n",
      "True Negative = 131493\n",
      "False Negative = 140\n",
      "True Positive = 237\n",
      "False Positive = 68130\n",
      "================================\n",
      "f1 score = 0.007\n",
      "================================\n",
      "ROC AUC Score = 0.644\n",
      "fitting sample\n",
      "(5, 3062, 88) (5, 3062)\n",
      "training\n",
      "Ensembler 0\n",
      "--------test---------\n",
      "================================\n",
      "Confusion Matrix:\n",
      "True Negative = 131491\n",
      "False Negative = 144\n",
      "True Positive = 233\n",
      "False Positive = 68132\n",
      "================================\n",
      "f1 score = 0.007\n",
      "================================\n",
      "ROC AUC Score = 0.638\n",
      "fitting sample\n",
      "(5, 3062, 88) (5, 3062)\n",
      "training\n",
      "Ensembler 0\n",
      "--------test---------\n",
      "================================\n",
      "Confusion Matrix:\n",
      "True Negative = 131554\n",
      "False Negative = 145\n",
      "True Positive = 232\n",
      "False Positive = 68069\n",
      "================================\n",
      "f1 score = 0.007\n",
      "================================\n",
      "ROC AUC Score = 0.637\n"
     ]
    }
   ],
   "source": [
    "def create_lr_proxy(C=1.0):\n",
    "    def create_lr():\n",
    "        return LogisticRegression(penalty='l1', C=C, random_state=KFOLD_SEED)\n",
    "    return create_lr\n",
    "\n",
    "dt = DecisionTreeClassifier(max_features=0.2, random_state=KFOLD_SEED)\n",
    "bc = BalanceCascade(estimator=dt, n_max_subset=5, random_state=KFOLD_SEED)\n",
    "\n",
    "ensembler_test(create_lr_proxy(2.0), [bc])\n",
    "ensembler_test(create_lr_proxy(1.0), [bc])\n",
    "ensembler_test(create_lr_proxy(0.8), [bc])\n",
    "ensembler_test(create_lr_proxy(0.5), [bc])\n",
    "ensembler_test(create_lr_proxy(0.2), [bc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensembling + Deep neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(random_state=KFOLD_SEED)\n",
    "bc = BalanceCascade(estimator=dt, n_max_subset=5, random_state=KFOLD_SEED)\n",
    "\n",
    "def deep_ensemble_merged(model_fn, model_fit_fn, ensembler, smote=None):\n",
    "    print(\"fitting sample\")\n",
    "    X_res, y_res = ensembler.fit_sample(features, labels)\n",
    "    print(X_res.shape, y_res.shape)\n",
    "    \n",
    "    model = model_fn()\n",
    "    print(\"training\")\n",
    "\n",
    "    # Merge sample batches\n",
    "    Xs = None\n",
    "    ys = None\n",
    "    for i, X_train in enumerate(X_res):\n",
    "        if Xs is None:\n",
    "            Xs = np.array(X_res[i])\n",
    "            ys = np.array(y_res[i])\n",
    "            print(Xs.shape, ys.shape)\n",
    "        else:\n",
    "            Xs = np.concatenate((Xs, np.array(X_res[i])))\n",
    "            ys = np.concatenate((ys, np.array(y_res[i])))\n",
    "    \n",
    "    print(Xs.shape, ys.shape)\n",
    "    shuffle(Xs, ys)\n",
    "    \n",
    "    # Generate more synthetic samples\n",
    "    if smote is not None:\n",
    "        Xs, ys = smote.fit_sample(Xs, ys)\n",
    "    \n",
    "    shuffle(Xs, ys)\n",
    "    ys = to_categorical(ys, 2)\n",
    "    model = model_fit_fn(model, Xs, ys)\n",
    "\n",
    "    predicted_scores = model.predict(test_features, verbose=1)\n",
    "    print(predicted_scores.shape)\n",
    "    keras_print_metrics(test_labels, predicted_scores, is_train=False)\n",
    "    return model\n",
    "\n",
    "\n",
    "def deep_ensemble(model_fn, model_fit_fn, ensembler, smote=None):\n",
    "    print(\"fitting sample\")\n",
    "    X_res, y_res = ensembler.fit_sample(features, labels)\n",
    "    print(X_res.shape, y_res.shape)\n",
    "    \n",
    "    model = model_fn()\n",
    "    print(\"training\")\n",
    "\n",
    "    for j, X_train in enumerate(X_res):\n",
    "        if smote is not None:\n",
    "            X, y = smote.fit_sample(X_train, y_res[j])\n",
    "            y = to_categorical(y, 2)\n",
    "            model = model_fit_fn(model, X, y)\n",
    "        else:\n",
    "            y = to_categorical(y_res[j], 2)\n",
    "            model = model_fit_fn(model, X_train, y)\n",
    "\n",
    "    predicted_scores = model.predict(test_features, verbose=1)\n",
    "    print(predicted_scores.shape)\n",
    "    keras_print_metrics(test_labels, predicted_scores, is_train=False)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting sample\n",
      "(5, 3062, 88) (5, 3062)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_45 (Dense)             (None, 88)                7832      \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 32)                2848      \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 10,746\n",
      "Trainable params: 10,746\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "training\n",
      "(3062, 88) (3062,)\n",
      "(15310, 88) (15310,)\n",
      "Train on 12248 samples, validate on 3062 samples\n",
      "Epoch 1/20\n",
      "12248/12248 [==============================] - 4s 325us/step - loss: 2.0913 - acc: 0.5732 - val_loss: 2.0168 - val_acc: 0.6081\n",
      "Epoch 2/20\n",
      "12248/12248 [==============================] - 3s 265us/step - loss: 1.9641 - acc: 0.6172 - val_loss: 1.9223 - val_acc: 0.6150\n",
      "Epoch 3/20\n",
      "12248/12248 [==============================] - 3s 265us/step - loss: 1.8773 - acc: 0.6251 - val_loss: 1.8425 - val_acc: 0.6182\n",
      "Epoch 4/20\n",
      "12248/12248 [==============================] - 3s 263us/step - loss: 1.8011 - acc: 0.6297 - val_loss: 1.7708 - val_acc: 0.6231\n",
      "Epoch 5/20\n",
      "12248/12248 [==============================] - 3s 260us/step - loss: 1.7316 - acc: 0.6355 - val_loss: 1.7046 - val_acc: 0.6228\n",
      "Epoch 6/20\n",
      "12248/12248 [==============================] - 3s 252us/step - loss: 1.6674 - acc: 0.6371 - val_loss: 1.6432 - val_acc: 0.6238\n",
      "Epoch 7/20\n",
      "12248/12248 [==============================] - 3s 251us/step - loss: 1.6074 - acc: 0.6385 - val_loss: 1.5857 - val_acc: 0.6267\n",
      "Epoch 8/20\n",
      "12248/12248 [==============================] - 3s 242us/step - loss: 1.5515 - acc: 0.6397 - val_loss: 1.5323 - val_acc: 0.6264\n",
      "Epoch 9/20\n",
      "12248/12248 [==============================] - 3s 243us/step - loss: 1.4994 - acc: 0.6412 - val_loss: 1.4822 - val_acc: 0.6257\n",
      "Epoch 10/20\n",
      "12248/12248 [==============================] - 3s 246us/step - loss: 1.4503 - acc: 0.6400 - val_loss: 1.4348 - val_acc: 0.6287\n",
      "Epoch 11/20\n",
      "12248/12248 [==============================] - 3s 239us/step - loss: 1.4044 - acc: 0.6401 - val_loss: 1.3906 - val_acc: 0.6293\n",
      "Epoch 12/20\n",
      "12248/12248 [==============================] - 3s 261us/step - loss: 1.3615 - acc: 0.6391 - val_loss: 1.3492 - val_acc: 0.6297\n",
      "Epoch 13/20\n",
      "12248/12248 [==============================] - 3s 252us/step - loss: 1.3212 - acc: 0.6420 - val_loss: 1.3104 - val_acc: 0.6303\n",
      "Epoch 14/20\n",
      "12248/12248 [==============================] - 3s 245us/step - loss: 1.2834 - acc: 0.6419 - val_loss: 1.2740 - val_acc: 0.6316\n",
      "Epoch 15/20\n",
      "12248/12248 [==============================] - 3s 249us/step - loss: 1.2478 - acc: 0.6417 - val_loss: 1.2400 - val_acc: 0.6303\n",
      "Epoch 16/20\n",
      "12248/12248 [==============================] - 3s 257us/step - loss: 1.2144 - acc: 0.6436 - val_loss: 1.2082 - val_acc: 0.6316\n",
      "Epoch 17/20\n",
      "12248/12248 [==============================] - 3s 264us/step - loss: 1.1833 - acc: 0.6410 - val_loss: 1.1774 - val_acc: 0.6329\n",
      "Epoch 18/20\n",
      "12248/12248 [==============================] - 3s 239us/step - loss: 1.1540 - acc: 0.6430 - val_loss: 1.1489 - val_acc: 0.6300\n",
      "Epoch 19/20\n",
      "12248/12248 [==============================] - 3s 262us/step - loss: 1.1264 - acc: 0.6430 - val_loss: 1.1221 - val_acc: 0.6323\n",
      "Epoch 20/20\n",
      "12248/12248 [==============================] - 3s 244us/step - loss: 1.1003 - acc: 0.6433 - val_loss: 1.0972 - val_acc: 0.6316\n",
      "200000/200000 [==============================] - 5s 24us/step\n",
      "(200000, 2)\n",
      "--------test---------\n",
      "================================\n",
      "Confusion Matrix:\n",
      "True Negative = 110810\n",
      "False Negative = 96\n",
      "True Positive = 281\n",
      "False Positive = 88813\n",
      "================================\n",
      "f1 score = 0.006\n",
      "================================\n",
      "ROC AUC Score = 0.650\n"
     ]
    }
   ],
   "source": [
    "def base_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(88, activation='tanh', kernel_regularizer=l2(0.01), input_shape=(88,)))\n",
    "    model.add(Dense(32, activation='tanh', kernel_regularizer=l2(0.01)))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=SGD(lr=0.001),\n",
    "                  metrics=['accuracy'])\n",
    "    return model;\n",
    "\n",
    "def model_fit(model, X, y):\n",
    "    model.fit(X, y,\n",
    "              batch_size=8,\n",
    "              epochs=20,\n",
    "              class_weight={0:1, 1:1.2},\n",
    "              validation_split=0.2,\n",
    "              callbacks=[EarlyStopping(patience=1)],\n",
    "              verbose=1)\n",
    "    return model\n",
    "\n",
    "model = deep_ensemble_merged(base_model, model_fit, bc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200000, 2) (200000,)\n"
     ]
    }
   ],
   "source": [
    "predicted_scores = model.predict(test_features)\n",
    "predicted_labels = predicted_scores.argmax(axis=-1)\n",
    "\n",
    "print(predicted_scores.shape, predicted_labels.shape)\n",
    "\n",
    "false_positive_scores = np.array([])\n",
    "false_negative_scores = np.array([])\n",
    "true_positive_scores = np.array([])\n",
    "true_negative_scores = np.array([])\n",
    "\n",
    "for i, s in enumerate(predicted_scores):\n",
    "    # False positive\n",
    "    if predicted_labels[i] == 1 and test_labels[i] == 0:\n",
    "        false_positive_scores = np.append(false_positive_scores, s[1])\n",
    "    # False negative\n",
    "    elif predicted_labels[i] == 0 and test_labels[i] == 1:\n",
    "        false_negative_scores = np.append(false_negative_scores, s[1])\n",
    "    # True positive\n",
    "    elif predicted_labels[i] == 1 and test_labels[i] == 1:\n",
    "        true_positive_scores = np.append(true_positive_scores, s[1])\n",
    "    # True negative\n",
    "    else:\n",
    "        true_negative_scores = np.append(true_negative_scores, s[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median false positive scores: 0.613\n",
      "Median false negative scores: 0.411\n",
      "Median true positive scores: 0.681\n",
      "Median true negative scores: 0.374\n"
     ]
    }
   ],
   "source": [
    "print(\"Median false positive scores: %0.3f\" % np.median(false_positive_scores))\n",
    "print(\"Median false negative scores: %0.3f\" % np.median(false_negative_scores))\n",
    "print(\"Median true positive scores: %0.3f\" % np.median(true_positive_scores))\n",
    "print(\"Median true negative scores: %0.3f\" % np.median(true_negative_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\teval-auc:0.658225\ttrain-auc:0.671267\n",
      "Multiple eval metrics have been passed: 'train-auc' will be used for early stopping.\n",
      "\n",
      "Will train until train-auc hasn't improved in 5 rounds.\n",
      "[1]\teval-auc:0.665471\ttrain-auc:0.676047\n",
      "[2]\teval-auc:0.672502\ttrain-auc:0.685894\n",
      "[3]\teval-auc:0.677156\ttrain-auc:0.688275\n",
      "[4]\teval-auc:0.675394\ttrain-auc:0.689337\n",
      "[5]\teval-auc:0.675938\ttrain-auc:0.6923\n",
      "[6]\teval-auc:0.679037\ttrain-auc:0.694587\n",
      "[7]\teval-auc:0.681365\ttrain-auc:0.697962\n",
      "[8]\teval-auc:0.68358\ttrain-auc:0.701068\n",
      "[9]\teval-auc:0.684133\ttrain-auc:0.702112\n",
      "[10]\teval-auc:0.685305\ttrain-auc:0.70487\n",
      "[11]\teval-auc:0.685707\ttrain-auc:0.707689\n",
      "[12]\teval-auc:0.68768\ttrain-auc:0.710071\n",
      "[13]\teval-auc:0.689337\ttrain-auc:0.712206\n",
      "[14]\teval-auc:0.69123\ttrain-auc:0.714041\n",
      "[15]\teval-auc:0.691438\ttrain-auc:0.714738\n",
      "[16]\teval-auc:0.692282\ttrain-auc:0.716251\n",
      "[17]\teval-auc:0.693602\ttrain-auc:0.718146\n",
      "[18]\teval-auc:0.694596\ttrain-auc:0.719794\n",
      "[19]\teval-auc:0.695377\ttrain-auc:0.721423\n"
     ]
    }
   ],
   "source": [
    "dtrain = xgb.DMatrix(features, labels)\n",
    "dtest = xgb.DMatrix(test_features, test_labels)\n",
    "\n",
    "params = {'max_depth': 3, 'eta':0.1, 'objective':'binary:logistic',\n",
    "         'nthread': 4, 'eval_metric':'auc', 'scale_pos_weight': 521, 'n_estimators': 10}\n",
    "evallist = [(dtest, 'eval'), (dtrain, 'train')]\n",
    "\n",
    "num_round = 20\n",
    "bst = xgb.train(params, dtrain, num_round, evallist, early_stopping_rounds=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200000,)\n",
      "[0.5872149  0.5316627  0.42117563 0.31765813 0.45734128]\n",
      "(200000,)\n",
      "================================\n",
      "Confusion Matrix:\n",
      "True Negative = 137863\n",
      "False Negative = 149\n",
      "True Positive = 228\n",
      "False Positive = 61760\n",
      "================================\n",
      "f1 score = 0.007\n",
      "ROC Score = 0.648\n"
     ]
    }
   ],
   "source": [
    "y_scores = bst.predict(dtest)\n",
    "\n",
    "print(y_scores.shape)\n",
    "print(y_scores[0:5])\n",
    "print_xgb_metrics(test_labels, y_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 88)                0         \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 88)                7832      \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 44)                3916      \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 44)                1980      \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 88)                3960      \n",
      "=================================================================\n",
      "Total params: 17,688\n",
      "Trainable params: 17,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoding_dim = 88\n",
    "input_dim = 88\n",
    "\n",
    "input_layer = Input(shape=(input_dim, ))\n",
    "\n",
    "encoder = Dense(encoding_dim, activation=\"tanh\", kernel_initializer='glorot_uniform',\n",
    "                activity_regularizer=l1(10e-5))(input_layer)\n",
    "encoder = Dense(int(encoding_dim / 2))(encoder)\n",
    "\n",
    "decoder = Dense(int(encoding_dim / 2), activation='tanh')(encoder)\n",
    "decoder = Dense(input_dim)(decoder)\n",
    "\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13472 samples, validate on 3369 samples\n",
      "Epoch 1/5\n",
      "13472/13472 [==============================] - 3s 256us/step - loss: 0.4204 - acc: 0.7316 - val_loss: 0.3262 - val_acc: 0.7183\n",
      "Epoch 2/5\n",
      "13472/13472 [==============================] - 3s 258us/step - loss: 0.4164 - acc: 0.7408 - val_loss: 0.3274 - val_acc: 0.7008\n",
      "Epoch 3/5\n",
      "13472/13472 [==============================] - 4s 271us/step - loss: 0.4139 - acc: 0.7493 - val_loss: 0.3230 - val_acc: 0.7251\n",
      "Epoch 4/5\n",
      "13472/13472 [==============================] - 3s 252us/step - loss: 0.4121 - acc: 0.7495 - val_loss: 0.3229 - val_acc: 0.7198\n",
      "Epoch 5/5\n",
      "13472/13472 [==============================] - 4s 268us/step - loss: 0.4108 - acc: 0.7498 - val_loss: 0.3220 - val_acc: 0.7207\n"
     ]
    }
   ],
   "source": [
    "rus = RandomUnderSampler(ratio={0: 1531*10, 1: 1531}, random_state=KFOLD_SEED)\n",
    "X_res, y_res = rus.fit_sample(features, labels)\n",
    "\n",
    "autoencoder.compile(optimizer='sgd', \n",
    "                    loss='mean_squared_error', \n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath=\"model.h5\",\n",
    "                               verbose=0,\n",
    "                               save_best_only=True)\n",
    "tensorboard = TensorBoard(log_dir='./logs',\n",
    "                          histogram_freq=0,\n",
    "                          write_graph=True,\n",
    "                          write_images=True)\n",
    "\n",
    "history = autoencoder.fit(X_res, X_res,\n",
    "                    epochs=5,\n",
    "                    batch_size=8,\n",
    "                    shuffle=True,\n",
    "                    validation_split=0.2,\n",
    "                    verbose=1,\n",
    "                    callbacks=[checkpointer, tensorboard]).history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reconstruction_error</th>\n",
       "      <th>true_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.419009</td>\n",
       "      <td>0.001885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.869610</td>\n",
       "      <td>0.043376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.031192</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.100455</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.160484</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.252664</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1964.795712</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reconstruction_error     true_class\n",
       "count         200000.000000  200000.000000\n",
       "mean               0.419009       0.001885\n",
       "std                8.869610       0.043376\n",
       "min                0.031192       0.000000\n",
       "25%                0.100455       0.000000\n",
       "50%                0.160484       0.000000\n",
       "75%                0.252664       0.000000\n",
       "max             1964.795712       1.000000"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_scores = autoencoder.predict(test_features)\n",
    "\n",
    "mse = np.mean(np.power(test_features - predicted_scores, 2), axis=1)\n",
    "error_df = pd.DataFrame({'reconstruction_error': mse,\n",
    "                        'true_class': test_labels})\n",
    "error_df.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABJwAAALJCAYAAADxvTmiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzs3Xu4XWV9J/DvLxcJgyASIgqopNSASDToASypilIQrUXbKlpRuThYqtQi1Hppp1KnTjsjBYs43spFLWK9VryNgzeoKNZEMyBFlEDQUIQQ5aaEJuSdP/ZKugm5HMjaCefw+TzPfrLXu9Z612+vs88T8uV931WttQAAAABAX6Zs7QIAAAAAmFwETgAAAAD0SuAEAAAAQK8ETgAAAAD0SuAEAAAAQK8ETgAAAAD0SuAEAExIVXVlVR28tesYlar6o6q6qarurKqZI+j/vKr66+79M6rq6qF9e1XVoqq6o6peX1XbVtXnquq2qvpE37WMUlUtqarf2tp1AMBDzbStXQAAsH5VtSTJLknuSXJnkv+T5MTW2p1bs671qapTk/x6a+0VI+r/vCRLW2t/saattfakUVzrwaCqpic5PcnTW2v/b9TXa639S5K9hpr+LMnXW2vzunpemcF3cWZrbdWo61lXVbUkT2itXbOlrw0APDBGOAHAg9vvtNYenmRekv2SvGUr1/OA1MCk+++OqrrP/7xbX9v97SODcGdGkisfQE193OvHr3Ptxyf50QMJm+7v/QAAJodJ9x9+ADAZtdZ+luTLGQRPSZKq2qaqTquqn3RTr95XVdsO7X9hNy3q9qpaXFWHd+27VtWFVfXzqrqmqo4fOufUqvp4VX24m051ZVWNDe1/U1Xd0O27uqoO6fp9a5KXdtO//l937Deq6h1VdWmSXyX5tXWnN3XX+8eh7d+sqm9V1a1V9dOqOqaqXpPkqCR/1vX/ue7YtX119+JdVfXv3etdVbVNt+/gqlpaVadU1c1VdWNVHbuhe11Vj6iqs7vjbqiqv66qqd2+Y6rq0qo6o6qWJzl1A21Tquovqur67pofrqpHdH3sUVWtql5dVT9J8rV1rj8nyZrpbbdW1de69oOq6rvdtLbvVtVBQ+fc516v53PtV1Xf6352/5RBoLVm38FVtbR7/7Ukz05yVne/L0jyl0M/31d3xx1XVVdV1S+q6stV9fih/lpVva6qfpzkx13b3lV1Ufe9u7qqjhw6/ryqek9VfaGr7ztVtWe375LusP/XXf+lG/i5Hd/Vc0dV/VtVPXU9xxxQVd/uvl83VtVZVfWwbl91P8Oba/A7c0VV7dvte37X5x3dd+JPh/p8QQ1+z27tvrtPHtp3n9+X9dUOAJORwAkAJoCq2j3J85IMTyn62yRzMgihfj3JbhkEA6mqA5J8OMkbk+yY5JlJlnTnfSzJ0iS7Jnlxkv9RVc8Z6veI7pgdk1yY5Kyuz72SnJhk/9ba9kmem2RJa+3/JPkfSf6ptfbw1tpThvp6ZZLXJNk+yfWb+IyPT/KlJO9OMqv7XItaax9Icn6S/9X1/zvrOf3Pkzy9O+cpSQ5I8hdD+x+d5BHdPXp1kvdU1SM3UMp5SVZlcE/3S3JYkv86tP/AJNdmMArpHRtoO6Z7PTuD8Ofh6e7jkGcleWIG93Gt1tqPkqyZLrhja+05VbVTki8kOTPJzAym232h7r220wbvdReq/HOSjyTZKcknkvz++j58a+05Sf4lg+mbD2+t/UHu/fM9u6pemEHI+HsZ/Kz+JckF63T1ou6+7FNV2yW5KMlHkzwqycuS/O+q2mfo+Jcl+askj8zge/6Orp5ndvuf0l3/n9atuapekuTUJK9KskMG3+Hl6/l49yR5Q5Kdk/xGkkOSvLbbd1gGvydzMviuHDnUx9lJ/rD73u+bLiSsqv2SnJPkDzP4ubw/yYU1CEDX+/uynpoAYFISOAHAg9s/V9UdSX6a5OYkb0sGozEyCBfe0Fr7eWvtjgxCgZd15706yTmttYtaa6tbaze01n5YVY9NMj/Jm1prK1pri5L8Qwb/UF/jm621L7bW7skgoFgTIN2TZJsMAoTprbUlrbXFm6j/vNbala21Va21lZs49uVJvtJau6C1trK1tryrbzyOSvL21trNrbVlGQQXrxzav7Lbv7K19sUM1sTaa91OqmqXJM9PclJr7ZettZuTnJH/vK9J8u+ttXd3n+muDbQdleT01tq13Zpbb0nysrr39LJTu2vclU377SQ/bq19pLvGBUl+mGQ4fNvYvX56kulJ3tXdg08m+e44rrshJyT5m9baVd00u/+RZN7wKKdu/8+7z/eCDMLJc7v6vp/kU0leMnT8Z1pr/9r1d36GRvONw3/NIJD8bhu4prV2n4CztbawtXZZV8OSDAKiZ3W7V2YQ1u2dpLrPduPQvn2qaofW2i9aa9/r2l+T5P2tte+01u5prX0oyd0Z3O8H8vsCAJOGwAkAHtxe1I2OODiDfwjv3LXPSvJfkizspvLcmsGi4rO6/Y9Nsr5/3O6aZE1Atcb1GYz8WeNnQ+9/lWRGVU3rFmw+KYORJDdX1ceqatdN1P/TTewftqGax2PX3HtUz/Vd2xrL11l/6FcZjDpa1+MzCGZuHLqv789gVM4a6/tM67atr55pGYyA2lg/G7Juf2v6HP65bay/XZPc0Fpr65z/QD0+yd8P3aOfJ6mN1PP4JAeuOb4756gMRp6tse73bn0/nw0Z13enquZU1eer6mdVdXsGQdnOSdJa+1oGo9Dek8H3+wNVtUN36u9nEEReX1UXV9VvDH2uU9b5XI9NsusD/H0BgElD4AQAE0Br7eIMpnqd1jXdkuSuJE9qre3YvR7RLTCeDP6xv+d6uvr3JDtV1fZDbY9LcsM46/hoa+03M/iHdkvyP9fs2tAp62z/MoOgbI3hwGFDNW+s/zX+vatpjcd1bffXTzMYobLz0H3dYZ0n4q2vlnXb1lfPqiQ3baKfDVm3vzV9Dv/cNtbfjUl260bGDZ//QP00gylmOw69tm2tfWsD9fw0ycXrHP/w1tofbUYN69azoe/OsPdmMDLsCa21HTKYFrj2nrTWzmytPS3JPhlMrXtj1/7d1toLMwge/znJx4eu+451Ptd/6Uagbez3BQAmPYETAEwc70pyaFU9pbW2OskHk5xRVY9KkqrararWrAd0dpJja7Co95Ru396ttZ8m+VaSv6mqGd0Cx69O8o/rud69VNVeVfWcGizGvSKDwGt1t/umJHvUpp+OtiiDqWXTa7AY+YuH9p2f5Leq6siqmlZVM6tqzbSqm7KehbCHXJDkL6pqVlXtnMFaVpv8TOvqplD93yR/V1U7dPduz6p61qbOXU89b6iq2VX18PznGkj3+ylvnS8mmVNVL+/uzUszCEU+P87zv51B4PX67t7/XgbrXD1Q70vylqp6UrJ2ofWXbOT4z3f1v7K7/vSq2r+qnjjO623q5/8PSf60qp7WLf796+tM71tj+yS3J7mzqvZOsjbw6uo5sKqmZxCMrkiyuqoeVlVHVdUjuqmKt+c/v/cfTHJCd15V1XZV9dtVtf0mfl8AYNITOAHABNGtTfThdAuDJ3lTBosrX9ZND/pKunWJWmv/muTYDNYfui3JxfnPETJ/kGSPDEbNfCbJ21prXxlHCdtksFD5LRlMf3pUBmsTJYNFqJNkeVV9bz3nrvHfMhiJ8osM1ln66NDn+0kG05ZOyWCK1qL85/pRZ2ewFs6tVfXP6+n3r5MsSHJ5kiuSfK9reyBeleRhSf6tq/OTSR5zP/s4J4P1ry5Jcl0GgcMfP8B60lpbnsE6SKdksJD1nyV5QWvtlnGe/x8ZLPB9TAb39qVJPr0Z9Xwmg9E6H+u+ez/IYFH7DR1/RwaLcr8sg+/dz7rztxnnJU9N8qHu53/kujtba5/IYJHxjya5I4NRSDutp58/zWCtsDsyCIuGFyDfoWv7RQbTDZcneWe375VJlnSf9YQMpgOmtbYgyfEZTMX7RQa/j8d052zs9wUAJr2691R+AAAAANg8RjgBAAAA0CuBEwAAAAC9EjgBAAAA0CuBEwAAAAC9mra1CxiVnXfeue2xxx5buwwAAACASWPhwoW3tNZmbeq4SRs47bHHHlmwYMHWLgMAAABg0qiq68dznCl1AAAAAPRK4AQAAABArwROAAAAAPRq0q7hBAAAADw0rFy5MkuXLs2KFSu2dimTxowZM7L77rtn+vTpD+h8gRMAAAAwoS1dujTbb7999thjj1TV1i5nwmutZfny5Vm6dGlmz579gPowpQ4AAACY0FasWJGZM2cKm3pSVZk5c+ZmjRgTOAEAAAATnrCpX5t7PwVOAAAAAPRK4AQAAACwmaoqp5xyytrt0047LaeeeuoWreGYY47JJz/5yS16zQ0ROAEAAABspm222Saf/vSnc8sttzyg81etWtVzRVuXp9QBAAAADymr7lmd0y/6Ub61eHkO2nNmTj50TqZN3bwxOdOmTctrXvOanHHGGXnHO95xr31LlizJcccdl1tuuSWzZs3Kueeem8c97nE55phjMmPGjHz/+9/P/Pnzs8MOO+S6667Ltddem5/85Cc544wzctlll+VLX/pSdtttt3zuc5/L9OnT8/a3vz2f+9znctddd+Wggw7K+9///gfdGlZGOAEAAAAPKadf9KOcc+l1WfTTW3POpdfljIt+1Eu/r3vd63L++efntttuu1f7H//xH+foo4/O5ZdfnqOOOiqvf/3r1+5bunRpvvWtb+X0009PkixevDhf+9rXcuGFF+YVr3hFnv3sZ+eKK67Itttumy984QtJkhNPPDHf/e5384Mf/CB33XVXPv/5z/dSf58ETgAAAMBDyrcWL8+KlauTJCtWrs6li5f30u8OO+yQV73qVTnzzDPv1f7tb387L3/5y5Mkr3zlK/PNb35z7b6XvOQlmTp16trt5z3veZk+fXrmzp2be+65J4cffniSZO7cuVmyZEmS5Otf/3oOPPDAzJ07N1/72tdy5ZVX9lJ/nwROAAAAwEPKQXvOzIzpg0hkxvQpmb/nzN76Pumkk3L22Wfnl7/85biO32677e61vc022yRJpkyZkunTp6+dKjdlypSsWrUqK1asyGtf+9p88pOfzBVXXJHjjz8+K1as6K3+vowscKqqx1bV16vq36rqyqr6k659p6q6qKp+3P35yK69qurMqrqmqi6vqqcO9XV0d/yPq+roUdUMAAAATH4nHzonr54/O/Meu2NePX923nDonN763mmnnXLkkUfm7LPPXtt20EEH5WMf+1iS5Pzzz88znvGMB9z/mnBp5513zp133vmgeSrduka5aPiqJKe01r5XVdsnWVhVFyU5JslXW2t/W1VvTvLmJG9K8rwkT+heByZ5b5IDq2qnJG9LMpakdf1c2Fr7xQhrBwAAACapaVOn5I2H7503jqj/U045JWedddba7Xe/+9059thj8853vnPtouEP1I477pjjjz8+++67bx796Edn//3376Pk3lVrbctcqOqzSc7qXge31m6sqsck+UZrba+qen/3/oLu+KuTHLzm1Vr7w679XsdtyNjYWFuwYMHIPg8AAADw4HDVVVfliU984tYuY9JZ332tqoWttbFNnbtF1nCqqj2S7JfkO0l2aa3d2O36WZJduve7Jfnp0GlLu7YNtQMAAADwIDTywKmqHp7kU0lOaq3dPryvDYZX9TbEqqpeU1ULqmrBsmXL+uoWAAAAgPthpIFTVU3PIGw6v7X26a75pm4qXbo/b+7ab0jy2KHTd+/aNtR+H621D7TWxlprY7NmzervgwAAAAAwbqN8Sl0lOTvJVa2104d2XZhkzZPmjk7y2aH2V3VPq3t6ktu6qXdfTnJYVT2ye6LdYV0bAAAAAA9Co3xK3fwkr0xyRVUt6tremuRvk3y8ql6d5PokR3b7vpjk+UmuSfKrJMcmSWvt51X135N8tzvu7a21n4+wbgAAAAA2w8gCp9baN5PUBnYfsp7jW5LXbaCvc5Kc0191AAAAAIzKFnlKHQAAAMBkNnXq1MybN2/ta8mSJb1fY8mSJdl3331773cURjmlDgAAAOAhYdttt82iRYs2uH/VqlWZNu2hE8MY4QQAAAA8tNyzKvnKXyUfPGTw5z2rRnKZ8847L0cccUSe85zn5JBDDsmdd96ZQw45JE996lMzd+7cfPazg+eorTty6bTTTsupp56aJFm4cGGe8pSn5ClPeUre8573jKTOUXjoRGsAMMmsumd1Tr/oR/nW4uU5aM+ZOfnQOZk2dfT/L2lrXRcA6M9D/u/zr78j+c57k5V3JTddmaSS3/rLzeryrrvuyrx585Iks2fPzmc+85kkyfe+971cfvnl2WmnnbJq1ap85jOfyQ477JBbbrklT3/603PEEUdstN9jjz02Z511Vp75zGfmjW9842bVuCUJnABggjr9oh/lnEuvy4qVq/PDn92eSvLGw/eetNcFAPrzkP/7/LpLBmFTkqy6K7nu4s3uckNT6g499NDstNNOSZLWWt761rfmkksuyZQpU3LDDTfkpptu2mCft956a2699dY885nPTJK88pWvzJe+9KXNrnVLeAjFlwAwuXxr8fKsWLk6SbJi5epcunj5pL4uANCfh/zf57OfmUzbdvB+2rbJ7GeN7FLbbbfd2vfnn39+li1bloULF2bRokXZZZddsmLFikybNi2rV69ee9yKFStGVs+WInACgAnqoD1nZsb0wV/lM6ZPyfw9Z07q6wIA/XnI/33+7D9Pnv7aZLexwZ/PfusWuextt92WRz3qUZk+fXq+/vWv5/rrr0+S7LLLLrn55puzfPny3H333fn85z+fJNlxxx2z44475pvf/GaSQWA1UZhSBwAT1MmHzkkluXTx8szfc2becOicSX1dAKA/D/m/z6dO2+w1mx6Io446Kr/zO7+TuXPnZmxsLHvvPZjGOH369PzlX/5lDjjggOy2225r25Pk3HPPzXHHHZeqymGHHbbFa36gqrW2tWsYibGxsbZgwYKtXQYAAAAwYldddVWe+MQnbu0yJp313deqWthaG9vUuabUAQAAANArgRMAAAAAvRI4AQAAANArgRMAAAAAvRI4AQAAANArgRMAAAAAvRI4AQAAAGyG5cuXZ968eZk3b14e/ehHZ7fddsu8efOy4447Zp999un9et/4xjfyghe84H6dc/DBB2fBggX3aT/vvPNy4okn9lXaWgInAAAAgM0wc+bMLFq0KIsWLcoJJ5yQN7zhDWu3p0zZdPSyatWqLVDlliVwAgAAABiRe+65J8cff3ye9KQn5bDDDstdd92VZDDi6KSTTsrY2Fj+/u//PsuWLcvv//7vZ//998/++++fSy+9NEly8cUXrx09td9+++WOO+5Iktx555158YtfnL333jtHHXVUWmtJkq9+9avZb7/9Mnfu3Bx33HG5++6771PTueeemzlz5uSAAw5Ye52+TRtJrwAAAABby8EH99vfN77xgE/98Y9/nAsuuCAf/OAHc+SRR+ZTn/pUXvGKVyRJ/uM//mPtNLeXv/zlecMb3pDf/M3fzE9+8pM897nPzVVXXZXTTjst73nPezJ//vzceeedmTFjRpLk+9//fq688srsuuuumT9/fi699NKMjY3lmGOOyVe/+tXMmTMnr3rVq/Le9743J5100tp6brzxxrztbW/LwoUL84hHPCLPfvazs99++z3we7MBRjgBAAAAjMjs2bMzb968JMnTnva0LFmyZO2+l770pWvff+UrX8mJJ56YefPm5Ygjjsjtt9+eO++8M/Pnz8/JJ5+cM888M7feemumTRuMHTrggAOy++67Z8qUKZk3b16WLFmSq6++OrNnz86cOXOSJEcffXQuueSSe9Xzne98JwcffHBmzZqVhz3sYfeqoU9GOAEAAACTy2aMSOrbNttss/b91KlT106pS5Lttttu7fvVq1fnsssuWzuCaY03v/nN+e3f/u188YtfzPz58/PlL395vf0+2NaBMsIJAAAAYCs77LDD8u53v3vt9qJFi5Ikixcvzty5c/OmN70p+++/f374wx9usI+99torS5YsyTXXXJMk+chHPpJnPetZ9zrmwAMPzMUXX5zly5dn5cqV+cQnPjGCTyNwAgAAANjqzjzzzCxYsCBPfvKTs88+++R973tfkuRd73pX9t133zz5yU/O9OnT87znPW+DfcyYMSPnnntuXvKSl2Tu3LmZMmVKTjjhhHsd85jHPCannnpqfuM3fiPz58/PE5/4xJF8nlqzivlkMzY21tYsvAUAAABMXlddddXIgpOHsvXd16pa2Fob29S5RjgBAAAA0CuBEwAAAAC9EjgBAAAAE95kXTJoa9nc+ylwAgAAACa0GTNmZPny5UKnnrTWsnz58syYMeMB9zGtx3oAAAAAtrjdd989S5cuzbJly7Z2KZPGjBkzsvvuuz/g8wVOAAAAwIQ2ffr0zJ49e2uXwRBT6gAAAADolcAJAAAAgF4JnAAAAADolcAJAAAAgF4JnAAAAADolcAJAAAAgF4JnAAAAADolcAJAAAAgF4JnAAAAADolcAJAAAAgF4JnAAAAADolcAJAAAAgF4JnAAAAADolcAJAAAAgF4JnAAAAADolcAJAAAAgF4JnAAAAADolcAJAAAAgF4JnAAAAADolcAJAAAAgF4JnAAAAADolcAJAAAAgF4JnAAAAADolcAJAAAAgF4JnAAAAADolcAJAAAAgF4JnAAAAADolcAJAAAAgF4JnAAAAADolcAJAAAAgF4JnAAAAADolcAJAAAAgF4JnAAAAADolcAJAAAAgF4JnAAAAADolcAJAAAAgF4JnAAAAADo1cgCp6o6p6purqofDLX9U1Ut6l5LqmpR175HVd01tO99Q+c8raquqKprqurMqqpR1QwAAADA5ps2wr7PS3JWkg+vaWitvXTN+6r6uyS3DR2/uLU2bz39vDfJ8Um+k+SLSQ5P8qUR1AsAAABAD0Y2wqm1dkmSn69vXzdK6cgkF2ysj6p6TJIdWmuXtdZaBuHVi/quFQAAAID+bK01nJ6R5KbW2o+H2mZX1fer6uKqekbXtluSpUPHLO3a1quqXlNVC6pqwbJly/qvGgAAAIBN2lqB0x/k3qObbkzyuNbafklOTvLRqtrh/nbaWvtAa22stTY2a9asnkoFAAAA4P4Y5RpO61VV05L8XpKnrWlrrd2d5O7u/cKqWpxkTpIbkuw+dPruXRsAAAAAD1JbY4TTbyX5YWtt7VS5qppVVVO797+W5AlJrm2t3Zjk9qp6erfu06uSfHYr1AwAAADAOI0scKqqC5J8O8leVbW0ql7d7XpZ7rtY+DOTXF5Vi5J8MskJrbU1C46/Nsk/JLkmyeJ4Qh0AAADAg1oNHv42+YyNjbUFCxZs7TIAAAAAJo2qWthaG9vUcVtr0XAAAAAAJimBEwAAAAC9EjgBAAAA0CuBEwAAAAC9EjgBAAAA0CuBEwAAAAC9EjgBAAAA0CuBEwAAAAC9EjgBAAAA0CuBEwAAAAC9EjgBAAAA0CuBEwAAAAC9EjgBAAAA0CuBEwAAAAC9EjgBAAAA0CuBEwAAAAC9EjgBAAAA0CuBEwAAAAC9EjgBAAAA0CuBEwAAAAC9EjgBAAAA0CuBEwAAAAC9EjgBAAAA0CuBEwAAAAC9EjgBAAAA0CuBEwAAAAC9EjgBAAAA0CuBEwAAAAC9EjgBAAAA0CuBEwAAAAC9EjgBAAAA0CuBEwAAAAC9EjgBAAAA0CuBEwAAAAC9EjgBAAAA0CuBEwAAAAC9EjgBAAAA0CuBEwAAAAC9EjgBAAAA0CuBEwAAAAC9EjgBAAAA0CuBEwAAAAC9EjgBAAAA0CuBEwAAAAC9EjgBAAAA0CuBEwAAAAC9EjgBAAAA0CuBEwAAAAC9EjgBAAAA0CuBEwAAAAC9EjgBAAAA0CuBEwAAAAC9EjgBAAAA0CuBEwAAAAC9EjgBAAAA0CuBEwAAAAC9EjgBAAAA0CuBEwAAAAC9EjgBAAAA0CuBEwAAAAC9EjgBAAAA0CuBEwAAAAC9EjgBAAAA0CuBEwAAAAC9EjgBAAAA0CuBEwAAAAC9GlngVFXnVNXNVfWDobZTq+qGqlrUvZ4/tO8tVXVNVV1dVc8daj+8a7umqt48qnoBAAAA6McoRzidl+Tw9bSf0Vqb172+mCRVtU+SlyV5UnfO/66qqVU1Ncl7kjwvyT5J/qA7FgAAAIAHqWmj6ri1dklV7THOw1+Y5GOttbuTXFdV1yQ5oNt3TWvt2iSpqo91x/5bz+UCAAAA0JOtsYbTiVV1eTfl7pFd225Jfjp0zNKubUPt61VVr6mqBVW1YNmyZX3XDQAAAMA4bOnA6b1J9kwyL8mNSf6uz85bax9orY211sZmzZrVZ9cAAAAAjNPIptStT2vtpjXvq+qDST7fbd6Q5LFDh+7etWUj7QAAAAA8CG3REU5V9Zihzd9NsuYJdhcmeVlVbVNVs5M8Icm/JvlukidU1eyqelgGC4tfuCVrBgAAAOD+GdkIp6q6IMnBSXauqqVJ3pbk4Kqal6QlWZLkD5OktXZlVX08g8XAVyV5XWvtnq6fE5N8OcnUJOe01q4cVc0AAAAAbL5qrW3tGkZibGysLViwYGuXAQAAADBpVNXC1trYpo7bGk+pAwAAAGASEzgBAAAA0CuBEwAAAAC9EjgBAAAA0CuBEwAAAAC9EjgBAAAA0CuBEwAAAAC9EjgBAAAA0CuBEwAAAAC9EjgBAAAA0CuBEwAAAAC9EjgBAAAA0CuBEwAAAAC9EjgBAAAA0CuBEwAAAAC9EjgBAAAA0CuBEwAAAAC9EjgBAAAA0CuBEwAAAAC9EjgBAAAA0CuBEwAAAAC9EjgBAAAA0CuBEwAAAAC9EjgBAAAA0CuBEwAAAAC9EjgBAAAA0CuBEwAAAAC9EjgBAAAA0CuBEwAAAAC9EjgBAAAA0CuBEwAAAAC9EjgBAAAA0CuBEwAAAAC9EjgBAAAA0CuBEwAAAAC9EjgBAAAA0CuBEwAAAAC9EjgBAAAA0CuBEwAAAAC9EjgBAAAA0CuBEwAAAAC9EjgBAAAA0CuBEwAAAAC9EjgBAAAA0CvDELkBAAAgAElEQVSBEwAAAAC9EjgBAAAA0CuBEwAAAAC9EjgBAAAA0CuBEwAAAAC9EjgBAAAA0CuBEwAAAAC9EjgBAAAA0CuBEwAAAAC9EjgBAAAA0CuBEwAAAAC9EjgBAAAA0CuBEwAAAAC9EjgBAAAA0CuBEwAAAAC9EjgBAAAA0CuBEwAAAAC9EjgBAAAA0CuBEwAAAAC9EjgBAAAA0CuBEwAAAAC9EjgBAAAA0CuBEwAAAAC9GlngVFXnVNXNVfWDobZ3VtUPq+ryqvpMVe3Yte9RVXdV1aLu9b6hc55WVVdU1TVVdWZV1ahqBgAAAGDzjXKE03lJDl+n7aIk+7bWnpzkR0neMrRvcWttXvc6Yaj9vUmOT/KE7rVunwAAAAA8iIwscGqtXZLk5+u0/d/W2qpu87Iku2+sj6p6TJIdWmuXtdZakg8nedEo6gUAAACgH1tzDafjknxpaHt2VX2/qi6uqmd0bbslWTp0zNKubb2q6jVVtaCqFixbtqz/igEAAADYpK0SOFXVnydZleT8runGJI9rre2X5OQkH62qHe5vv621D7TWxlprY7NmzeqvYAAAAADGbdqWvmBVHZPkBUkO6abJpbV2d5K7u/cLq2pxkjlJbsi9p93t3rUBAAAA8CC1RUc4VdXhSf4syRGttV8Ntc+qqqnd+1/LYHHwa1trNya5vaqe3j2d7lVJPrslawYAAADg/hnZCKequiDJwUl2rqqlSd6WwVPptkly0SA/ymXdE+memeTtVbUyyeokJ7TW1iw4/toMnni3bQZrPg2v+wQAAADAg0x1s9omnbGxsbZgwYKtXQYAAADApFFVC1trY5s6bms+pQ4AAACASUjgBAAAAECvBE4AAAAA9ErgBAAAAECvBE4AAAAA9ErgBAAAAECvBE4AAAAA9ErgBAAAAECvBE4AAAAA9ErgBAAAAECvBE4AAAAA9ErgBAAAAECvBE4AAAAA9ErgBAAAAECvBE4AAAAA9ErgBAAAAECvBE4AAAAA9ErgBAAAAECvBE4AAAAA9ErgBAAAAECvBE4AAAAA9ErgBAAAAECvBE4AAAAA9ErgBAAAAECvBE4AAAAA9GqjgVNVTa2q87dUMQAAAABMfBsNnFpr9yR5fFU9bAvVAwAAAMAEN20cx1yb5NKqujDJL9c0ttZOH1lVAAAAAExY4wmcFnevKUm2H205AAAAAEx0mwycWmt/lSRV9fBu+85RFwUAAADAxLXJp9RV1b5V9f0kVya5sqoWVtWTRl8aAAAAABPRJgOnJB9IcnJr7fGttccnOSXJB0dbFgAAAAAT1XgCp+1aa19fs9Fa+0aS7UZWEQAAAAAT2rieUldV/y3JR7rtV2Tw5DoAAAAAuI/xjHA6LsmsJJ9O8qkkO3dtAAAAAHAfGx3hVFVTk/x5a+31W6gegIe8VfeszukX/SjfWrw8B+05MycfOifTpo7n/w8AAAA8OGw0cGqt3VNVv7mligEgOf2iH+WcS6/LipWr88Of3Z5K8sbD997aZQEAAIzbeNZw+n5VXZjkE0l+uaaxtfbpkVUF8BD2rcXLs2Ll6iTJipWrc+ni5XnjVq4JAADg/hhP4DQjyfIkzxlqaxms6QRAzw7ac2Z++LPbs2Ll6syYPiXz95y5tUsCAAC4X8azhtPlrbUztlA9AA95Jx86J5Xk0sXLM3/PmXnDoXO2dkkAAAD3S7XWNn5A1b+21g7YQvX0ZmxsrC1YsGBrlwEAAAAwaVTVwtba2KaOG8+Uukur6qwk/5R7r+H0vc2oDwAAAIBJajyB07zuz7cPtbXce00nAAAAAEgyjsCptfbsLVEIAAAAAJPDlE0dUFW7VNXZVfWlbnufqnr16EsDAAAAYCLaZOCU5LwkX06ya7f9oyQnjaogAAAAACa28QROO7fWPp5kdZK01lYluWekVQEAAAAwYY0ncPplVc3MYKHwVNXTk9w20qoAAAAAmLDG85S6k5NcmGTPqro0yawkLx5pVQAAAABMWON5St33qupZSfZKUkmubq2tHHllAAAAAExI4xnhtGbdpitHXAsAAAAAk8B41nACAAAAgHETOAEAAADQq3FNqauq3ZI8fvj41toloyoKAAAAgIlrk4FTVf3PJC9N8m9J7umaWxKBEwAAAAD3MZ4RTi9Ksldr7e5RFwMAAADAxDeeNZyuTTJ91IUAAAAAMDmMZ4TTr5IsqqqvJlk7yqm19vqRVQUAAADAhDWewOnC7gUAAAAAm7TJwKm19qGqeliSOV3T1a21laMtCwAAAICJajxPqTs4yYeSLElSSR5bVUe31jylDgAAAID7GM+Uur9Lclhr7eokqao5SS5I8rRRFgYAAADAxDSep9RNXxM2JUlr7Ufx1DoAAAAANmA8I5wWVNU/JPnHbvuoJAtGVxIAAAAAE9l4Aqc/SvK6JK/vtv8lyf8eWUUAAAAATGjjeUrd3UlO714AAAAAsFEbXMOpqj7e/XlFVV2+7ms8nVfVOVV1c1X9YKhtp6q6qKp+3P35yK69qurMqrqmu8ZTh845ujv+x1V19AP/uAAAAACM2sZGOP1J9+cLNqP/85KcleTDQ21vTvLV1trfVtWbu+03JXlekid0rwOTvDfJgVW1U5K3JRlL0pIsrKoLW2u/2Iy6AAAAABiRDY5waq3d2L19bWvt+uFXkteOp/PW2iVJfr5O8wuTfKh7/6EkLxpq/3AbuCzJjlX1mCTPTXJRa+3nXch0UZLDx3N9AAAAALa8DQZOQw5dT9vzNuOauwyFWT9Lskv3frckPx06bmnXtqH2+6iq11TVgqpasGzZss0oEQAAAIAHamNrOP1RVV2RZO911m+6LskVfVy8tdYymCbXi9baB1prY621sVmzZvXVLQAAAAD3w8bWcPpoki8l+ZsM1lla447W2rrT5O6Pm6rqMa21G7spczd37TckeezQcbt3bTckOXid9m9sxvUBAAAAGKGNreF0W2ttSZK/T/LzofWbVlXVgZtxzQuTrHnS3NFJPjvU/qruaXVPT3JbN/Xuy0kOq6pHdk+0O6xrAwAAAOBBaGMjnNZ4b5KnDm3fuZ629aqqCzIYnbRzVS3N4Glzf5vk41X16iTXJzmyO/yLSZ6f5Jokv0pybJK01n5eVf89yXe7496+mSOsAAAAABih8QRO1a21lCRpra2uqvGcl9baH2xg1yHrObYled0G+jknyTnjuSYAAAAAW9d4nlJ3bVW9vqqmd68/SXLtqAsDAAAAYGIaT+B0QpKDMli8e2mSA5O8ZpRFAQAAADBxbXJqXGvt5iQv2wK1AAAAADAJbDJwqqpzk7R121trx42kIgAAAAAmtPEs/v35ofczkvxukn8fTTkAAAAATHTjmVL3qeHtqrogyTdHVhEAAAAAE9p4Fg1f1xOSPKrvQgAAAACYHMazhtMdufcaTj9L8qaRVQQAAADAhLbRwKmqKsmTWms/2UL1AAAAADDBbXRKXWutJfnCFqoFAAAAgElgPGs4fa+q9h95JQAAAABMCptcwynJgUmOqqrrk/wySWUw+OnJI60MAAAAgAlpPIHTc0deBQAAAACTxnim1P11a+364VeSvx51YQAAAABMTOMJnJ40vFFVU5M8bTTlAAAAADDRbTBwqqq3VNUdSZ5cVbd3rzuS3Jzks1usQgAAAAAmlA0GTq21v2mtbZ/kna21HbrX9q21ma21t2zBGgEAAACYQMYzpe7zVbVdklTVK6rq9Kp6/IjrAgAAAGCCGk/g9N4kv6qqpyQ5JcniJB8eaVUAAAAATFjjCZxWtdZakhcmOau19p4k24+2LAAAAAAmqmnjOOaOqnpLklckeWZVTUkyfbRlAQAAADBRjWeE00uT3J3k1a21nyXZPck7R1oVAAAAABPWJkc4dSHT6UPbP4k1nAAAAADYgE2OcKqq36uqH1fVbVV1e1XdUVW3b4niAAAAAJh4xrOG0/9K8juttatGXQwAAAAAE9941nC6SdgEAAAAwHiNZ4TTgqr6pyT/nMHi4UmS1tqnR1YVAAAAABPWeAKnHZL8KslhQ20ticAJAAAAgPsYz1Pqjt0ShQAAAAAwOYznKXW7V9Vnqurm7vWpqtp9SxQHAAAAwMQznkXDz01yYZJdu9fnujYAAAAAuI/xBE6zWmvnttZWda/zkswacV0AAAAATFDjCZyWV9Urqmpq93pFkuWjLgwAAACAiWk8gdNxSY5M8rMkNyZ5cRILiQMAAACwXuN5St31SY7YArUAAAAAMAmM5yl1H6qqHYe2H1lV54y2LAAAAAAmqvFMqXtya+3WNRuttV8k2W90JQEAAAAwkY0ncJpSVY9cs1FVO2UcU/EAAAAAeGgaT3D0d0m+XVWf6LZfkuQdoysJAAAAgIlsPIuGf7iqFiR5Ttf0e621fxttWQAAAABMVOOZUpckOyX5ZWvtrCTLqmr2CGsCAAAAYAIbz1Pq3pbkTUne0jVNT/KPoywKAAAAgIlrPCOcfjfJEUl+mSSttX9Psv0oiwIAAABg4hpP4PQfrbWWpCVJVW032pIAAAAAmMjGEzh9vKren2THqjo+yVeS/MNoywIAAABgohrPU+pOq6pDk9yeZK8kf9lau2jklQEAAAAwIW0ycEqSLmC6KEmqakpVHdVaO3+klQEAAAAwIW1wSl1V7VBVb6mqs6rqsBo4Mcm1SY7cciUCAAAAMJFsbITTR5L8Ism3k/zXJG9NUkle1FpbtAVqAwAAAGAC2ljg9GuttblJUlX/kOTGJI9rra3YIpUBAAAAMCFt7Cl1K9e8aa3dk2SpsAkAAACATdnYCKenVNXt3ftKsm23XUlaa22HkVcHAAAAwISzwcCptTZ1SxYCAAAAwOSwsSl1AAAAAHC/CZwAAAAA6JXACQAAAIBeCZwAAAAA6JXACQAAAIBeCZwAAAAA6JXACQAAAIBeCZwAAAAA6JXACQAAAIBeCZwAAAAA6JXACQAAAIBeCZwAAAAA6JXACQAAAIBeCZwAAAAA6NUWD5yqaq+qWjT0ur2qTqqqU6vqhqH25w+d85aquqaqrq6q527pmgEAAAAYv2lb+oKttauTzEuSqpqa5IYkn0lybJIzWmunDR9fVfskeVmSJyXZNclXqmpOa+2eLVo4AAAAAOOytafUHZJkcWvt+o0c88IkH2ut3d1auy7JNUkO2CLVAQAAAHC/be3A6WVJLhjaPrGqLq+qc6rqkV3b/2fvzuPkqO57739Pd8+iZSShERIgCRADQoANkpEBSTZxsBVjZwFjm9jO4sTci3nixDFylOXGz70iuX5dX4JF4sSxjW0cfJ3YAYOX5PEDVnBs0IaREEYWSINGEtYIrSON1lm6u879o7t6qqurepmpXqbn8369eDHq6a46+zn166ozcyXt97ynN/taAWPM3caYLcaYLUePHq1OigEAAAAAAFBU3QJOxphWSb8h6bHsS1+Q1KXM43YHJX220mNaax+y1i611i49//zzI0srAAAAAAAAylfPO5zeJekFa+1hSbLWHrbWpq21jqQva+SxuQOS5ns+Ny/7GgAAAAAAABpQPQNOH5TncTpjzIWe371H0s+zP39f0geMMW3GmAWSrpD005qlEgAAAAAAABWp+V+pkyRjzBRJKyV91PPy/caYxZKspH3u76y1O4wxj0p6WVJK0sf4C3UAAAAAAACNqy4BJ2vtWUmdvtd+p8j7Py3p09VOFwAAAAAAAMau3n+lDgAAAAAAAE2GgBMAAAAAAAAiRcAJAAAAAAAAkSLgBAAAAAAAgEgRcAIAAAAAAECkCDgBAAAAAAAgUgScAAAAAAAAECkCTgAAAAAAAIgUAScAAAAAAABEioATAAAAAAAAIkXACQAAAAAAAJEi4AQAAAAAAIBIEXACAAAAAABApAg4AQAAAAAAIFIEnAAAAAAAABApAk4AAAAAAACIFAEnAAAAAAAARIqAEwAAAAAAACJFwAkAAAAAAACRIuAEAAAAAACASBFwAgAAAAAAQKQIOAEAAAAAACBSBJwAAAAAAAAQKQJOAAAAAAAAiBQBJwAAAAAAAESKgBMAAAAAAAAiRcAJAAAAAAAAkSLgBAAAAAAAgEgRcAIAAAAAAECkCDgBAAAAAAAgUgScAAAAAAAAECkCTgAAAAAAAIgUAScAAAAAAABEioATAAAAAAAAIkXACQAAAAAAAJEi4AQAAAAAAIBIEXACAAAAAABApAg4AQAAAAAAIFIEnAAAAAAAABApAk4AAAAAAACIFAEnAAAAAAAARIqAEwAAAAAAACJFwAkAAAAAAACRIuAEAAAAAACASBFwAgAAAAAAQKQIOAEAAAAAACBSBJwAAAAAAAAQKQJOAAAAAAAAiBQBJwAAAAAAAESKgBMAAAAAAAAiRcAJAAAAAAAAkSLgBAAAAAAAgEgRcAIAAAAAAECkCDgBAAAAAAAgUgScAAAAAAAAECkCTgAAAAAAAIgUAScAAAAAAABEioATAAAAAAAAIkXACQAAAAAAAJEi4AQAAAAAAIBIEXACAAAAAABApAg4AQAAAAAAIFIEnAAAAAAAABApAk4AAAAAAACIFAEnAAAAAAAARIqAEwAAAAAAACJVt4CTMWafMWa7MeZFY8yW7GszjTHrjDGvZv9/XvZ1Y4z5nDFmtzHmJWPMm+qVbgAAAAAAABRX7zucftlau9hauzT77z+X9LS19gpJT2f/LUnvknRF9r+7JX2h5ikFAAAAAABAWeodcPK7TdIj2Z8fkXS75/Wv24zNkmYYYy6sRwIBAAAAAABQXD0DTlbSD40xW40xd2dfm2OtPZj9+ZCkOdmf50ra7/lsb/a1PMaYu40xW4wxW44ePVqtdAMAAAAAAKCIRB3P/RZr7QFjzGxJ64wxO72/tNZaY4yt5IDW2ockPSRJS5cureizAAAAAAAAiEbd7nCy1h7I/v+IpO9IukHSYfdRuez/j2TffkDSfM/H52VfAwAAAAAAQIOpS8DJGDPFGNPh/izpVyT9XNL3JX04+7YPS/pe9ufvS/rd7F+ru0nSSc+jdwAAAAAAAGgg9Xqkbo6k7xhj3DT8i7X2SWPM85IeNcbcJek1SXdm3/8DSe+WtFvSOUm/X/skAwAAAAAAoBx1CThZa/dIui7g9T5Jbw943Ur6WA2SBgAAAAAAgDGq51+pAwAAAAAAQBMi4AQAAAAAAIBIEXACAAAAAABApAg4AQAAAAAAIFIEnAAAAAAAABApAk4AAAAAAACIFAEnAAAAAAAARIqAEwAAAAAAACJFwAkAAAAAAACRIuAEAAAAAACASBFwAgAAAAAAQKQIOAEAAAAAACBSBJwAAAAAAAAQKQJOAAAAAAAAiBQBJwAAAAAAAESKgBMAAAAAAAAiRcAJAAAAAAAAkSLgBAAAAAAAgEgl6p0AAONLKu1o7bpubezp0/KuTq1auVCJOLFrAADGM+Z3oDz0FaB8BJwAVGTtum49vGGvBpOOdh46JSNp9a2L6p0sAAAwBszvQHnoK0D5CMUCqMjGnj4NJh1J0mDS0YaevjqnCAAAjBXzO1Ae+gpQPgJOACqyvKtT7S2ZoaO9JaYVXZ11ThEAABgr5negPPQVoHw8UgegIqtWLpSRtKGnTyu6OnXvyoX1ThIAABgj5negPPQVoHzGWlvvNFTF0qVL7ZYtW+qdDAAAAAAAgKZhjNlqrV1a6n08UgcAAAAAAIBIEXACAAAAAABApAg4AQAAAAAAIFIEnAAAAAAAABApAk4AAAAAAACIFAEnAAAAAAAARIqAEwAAAAAAACJFwAkAAAAAAACRIuAEAAAAAACASCXqnQAAAAA0l1Ta0dp13drY06flXZ1atXKhEnG+5wQAYCIh4AQAAIBIrV3XrYc37NVg0tHOQ6dkJK2+dVG9kwUAAGqIr5oAAAAQqY09fRpMOpKkwaSjDT19dU4RAACoNQJOAAAAiNTyrk61t2SWme0tMa3o6qxzigAAQK3xSB0AAAAitWrlQhlJG3r6tKKrU/euXFjvJAEAgBoj4AQAAIBIJeIxrb51kVbXOyEAAKBueKQOAAAAAAAAkSLgBAAAAAAAgEgRcAIAAAAAAECkCDgBAAAAAAAgUgScAAAAAAAAECkCTgAAAAAAAIgUAScAAAAAAABEioATAAAAAAAAIkXACQAAAAAAAJEi4AQAAAAAAIBIEXACAAAAAABApAg4AQAAAAAAIFIEnAAAAAAAABApAk4AAAAAAACIFAEnAAAAAAAARIqAEwAAAAAAACJFwAkAAAAAAACRIuAEAAAAAACASBFwAgAAAAAAQKQIOAEAAAAAACBSBJwAAAAAAAAQKQJOAAAAAAAAiBQBJwAAAAAAAESKgBMAAAAAAAAiRcAJAAAAAAAAkSLgBAAAAAAAgEgRcAIAAAAAAECkah5wMsbMN8b8pzHmZWPMDmPMH2dfX2OMOWCMeTH737s9n/kLY8xuY8wuY8w7a51mAAAAAAAAlC9Rh3OmJH3SWvuCMaZD0lZjzLrs7x601j7gfbMx5mpJH5B0jaSLJP2HMWahtTZd01QDAAAAUCrtaO26bm3s6dPyrk6tWrlQiXjjPDjR6OkDgImi5gEna+1BSQezP582xrwiaW6Rj9wm6VvW2iFJe40xuyXdIGlT1ROLcaGRFhXVSksj5bGW/Pn++C2X63M/2j3hygFAc/KOcTctmClJ2rz3OONbk2qmuXztum49vGGvBpOOXjl4Upt7+uRIDZMvb/p2HjolI2n1rYvqmiZUVzX6VzP12XJNxDyjuupxh1OOMeZSSUskPSdphaQ/NMb8rqQtytwFdUKZYNRmz8d6VTxAhQmmkRYV1UpLI+Wxlvz53tzTp5cPnZpw5QCgOXnHuO29/TLGKOVYxrcm1Uxz+caePg0mHUnSUMpqW2+/rFXD5MubvsGkow09fVpd1xSh2qrRv5qpz5ZrIuYZ1VW3cKUxZqqkxyV9wlp7StIXJHVJWqzMHVCfHcUx7zbGbDHGbDl69Gik6UXjClpUNFtaGimPteTP987DpydkOQBoTt4xLm2llGMlMb41q2aay5d3daq9JXMZYSTZTNNtmHx509feEtOKrs46pwjVVo3+1Ux9tlwTMc+orroEnIwxLcoEm/7ZWvuEJFlrD1tr09ZaR9KXlXlsTpIOSJrv+fi87GsFrLUPWWuXWmuXnn/++dXLABpKIy0qqpWWRspjLfnzvWhOx4QsBwDNyTvGxY2UiBlJjG/Nqpnm8lUrF+quFQu0eP4MLZk/o+Hy5U3fXSsW6N6VC+udJFRZNfpXM/XZck3EPKO6jHW/kqjVCY0xkh6RdNxa+wnP6xdm93eSMeZeSTdaaz9gjLlG0r8oE4C6SNLTkq4otWn40qVL7ZYtW6qVDTSQVNrRg+u6taGnTyu6OnVvnfdwqkZaGimPteTP9x/dcrn+/ke7J1w5AGhO3jFu2YKZkpE27TnO+NakmnUub9Z8YXypRjuciG17IuYZo2OM2WqtXVryfXUIOL1F0rOStktysi//N0kfVOZxOitpn6SPegJQfynpI8r8hbtPWGv//1LnIeAEAAAAAAAQrXIDTvX4K3XrlXnc2+8HRT7zaUmfrlqiAAAAAAAAEBnujwMAAAAAAECkCDgBAAAAAAAgUgScAAAAAAAAECkCTgAAAAAAAIgUAScAAAAAAABEioATAAAAAAAAIpWodwIATCyptKO167q1sadPy7s6tWrlQiXixL7rZbzUx3hJJzBeNHufqlX+mr0cJ7LxVLfjKa0AJhYCTgBqau26bj28Ya8Gk452HjolI2n1rYsiPw+Lr/LUqj7GarykExgvmr1P1Sp/zV6OE9l4qtvxlFYAEwtXXwBqamNPnwaTjiRpMOloQ09fVc7jLr5e3N+vhzfs1YPruqtynvGuVvUxVuMlncB40ex9qlb5a/ZynMjGU92Op7QCmFgIOAGoqeVdnWpvyQw97S0xrejqrMp5WHyVp1b1MVbjJZ3AeNHsfapW+Wv2cpzIxlPdjqe0AphYeKQOQE2tWrlQRtKGnj6t6OrUvSsXVuU8y7s6tfPQKQ0mHRZfRdSqPsZqvKQTGC+avU/VKn/NXo4T2Xiq2/GUVgATi7HW1jsNVbF06VK7ZcuWeicDQJ2k0o4eXNedt/hiDycAAAAAGBtjzFZr7dJS7+MOJwBNKRGPafWti7S63gkBAAAAgAmIr/sBAAAAAAAQKQJOAAAAAAAAiBSP1KFhpdKO1q7r1saePi3v6tQq9uABgKbSLON8rfLRLOU1FuO9DCpJf9h7x3sZNJvxWh+Nku5GSQeqg/oFASc0rLXruvXwhr0aTDraeeiUjKTVty6qd7IAABFplnG+VvlolvIai/FeBpWkP+y9470Mms14rY9GSXejpAPVQf2C8CIa1saePg0mHUnSYNLRhp6+OqcIABClZhnna5WPZimvsRjvZVBJ+sPeO97LoNmM1/polHQ3SjpQHdQvCDihYS3v6lR7S6aJtrfEtKKrs84pAgBEqVnG+Vrlo1nKayzGexlUkv6w9473Mmg247U+GiXdjZIOVAf1Cx6pQ8NatXKhjKQNPX1a0dWpe1curHeSAAARapZxvlb5aJbyGovxXgaVpD/sveO9DJrNeK2PRkl3o6QD1VFO/bLPU3Mz1tp6p6Eqli5dards2VLvZAAAAAAAgAD3P7kzt89Te0tMd61YwD5P44AxZqu1dmmp9xE6BAAAAAAANcc+T82NgBMAAAAAAKg59nlqbuzhBAAAAAAAao59vJobAScAAAAAAFBziXhMq29dpNX1TgiqgkfqAAAAAAAAECkCTgAAAAAAAIgUAScAAAAAAABEioATAAAAAAAAIkXACQAAAAAAAJEi4AQAAAAAAIBIEXACAAAAAABApBL1TgAAYHxIpR2tXdetjT19Wt7VqVUrFyoR53sLAAAAAIUIOAEAyrJ2Xbce3rBXg0lHOw+dkpG0+tZF9U4WAAAAgAbEVywdBeYAACAASURBVNMAgLJs7OnTYNKRJA0mHW3o6atzigAAAAA0KgJOAICyLO/qVHtLZtpob4lpRVdnnVMEAAAAoFHxSB0AoCyrVi6UkbShp08rujp178qF9U4SAAAYBfZlBFALBJwAAGVJxGNafesira53QgAAwJiwLyOAWiCMDQAAAAATCPsyAqgFAk4AAAAAMIGwLyOAWuCROgAAAACYQNiXEUAtEHACAABADpsJA82PfRkB1AIBJwAAAOSwmTAAAIgCX1cBwASWSju6/8mduv3zG3T/kzuVSjv1ThKAOmMzYQCYGFgHotq4wwkAJjDuZADgt7yrUzsPndJg0mEzYQBoYqwDUW0EnABgAgu6k4H9HICJjc2EAWBiYB2IaiPgBAATGHcyAPBjM2EAmBhYB6LaCDgBwATGnQwAAAATE+tAVJux1tY7DVWxdOlSu2XLlnonAwAAjEIq7Wjtum5t7OnT8q5OrVq5UIk4f+sEAACg3owxW621S0u9jzuc0FQm2gWKm98Nu48pbozS1mrF5bO0KvvtxHgoC2+d3bRgpiRp897jDZ1mv1Ta0QNP7dIT2w5Iku5YMld/8s4rlYjHat4ma12eYfmrdr5He/ygz0nSA0/t0uMv9OrMUEod7S15dTieFRsjKs2bv+w+fsvl+tyPdhetg7G0x7FuZDqWNthIc0m9+tho0uT//VjaXSPVwWg1Qx7qoZrlNt7qpJz0RpmnRiufao5/xY4dtqYbzfFqrVHWX1Gu0xq5j6I0Ak5oKhPtLy148+vadfi0jCQrjYuy8OZhe2+/jDFKObah0+y3dl23vvzsHqWzN4x+Zf1exWNGq29dVPM2WevyDMtftfM92uMHfc5KefU3kBzKq8PxrNgYUWne/GW3uadPL2f3fQirg7G0R/9Gpl/buE9WKnvxOZY22EhzSbX62FgW9aXOHUW7a6Q6GK1myEM9VLPcxludlJPeteu69dX1ezSUsvrZ/n5t7unTo/csG9VFeqOVTzXXGMWOHbamG83xaq1R1l9RrtMauY+iNMKFaCpBf2mhmXnz63LzPV7KwpvOtJVSTmaGb+Q0+23s6cstTKRMHty017oeal2eYfmrdr5He/ygz/nrT8qvw/Gs2BgxlmMNJh3tPHy6ZB2MpT0u7+pUe8vIMuXccFoPb9irB9d1jyq9leS5kcbPavUxd1H/4v7+isq1nHNH0e4aqQ5GqxnyUA/VLLfxViflpHdjT5+GUpmx1Uratr+/ov5c6flqqZprjGLHDlvTjeZ4tdYo668o12kY3wg4oal4L1Amwl9a8F+QSSP5Hi9l4U1n3EiJmJHU2Gn2W97VqbgZ+XciZnJpr2Y9pNKO7n9yp27//Abd/+ROpdJOzcszLH/Vbn+jPX7Q5/z1J+XX4XhWbIwYy7HaW2JaNKejZB2MpT2uWrlQd61YoMmt8dxrlSw+x9IGG2n8rFYfG8uivtS5o2h3jVQHo1XvPATNEeNBNcut3nVSqXLSu7yrU94pzEqjvkhvtPKp5hqj2LHD1nSjOV6tNcr6K8p1Gsa3+Jo1a+qdhqp46KGH1tx99931TgaU3d/mh7v0N0/t0i+On9ONC2bKsTbvtesvnqG167rz3hOLmdIH97lxwUwNJR0l01a3XXeR7l25MPA4QWly3xf2u2KfKTffo8lTOfkdTjmaO2OSZne06fbFc3XvyoVa1tVZtCxKpS/K9Bc7lrfObr/uIl1/yXlKOYVpdhfOqx79mb787B71nRnSTZd1jrlMozjujQtmajjp6BfHz2lKW0K/fdPFWvUrVyoWMyXbZLnlHPQ+9w6F3hMD2n7gpIaSjlatXJhXnksunqH9xwfUlohr0QUdRfM2mjr35++Pbrk8t3fLnGntmt3Rpt+49iI5Vnrgh93ad+ys1r96TA/8sLuifnT/kzv12PP79S8//YX2Hz+nj99yuZJpW3DeUmkPqo9lXZ0aTjp6re+sJOm8ya15dViOsY4P5Y6BlZwnlXb07KtHtf3AKTmO1eyONnWdPzU3RlTad/xld//7rs2rg6Bjltu/g/LXe2JAq1Yu1EAyre0HTirlWLW3xHTbdRdpxeWzyk7vUDKtVNpq054+fX3jPj30zB595dm9Rft6UDsZTqV15xc36b5/f1lPv3xYv/rGC/S3T79a1TE+LC3ljC2l/OL4uVGVa7E0+X8/nHJ00fR2pR2r9pb8MahUWx5t/mox95brxgUzNTCU1i+Onwsdg0eb3nI+98APdxXMEd46HktZuXPnvf/6oh5c162vb3ptVPNnUBpKrV9GK5V2tGH3Me05dlZT2hL60A3h47w3XZXOW1Eqpx/cuGCm1r96TAdPDUqSjKSLprfrvdfPK7m2Hc35KjWW+XHujElacvGMgnnD27da4zGdHEjqm9n1QdB1RtA637tO8c6LxdZ0o0lrNcsqSNi6bKxjYli5hR3fOw/M6WjXvr6zemxLb24dV8s2iOq47777Dq5Zs+ahUu/jr9Sh6u5/cmfuWdz2lpjuWrEgb3+h9paYrr5gWm4vEPc91XxeNyhN7vnCflfsM5WeoxGUSl+U6Y/iWPc/uVNf+klP7jbnRMzoozdfNuYy/cwPXtGXntkjdySMx4zuieC45Sq3bILet6GnTy/u78+9Z/H8Gfrux1aM6viVvreS/Hj7e9wot49Ppf3IVe32OhpjHR/KHQMrrc9q9JlqCsrfvSsX6sF13Xl/srmSvUnu+PwGveDpJ65Ky8N/nDkdbTo5mCyoi/Gy4Wkq7YypXMsV1Zw61vPVS7Xm2nI+d/vnNxTMEd++Z1mufcakUa+9/OOLNLoxppb1Ndr5sNJ5qx5SaUd3fnGTtvX2y9rCedKfn86pbXrf9fNqMj5Va/1cbH3gv84oZ51fi7yN9XijmV+iSuNox/Jy13EYf8r9K3WNtwJC0wnbM6XSvUCqnaZSv6v08YNGfwa5kj04xpr+qJ61H80z9aU8se2AvGH3dI337hnLs/Dl3HZcSdlXa0+EsezjU+5eMPXub2MdH8odAyutz2r0mWoKyl8iHtPqWxfpux9bodW3Lqr44mjn4dOBr7vlUe5jR/7jHDkzFFgXY9kbqZbGWq7lqvU+b/UeCypNTzX3OQmaI7ztc9v+/lGXVVR739WyvkY7H5Yzb9X78cVEPCZHknsfQbG1bdpKR04P1Wx8qtb6ebT7mDba+rbc47lBxS/8pKei+aXUMcttt6Mdy6PcSxLjEwEnVF3YnimV7gVS7TSV+l2lzxQ3+jPIlezBMdb0R/Ws/WieqR+NWtZVuWVz04KZufzHjbRswczcHjeL58/I3Qky2uNX+t5KjjGWfXzK3Qum3v1trONDuWNgpfVZqz4TlVL5G81F3aI5HYGvu+VRboDIf5zZU9sC09poAY96q/U+b/UeCypNTzX3OQmaI7zt00q5vX/aEkYxqey+FdXed7Wsr9HOh+XMW40QaC53beuq1fhUrfXzaPcxbbT1bbnHW7uuW9v294cGFUd7zHLbrfc43vEilj1u0PH9n3M1wtiM2mEPJ1Rd2J4ple4FUu00Be0pNJb9Mhr9GeRy9+CIIv1RHKuSZ+or0XdmSC/84kTuLqc3zZ+htb+5uKH2Z5Ck9a8e07b9/XJs5rG/6y85TzcvnK0Vl8/SB264WCsunzXmZ+Gjqqdi/b2cfXyCjhe0T1m12utojHV8KHcMrLQ+q9FnyhXFnmD+/JXakybIbYsv0saePvWfG9bMKS2a3JrQVE95fHZdt3pPDGTS7GTq4AM3XBx6nJMDSb1x7nR99w+WK+2oIK1j2RupGVVrD6pKz1cv1Zpry/lcLGYK5ghv+2xLGF07b4bmTGvXnI52vXzoVNl9yx1fxrL33VjyPxqjnQ/Lmbfcfeek4uNINZWztn2t75yGUo6c7GN3tRifqrV+Hu0+po22vi33eH/z1C4dyu7TJWWCxe9909yS9VfqmOW2W+9xvOPF8XNDuuai6Zozrb3knn7F1nEYf9jDiT2cMArjZe8NRKdW+5iMVdBeHP79moBGUo29WarRD6JOZyVjCnMOai2sfVbat2i7+aIYR2pRpuNlzYNC9z+5U19dv0dDKSsjacnFM/ToR5eNqf5G225Zk0Iqfw+nRC0SA9TKWCdr99bSwaSjnYdOyUhsaOfRjAtMdx+T1fVOSAnLuzq107O560S8FbkZ21+jqEbZBj1aNtZ+Vo1+sGrlQhkp7wJsLCoZU5hzGlO1xxr/8T9+y+X63I9212RsC2uflfYt2m6+KMaRWpTpeFnzoFBQGxvrODHadsuaFJUg4ISmMtbJuhoXSM1kPC4wmyVI4S4K1u8+prgxWr/7mOyTO8dtfkajlu2vWdpNuapRtuMhOCTV9wIsyjlnorXZagrqD/euXBhZ+fqPv7mnL/fX4uo1t1bat1gvjfD3vdEGAihTFFONuWq0x6zGXIzmRcAJTWWsk3UlF0jeBcZNC2ZKkjbvPZ5biEoa14v/oIsXf/l+beM+Wamh81buhXQ5F2v1vqCzkg6eHNTxs8NKOVa7Dp+u+YVJPcugWovxoDzVIrhVq7Is5zxBZXvvGNNXakE6mvxXsjguNUZX41GVSvOzvKtTrxw8mXtEIpY9TqnPBeXtiW0HcmNDqTbbSG2v0aTSjr69tbegP1hFNyb4+9tL2T2VvOerdeAxEY/p3pULZdWdy6//s95juxsFVxpQHmubqGa/DkqbVHotF9V8Uc+7RurRV0dzzvE4pjSjWn9RQ72PbwScUBVhA8PgcEof+vJzeuXQKU1rb9Gcae16yxWzyh44Sg04YZN1Ku3ogad26YltByRJdyyZqz9555UF5/ReIC1bMFNpx+r2z28IXNR4Fxjbe/tljFHKsdre269vb+3VvBmTKvrGstw0hpXHTQtmyrFWT7zQq+Nnk7KSJrXG9Ts3XKQ/a3tCsX3Pyrn0rfps6n3asOdk0QHb/dOr23ozfw3DTf9NC2Zq+4GTSmcXxueG0/rCj3u0uadPj96TeY58rPmodCJx29TOw6e1aE6H/uW/3qj21pGhrdwgxQNP7dKXn92jtJVe3J+pwzuWzJU0Uu+OY/VPm/aVVael0hWWZ/f1Ddk7mdLWasXls/LOnTtHkfyEPbLhPe6yyzrlWKvvvvi6pJG6ksIX2GvXdef2EHhxf7/+aeM+/e5Nl+gT77hCf/sfr+bV+yfecUXFj4kUS/fB/oG89xrZohfnxR5bcfv0pj19OnRyUIdPD0mSfra/X5t7+pRybF67Wb/7WK6cS7XxsDr053+0FymVtJ1ll3Vq854+vdh7UpK0/cBJWWv1Z++6Ku94RiP7OcZjRssum1kwxn17a6/ed/28ii5g3QVpUJofeGqXvrJ+b2bc9KWr1JhQqowHh1O6+f7/1JEzw5IyfdqYzJ8Mf6m3X5t6jsnKFJSfv16D2nVYHxlNfX78lsv1+NZeHT49JCvp5YOn9NmndslKRcdQ77m8+2i4BpOOHt6wV1/buE+L5nTo6x95s/7xJ3ty5bX/xDmdOJcsKzjlV8l47R1Xt/f2yzpWf/buqwre5/71wf+z+TUNJh0ZSY4yfxns95Zfoj9/11WjHjtKPbbmn0c37zmmI9nxQMr2hwUzA4NQq0POXaqP+OdSN9jkCgo8lnuOYv3W/f36V4/q8KkhnRpMadGcqXrzgpn66b4TMtbqpQMnlbbSS/sL+4n32G0Jo6svnKbeEwM6M5TSY1t7lXZsyfl+7bpufeXZHg2nM/3y8a29enrVzXntM2jMdPP/7a29ucCqd+0V1I5LjRNun3/8hV6dGUrJWquhlJXVSHs1MVNyHAxbZ5RbZ950xIzR7I42vfdNc0veNVLpOqJYAK2S8ctfblNa44rFjE4PpvPaU6m+UOmYGbY2jerLoHLmnWqtb4utGSut54/fcnngvDWaAI2/rjvaW3T74osUM6aqX+C45/YGuOt9FyhGj03DURUFG9vNn6FH71mmO7+4SS/4FsftLTH9/rJLZWKm5IBcanO7sM0Q739yp770kx6ls809ETP66M2XFR2svOeKG+UWNe55N/T0BS70XUaSt3ddN2+6Vlw+KzSPpdJY6oItzOrEt3RX4im1a0hJ06avpt+lzwzfqbaE0TUXTpcjFaTn/id36gs/7slL/+L5M3TTgpn68vo98v+1ZCPpnpsvk4kZfXtrr45mL578+QibNP1l3Tm1Te+7fl5Z+1q85x/Wa1v2YlqSZne06sLpk3KTb9yY3CRVbEPEGz79H3kXGX5GmQDeueF07jW3ToMWdf62boy0ZN6M3CIiqC3fG/A5KdNHprW3FKTPmx/vwiMmaefh0xrILoDjRmqJGw2mCtuJt526dWWlwP4btLGsa05Hm46dGcprv9fOnZ4re2+9FluceMcOSZrcEtNAylHQVBUz0uJ5M3Jt2N9evEG69paYrr5gWl563D4dVCazO9pyQSg3f0+vulm/+/DzeulAv1KePhA30nWedAQFB4Pa3mg33QwbB72vFzO1xWrN1O9pWWyHXpt2vT559Nd08EwqL//33HyZNu7p0888fSssH6UWw/7FonuMR7fs19FsQEiSzp/aquc/tbJoHoMuOoPSdsfnNxT0ozBL5k/XY/cs19p13QVj8LVzp+tnvf0F47KV8tL3+8su1ePbDuT10XLqM2isnd3Rpj5fX/LPV7f9w/qCuilmTkebTg4mQ9uGN62lLpD88/viedN142Wd2rSnr2AcXP6ZH+WVyeyONv30L98RWA7/+OOe0PR/7G1dJQMJMSkXLIkb6dp50/XywVO5sSQRk6T8edzfb+JGuXJ3JWKZv8B27Mxwrp7iRrp27nQdOJn5i1FzZ7TrlUOnc4EY/9wq5V/kD6fS+ur6fQqbud1juHNYJXNZ0BjtjuM3LJipr23cmyuT3O+zwdgwYesef1uNSZo1tVXHzw2rNRHXb994sf40m0Y3/6/3DxTMZXM62tQ/MJyXLu+8GNbn/fzrrGJjcdh8m5fvhNG0Sa0F6fXPi+WMyWFj5wNP7dIjm/bl5mvXkmy/KnZRX2wd4QZlXG6b6u0fyJWjN03+MeW6edP1vT98S2C5+Nerfm578ufZP7Zs2H0s75zuOFTsi7cvPbOnYLy8aMaksgIfbmD765v2aSiVyf+VczpkpYIv9oLq6zM/eCXv/PGY0T03X6Y/uuXy0GBRsfWtNygkWR05PTIfTm6N6/eWX1qw1o/HjO5+6wJ98leuDKzn9paYFs2eqhcPnMq95qZzNAGaz/zgFX3xmT0Fr3vr2L2GW//qUR06Oai+s8Nqaxnp/+UGo/zB/+f2Zr4ss7bwmopNyhsDm4ajrjb29OUWDlbStv39enBdt3YePl3w3sGko3/alFl4lYpcl3O3ivX93/2cd2JMObbkLevec6Wtcqsx97zeu6mCLl6tRgbI9paY4sYU/SanVBqDvgl6YtuBoosvSVoe26F2ZRZLLXZIN2q7pDs1lLKh3xJt7OnLKz8jaUVXpzb09BUEm9y8Pr7tgE4FXNCkHJu7OyTs2yx/WR85PaQv/aRHX9uwV8m0Lfot/PbX8y+6jpwezpu02xJG11w0XY7VmJ4zt8rc0ZWIjVyweOvU9VLvSW3u6dMrh/LburXSC/v79aGvPKcn/mBFYFu26ta2kLsVprWPPL4Qj0mdU9r0/uvn5fLjLVu/tJXSAcEmN18ut81JCuy/q29dpOVdnYEBp8O+BXnKsdp5+HRBvT68YW/Rb6a8Y4cknSsSPHGs8tqwfx+Uae0teWXsT0/YFZaVdGowmffakdNDevvaZwry6R7Lmw7veV1B49VoH58IGwe9rxfz/9h/1bvPPanJZlgzz7yq30qd1AP6zbz8b9p7XHFT+OeKg/IR1q+9r3sXi+4xzgyl8o591hPMDctjsXbufV/QXBNm2/6TejC7yPWPwTsPnw4cl93zuf9/fNsB9Z0ZaRuJmCmrPoPGWkkl56ugunFNbo1rOOXkzQ1HzgyFBhT8ba/UXQcF83vvSW1//VTe+dzHfcu1MVumYYq1uSBpK20/kJ+mTJC48LG1grneJ+UoLzAqSa2JWC64JSkvIBE0t3oDlN7XwniP4QrqQ0FrGO+44nLH8d7+gYJgk1Q82OQ9n3/MkvLLzJFydxWmhtP68rN71RKP5eU/HtAwgtpn0KOMfv4vA/3rrGJjcdh8m/felNVQwJjvnxfDHh8udeeT/ws6L2+/Clv/FFtH+I8Z1Ka8afKPKfEif67eP1YWlI8tPL5UOLZcfcG0wMcyg/r3rsOnNa29pSBfx88O68jpobLuelm7rlsPPbs39++BpJO7+9c9frFrjCe2Hcg7fzo7Nm/q6csFLr3rPLesgtZBm3v68r7M8Ds3nM6tl7xr/bRj9fgLB2SMCaznwaSjn3mCTe5nHtvam+vDldyRNBIQy+et46D1/zlP/y830OW/O9NbNv5rKjYpH18IOKEqlnd16meegdAqMxEvmtMR+G2S95sd9/EV++TOgm83blows+h+F2G379+0YGbeRbJ7i/z9T+4MfFZ/w+5jOnRyMHBR6A50965cqHTa0dc3v6bhlKPOKa1yrM09puAPdKzffazoRLa8qzNvgPVfsPgXFut3H9Np30VxkI3ONVpk9muSGVbStOk5vVFSduD2TBje/Zj8+4osuXhGbm8H/0LWPZZ7nCAxk/mWxHs3lvcxpaAFctoWtosvPdOjx7KPurmPawUFwLyGUlaOlb59zzKtXdet931xU97jVHFjlHKc0ituD/d296ALfCsV/cZ0ZzYQddOCmXqpt1/u9ZCR1YbdxwIXnkbSexZfpOdfO6Gdh05r0QUd+pf/Ev7oYLniSmtV4jEtj+3QRuca/V36/YpJ6vU9uub239XKtI+HntlTMtCZiBldOWeqXjpwKvfYiJTfv4Me63jdd+5SvG3Yvw+KN0jX3hLTojkdetFT5qHlEjNadEGHXtx/Mm8MO3Im/A44bzqGkkOKx0xevttbYgVjzsdvuTx0jyPv7fULz58iGaPuI2e0aE6Hll4yI/c4TjxmdOOC83T/kzv1ev9AwXmDLI/t0GSTuSicZIa1Iv5zPZAeCTjFTWZsfeXQqYLP+h9VXruuW/+0cV/g2OZtk/7F4rIFM9XtC8pObRtpz2HBuGLt3Pu+sLkmjLsQ94/Bi+Z06MX9/XLPaCTdeOl5en7fibz8WGsLgkT/+OMefW3jPv32jRdr1cqFBY9zbt57XDFJrXHJjbXN7mjTbYsv0lfX7807nn+uSxcZr66YPVWS1c96R+qvPZHpB/5PTW6N68PLLslre8W+2EmlHQVdpvjHA/dzdyyZmxv33Xni/oA/dhAWyHYZWQ0Op3KP17565EzJ8a4lnukLQSXl7Y8HTpwrehw/N8gReneH8seDR7fsz60L3NfK4a9ibx+KG+n1/oHAsnSDH49t7c0LhLmHC1rXuK+FBcK86x7vmJV2bG7NFZiH7Hsdx8m76PbfSTZrSouOnilczxjZvPHFFTNSWyKmKa1xxWMxnRpM6aoLO5RK5z8K7Z0DXG7ZfXtrb9GgnzcPxfK2WsX/8p+7lpKkg/0Duu0f1ufdrVaMf71kfWtW/zjpPvoZto7wtym3XlNpp2DeTzs29E6jmxbMLAgIhPE++u4fW1KOo6svmJa7M+iPbrlcUvA4n/niIqm2hMkLmnrL6NlXj2r1ysul//y0tPcZacHNSv3SX2jt03tyd9eFGUw6mtKSznvtzZfMyJwjWw7HzxauAVZ0deprG/flvbZtf3+ubwatb90vwEqVnzuO+h07PaSHfHd6uYLu0pSko6eHdOT0UOijzWGP5Pu/GPJzA89BbdnbR8o51xMv9OaNE36TWuOa2pYo63FTNBYeqUPk3FtW/QvmhJFmTG5RPBbTyYGkhlKFi18p+FZ4KTNZXjd3ml4/NZS3iDLKDHhXnD9FL72efwFjJH305suUchx9Zf2+3OvtCSNjTF5AY05Hm379ugvz3hfG/fxwysnL46SE0XDayhhp5uRW3fGmebnAyGNb9uduyXdvifXuV3L/kzv1yMa9GkqPpH3W1Fa9903z9CfvvDIv8u8+HuS9EAoTV1r3xr+tX2p5WT81b9Dfpt+vSW1teY8AeMtryfzMY19//6PdeRfCqbSjDz60WdtfP1VwcZUw0vTJLeo7GxwAm9QSfLEzKWH0Wzddou9uO6D+gWTeY0qlLJ43Pe+OlTAxSa0Jo2TalrU4Kod7q7WVcntRlGvJ/Ola1jVLj/keJ5Iy5eS/rV7KXBReOWdqrr68tzB792Q5fi5ZMtjgtTrxLf1+PHOny4Bt1SP23frfyTsLFqVxI9391sv0x++4Qh/68nOBF/Lufh4H+jOPmLz3TXOVdqy+sn5vyQBPuWZ3tMpxrPoHkpKMEjEFPiboiklqb43LWumqCzv08Iev19JP/6hksEzK9HH/sWd3tObdPWeU6cszp7So78xwYPuKKXPruTGZ92XGvswb3UcdJclaq5STed8b506TY/MDBmHiMemNF03TS72ncmPBJM8jmN6LzLZsP1gV/5Y+En9Sk7L1/tXUu7TW+U21JUYuWoP6qzc/bS1xLZw9Va8cyh+nY5KunTdNr58c0unBzDhvbCaw+UuJV/RK+2L969Tf1Wv9QwXtf/bUVr1v6Xytyo433kcUvv6RN+sf/rMn8NGTya1xXX7+VL31ilm5x6jPDAzrlrU/0ZHTw0rEjK6+sEMvHzwdWvdzOto0Z1qb4jGT14Y/9rau3HGCGEmL50/Xgf6B0Pe4dWKz5ere4eG2l8ktsbw7+WKSZk5J6NjZ/IV+e8Kooz2h42eTmccZVBhcjCutT016XEvS27XBuUZrU+9XWvHQdLUnYvrwskv0/L4TueDmgZODubqJmcyjNe4+Po5jAx/JCiqXJfNn6OsfebN+5+Gfatv+/DtRZ09t1YUzJuX2G1m7rjtv/vOLx4zeeGFH3mMipcyaktCJc6mAR+SMDp0gowAAHY5JREFU7lpxqYwxBY9bxY3UOaU1d5dOYFpCLuikTN84f2qbTp4b1lCZE45R5nHSvrPDcmx4gMMdY185eCo3NsWN9F/eskDGmMB9xu784qbcXRDuvLF5b19efczuaNV7Fs/Vc/tOyMjmBdq9ZWKVGZ9+7doL9O8vHdTRM8M6f0qrfvXaC/St53sD5y6p8HFrN8+xIuVYSljQbLbn0dG4kWZOadW88yZp//FzOjuc1lDSyVszlbrTzCvzOFT+HDCpJaZFF3Rkvlj07Q21dl23ntl1WDsOnin7HMUsnj9dr3jWxZNbM48sGSM9vrVXJ84lA9tPIia1xOOa1p7QyYGkBlOZu6TPm9SimDEFbd39Iuo903v02vSl+shr79BAemQMiUlqiWfu/EvbkS8c4zEFruGMpHt+6bLcI2Du+sFfX+0tMX34pkv0073H87ZJ8JvT0aa+s5n25F+rTG6J6a+mPqHbhr6vFmdQNjFJj7f+uv7k+O2lCziAew0hqWCscH9/11su1ZZ9J3J3SrliktpaYpraltDcGZO069ApDWTrLmakzoAga9C6IxHLjFnF1jrl5MP76UktMW1f8868/dHe/4WNBeVebKxzj7Pogg71nhgIXH96t9vwfsE4rT1R8Ih3TCp5PeNKxIw+suLSih7XQ3WU+0jduAk4GWNulfR3kuKSvmKt/Uyx9xNwqp+/+ref6+ENrxV9z+ypxRd0jch/N0iphXwpk1piumJ2h5Z3dWrznmN6sYyLy6iFDfAxZSan8TE6VEfU9R21SibnMN9p/X+1JDayb8o2p0vvGf7rwPeeP7VFx84kQ9vEtXM79Hr/YN63Ye7mq80ks3iPqaM9ERpkahRhbcQNQq+I/1wb0m/Qg+n3VbVt+wObX029K++OKr+gQEGp9j6pJSbHsYqZTBBytNUSjxl9ZMUlen7P8YqCG42i0rKWSl9UNKNq5zkRcvE9pmMaaQzXnCUZSZNa4jqXrOAblArEldYnE/+qO+LrJSs94bxVn03d2VDzahQqCWI1otGMIeW45sKp2nHwTAQpzOdfqy2P7Sh7XROVN17Uoe2vl/8Yd1TKWacumT9dL79+MjSYX/U0jnGsLRacjhtp66ferhlT2kd/AoxJUwWcjDFxSd2SVkrqlfS8pA9aa18O+wwBp/r4ya5D+vDXtnoGwZ/LUUwx2dxgKCl0gPQvSL7jrJBVTMtiL2ce+UndoT9OPJE9rnSlOaBWpbTdLtCHh/9Uj7Ter0Vmv3ba+frt4b/QHyW+kz2W1QGdn5cO/6BcbOBu1bB+0vYJXaDMXzpK2pgeSv2q1qbv9OTTaK45pqka1Bk7SU84b9Xfp27XN1r/VzZN8/S8s0g3xnZqk3OVJKNlsZc9P+/IK6v8vHqP3SZrYpqmAe208/XB4U9pWK156c8/ptFc05ctg1m+n49pqgYkGZ2xk/RdZ5neHHs1V4YfHP6U0orrTxLf1O/En1arkupTRyYgZZUrU29+guo0v4yOaqZOKy6rAbWq286XkXx53qHNzpW6Idadl5ZhtQa2u5F282y2jFo11QxrkoY0oFad1SRZa3ILXEmh7WyTc5WMrH4n/h+arCHFsvXdZzuy+XUK2nRhXvPbUGH6Mu3jb1PvzeU3rB24fWakDjLtvVh5lCOutB5tvU9LzO7sZG60zblcv5n8754+OtJ/i9VHsf7u5r+wbY6U9Xvi63PlkmmD3Xqj2SPJ6ITtUG8FfbecvlUqeDhSX89ops5IMqFlXmlgslXD+mbrX+uNZq+G1aJvpm/Wr8Wf12yd1BFN1y1Dn9WAJvnSMdJu/GNisfMVS1vQ71o1rB+1fTKXlpVD9+sPEt/Pnn9knHDb7icS3/akrU3WGHUqE6RJqkU2+35JuiA2clfcNucybXTeEFpm+fOAUzDepRUPGevy54T89/rb8q5cHXwj/Q79TeoDBWkor005vv8Xtv38OmzPGzdH2n9++brjVLF5dKSNZsbTtIzikuJmZF2XttKA2rLn9Y7/5yumtC4wx9Wp0xpWomAsvjfxqG/ct5qiId+xFJDekXqNK61vtv7P3By4xVmo2+KbAufk0p/PzOt/mPhe0f5Wuu5eLjpnjfTT/6lF5hc6pck6bM9TWvGCevbOWcX6WNBYF76+2VG0TIJeKzWeFc7hVlM0oFallRnfLs2tT7zHHCmH0vNw2Lnd9j9Tp9UiR+5WQdZKScV03E4re1wbyct/aJKGNaC2XP/1l8vfpe7QqsS39JH4D5WQo3MBa4Gwc2SOs93TP1r0z+lb5Cim98Sf9cwLheXmTUdwm9uuK8xBTdKwjmia/i29TL8R3yxZedZgbrubqU3OooK5V1LePPLP6VuUVsI3r24osY7eUdAP/i51h55p/UTeeJ220lE7I6/Mgub8UvNtqbYUNCdJKhmk9AbIRoISRnFjcwGzz6Xfo2+2/pUWZ9cVP7OX6beG/1t2LNmuC8yJ3BwzMsaEvV64vgtP97O+tc2rWmRek1VMRjZXxxudq4uWXX5fWp9bw19penPr1AHboh3OJZoXO5o3n3zHyewllWkPTnae9rdpf1tx05HWBaZPs5XZH+ucb80e3t53+K5b2kteg0kKbCNpxQvmOm+/Tyuu9143U5/94LKAEQjV1mwBp2WS1lhr35n9919IkrX2f4V9pmkCTm97W71TUJHNezLPGs83R3SBOa64bG5fgLSMDtlOSTb3O/e1/fb83OcuNH25PSIyd9kYxbLvPWfbNdkM5h3XfZ+76He/XRpWQi1K5R3Lmw73nC5vmv3vuSa2T1M1kLcJ6rASOmqnF+TT5UhKKqFWpfK+8TJyv6nP5Mv7szeNYXn15sVKOmMnaYe9NC/9Qcf0fs7/s/e4bhrdY5/SZF1k+ooew3u+oDoNKyPvMYLy7E/LDnupgvjbjb+M3HM6kg7aWZJsaDtzsu8P2i7Tf8xiefX+Lih9jqSzdlIuv2HtwO0z/jooVh7lKLePevMsBddHsWO5+Q9qm0Fl7T2P97Vy+245fSvoGP7jBbWnoDIvNm4E8Y8l/nIdVkLbnCuKpsM7JhY7X7G0Bf1ulunPG6/S2THVf8O623anmIHAPufn3i0ZU2E/D8pD0DzgH5PKGev87w1qy+57X7ezCtJQbpsqNi4UG5vCxhrvOFWsj/qPW0rQ2B30mltHU81Awe+CjhWUXjed08zZXHsPKvdSawP/54eVUELpov2tnLorNmdJ+f20WD0Xa8v+dASNdWHrm2JlUmwNFWS+ORI6h3vT4taN95j+cqh03glq/0HKHdf8eXE/+3pA+yvWht02G3aOoDVL2NzkvhZUP+WMHd5j+ueDsLlXRgXzSNgaplg78/eDc7a9YFx3ecssqIxKzbel2lLQnOQf54Lq7ZrYPnWocN9JRzEdsjMLzu3+3juWeMu6nNfD1jfFxuewui2n7NzyCZoX/fku1rf97wn6nVXw/OY/R6n2XupzQX0vqI2c0uTAMcQ/b9902TjaRPzHP653CiJTbsBpvDz4OFfSfs+/e7Ov5THG3G2M2WKM2XL06NGaJQ6FpplzimeHMnfAictqmjmb9zv3Ne/nvI3SSIp53jvZDBUcd+R9Tu41I6nVE2wKSkexNPvfM1lDgRdSQfl0xbJp8KbJeH7n5sv7szeNYXn1/ttImmyGCtISdMxiP3tf8x97mjlX8hje8wXVaVg+vK8F5Tkon0H87cZ7XO85Y1KuDYa1s1hIOoOOWSyv3t8FpS+WzVOxOvP2GX+5FyuPcpTbR4Par//8xY7l/72/7Qe17bD2Xk7fLadvBR3Df7yg9hRU5sXGjSD+scRfrq0aeSQxLB1hfa2StAX9zj9exQOCTdJI2w3rc35GUkoJndakzALVqGiZBfVP9//umFTOWOd/b1Bbdv8dlIZy21SxcaHY2BQ21njHqWJ9tNIFXNDYHfSaOxYXmy/CxlV/vXrbe1C5+/NU6vOtSpXsb+XUXbE5S1LBeYP+75+zivWxsLEubH1TrEwqHXOKzeHe14Lar78cKp13ym2n5Y5r/ry4nw0ql2Jt2G2zYecIW2cWm6+C6qecsaPYfBD02mQzFDiPhM2rxdqZvx8Ejesub5kFlVGp+bZUWwpr6970BNXbKTtZaV/NuP9ygxFB5eUdS8LGmHLGnnLS7R4jbCwpZ60SNi+6goJN/vP6Pxf0u7D5zf/5Uu290nWcFNxGwsaQoHkbjaup/kqdtfYhSQ9JmTuc6pycaIyzKOgH/vz/k5S5xdXdlNZayRhlNiVOvUvG2LwNax/xPB++OvEt3R3/d7WYkb9S4CimFuNowLZqh3OJrom9lndcKXNr9lm1aYqGMpupWumQPU+zzMncsfzp8D+T7k2z/z2Pt/53vcnszp3PsdKXU78uGVOQT1fSxnTMTtcF5kQuTY4yt/kmbWb4bDFO3s/eNIbl1ZsXa6VtzhX6QPK+vPQHHdP7Of/PLsdmJyDPsZ+zV+me+PdzG90GHcN7vqA6DSsjf71485y2IxsUe/MZxN9u/GXknjNpY3ok9esyxoa2s6TN3NXhza9bb/5jFsur93dB6UvamH7mdOXyG9YO3D7jr4Ni5VGOcvuo+7ti9VHsWG7+g9qmv6yl/Dbor8ty+m45fSvoGP7jBbWnoDIvNm4E8Y8l7k3GbrketOfpA8OfCU2Hf0wsdr5iaQv63XsTP8k9NmytdEZtalcy7/zSSNu9LtYT2Of8Mo8g/3rJfhJW/v7x7jl7VVljnf+9QW05U6ZGj6R+oyAN5bapYuNCsbEpqP275eWOU8X6qP+4XsXGWn+Z+l9zx+IlsVcDx33/v4PS66ZzWXxHrr1bm73TLeB8YWsD/+cP2fM0w5wt2t/Kqbtic5aU30+L1bN3zirWx4Lqutj6pliZFFtDBVmd+FboHO6tT3ee8x7TXw6VzjvF2r9XueOaPy+Zz2b6r79cdjiX6E2xVwvamzTSZsPOEbRmcTfkjvv6gb/cvOkoZ+yQ8s/hXYOFzb3G2II1aTpkDVOsnfn7QbF1p7fMgsqo1Hxbqi0FzQv+cS6o3tw9CX8v8WTeNYD3+EHzrncs8dZHOa+HrW+Kjc9BdVtu2bnlEzQvusc+7LvmcbmPGQb1f3+b9raVoDnC+/lS7b3U54L6XlAbec5eFTjX+eftfZ/51YLjoXHE16xZU+80lHTfffdNl3TbmjVrvpH9969KOr1mzZr1YZ956KGH1tx99921SiKylsyfpu++eFDPOVepTUm1mKQO2Fk6rPP0vfQKPZh+nzY512R/l8q9ZrPx6+ecq9SqpC42R3TGtusb6bdrq3OlWkxa30uv0J+m7larUtnjztQ0Zf6c8YtOl35t+NO6MbZT081ZveRcptuTf6W4nOyx2rTLzs9Lh/XFzEfSXJiu76ZXaHns55plTmpAbXo4das+m/5NT16SOmA7FVNastJx26FvpN+hj6U+rptir2TTtED/ll6uhEnru+kV2uosVEvez6m8ssrPq/fYU3RakxWT1UvOZfpQ8i+VVjwv/fnH7FRMTrYM5ikm6/nZkazVsE1k0/zLMlKuDD+U/EttdN6gVg3rKvMLSVZH7HSdte15ZerNT1Cd5pdRSi0285c5zqlVP3cWBOQ5pe+mlysmm5eWsH0qRtrN4WwZTZYUU0IpnbVt6rcd2fb0Dj2Yfr82OdeEtrNMXq7QfHNUZ2y7/k/67driXOmpnxkFbbowr/ltqDB9mfbxp6mPevIb3A7cPjNSB5n2/qHkp4ru21FKeX105HfF6qPYsdz8F7ZNb1kf8ZTLL8vIapY5qZRi6rPTtLOCvltO3wo6RnB7OqQWm5KjWGiZFxs3gvjHkkdSt+gic1yTNaRD9jy9ffgBpdTiS4e33eSPicXOVyxtQb/7RvoduiP+bC4tbxteq5hs9vzecSLTdluU8qRtik5rkto0rJSMBm1L3vsfTL+/ZD8pLP8jOmNbC8a7jc4bQsa6/DnhQ8lPed7rb8uOZplTGlCbvpa6VWvTdxakobw2NaPouFBYh1N946bb/m1BeQX1w/wxwW2jSUlWScU0aFuz5/CP71N94/98HdZ0WUltSmogYCxuUdI37rd58uAeqz0gvSP1+kT6rVoe2+GZA2/SPHM0cE4u/fnMvJ6QU7S/la67dNE5a6Sf7tB0c0bH7DT12At1wJ5fUM/eOatYHwse68LWN6miZVJsDRU+nvnn8DbJOrLWZMe3y3LrE+8xR8qh9Dwcfm5/+w9ae5Q3ro3k5TXF5Oicp//6yyXThod0rdmrmGzgWiDsHJnjDHv6R5u+lvoVbXEWar457JkXCsvNm47gNjesqRpUQmkdsjP0zfTbNM8cy6bLXYO57e6iwLn38fTNefNIJm3Ba5hi7czfD8LXnfnjeNCcX2q+LdWWguaFwrVaYb1ZxbTRvkFfSb879PiZc2/XbNMvq8x+lbcl/zo7lgzLfYwsf4wJe71wfRee7sO+tY003ZzWkM08gzFSx6XXKvnzYlveGPz19Erdk7rXMyf7+9bCbHtozc7T/jbtbytuOqbLymqSMnejnfXNE+HtPeVrP1NLXoON1NOOgGsQ71ynvH5vFdN7r5upd75xftnjEqJz3333HVyzZs1Dpd43XvZwSiizafjbJR1QZtPwD1lrd4R9pmn2cAIAAAAAAGgQ5e7hNC4eqbPWpowxfyjpKUlxSQ8XCzYBAAAAAACgfsZFwEmSrLU/kPSDeqcDAAAAAAAAxY2Xv1IHAAAAAACAcYKAEwAAAAAAACJFwAkAAAAAAACRIuAEAAAAAACASBFwAgAAAAAAQKQIOAEAAAAAACBSBJwAAAAAAAAQKQJOAAAAAAAAiBQBJwAAAAAAAESKgBMAAAAAAAAiRcAJAAAAAAAAkSLgBAAAAAAAgEgRcAIAAAAAAECkCDgBAAAAAAAgUgScAAAAAAAAECkCTgAAAAAAAIgUAScAAAAAAABEioATAAAAAAAAIkXACQAAAAAAAJEi4AQAAAAAAIBIEXACAAAAAABApAg4AQAAAAAAIFIEnAAAAAAAABApAk4AAAAAAACIlLHW1jsNVWGMOSrptXqnYwKbJelYvRMBoAB9E2hM9E2g8dAvgcZE36y/S6y155d6U9MGnFBfxpgt1tql9U4HgHz0TaAx0TeBxkO/BBoTfXP84JE6AAAAAAAARIqAEwAAAAAAACJFwAnV8lC9EwAgEH0TaEz0TaDx0C+BxkTfHCfYwwkAAAAAAACR4g4nAAAAAAAARIqAEwAAAAAAACJFwAmRM8bcaozZZYzZbYz583qnB2hGxph9xpjtxpgXjTFbsq/NNMasM8a8mv3/ednXjTHmc9k++ZIx5k2e43w4+/5XjTEf9rx+ffb4u7OfNbXPJdD4jDEPG2OOGGN+7nmt6n0x7BwAQvvlGmPMgey8+aIx5t2e3/1Fto/tMsa80/N64JrWGLPAGPNc9vV/Nca0Zl9vy/57d/b3l9Ymx8D4YIyZb4z5T2PMy8aYHcaYP86+zrzZpAg4IVLGmLikz0t6l6SrJX3QGHN1fVMFNK1fttYuttYuzf77zyU9ba29QtLT2X9Lmf54Rfa/uyV9QcpMvJL+h6QbJd0g6X94Jt8vSPqvns/dWv3sAOPSP6mwf9SiL4adA0Bwv5SkB7Pz5mJr7Q8kKbtO/YCka7Kf+UdjTLzEmvZ/Z491uaQTku7Kvn6XpBPZ1x/Mvg/AiJSkT1prr5Z0k6SPZfsV82aTIuCEqN0gabe1do+1dljStyTdVuc0ARPFbZIeyf78iP5ve/cfqmdZx3H8/cGtXE7NNCU0W65Sw3JZhNUckrX6qxZImqSmqGUauEwIE6xgUEQ/CFPBUtN0o5mWCKKLfrjMZf7csqWUFk7WFq02y7Tavv1xX0efnZ3n7Jz1nB085/2Cce7ne1/XfV8P49p973uuH7CoJ35ddVYBL0/yKuB9wIqq2lRVfwNWAO9v5/apqlXV7SxxXc+1JPWoqruATcPCu6Mv9ruHNO316Zf9fBBYVlXPVdUTwO/p3mdHfKdtoyXeDdzU6g/v40P98ibgBEcISy+oqvVV9UA7fhpYCxyMz80py4STBu1g4Mmez+taTNJgFXBnkvuTnNNiB1XV+nb8Z+CgdtyvX44WXzdCXNLY7I6+2O8ekvo7v03LubpnNMR4++X+wN+r6r/D4ttdq53f3MpLGqZNOX0L8Ct8bk5ZJpwk6cVpflUdQzfU+LwkC3pPtt/q1KS0TNLzdkdftL9LY3IFMBeYB6wHvjq5zZGmrySzgR8AF1TVlt5zPjenFhNOGrSngFf3fD6kxSQNUFU91X5uBG6hG/q/oQ0lpv3c2Ir365ejxQ8ZIS5pbHZHX+x3D0kjqKoNVbW1qrYBV9E9N2H8/fKvdNN6ZgyLb3etdn7fVl5Sk2QmXbLphqq6uYV9bk5RJpw0aL8GXt9273gJ3SKMt05ym6QpJcleSfYeOgYWAr+h62tDu3ScDvyoHd8KnNZ2+jgW2NyGFN8BLEyyX5tasBC4o53bkuTYtvbEaT3XkrRzu6Mv9ruHpBEM/Uez+RDdcxO6vnRy22HutXSLDN9Ln3faNjLip8CJrf7wPj7UL08EftLKS6LbdQ74DrC2qr7Wc8rn5hQV/w3UoKXbZvYbwB7A1VW1ZJKbJE0pSQ6jG9UEMAO4saqWJNkf+D5wKPAn4MNVtak9cC+j26XjGeCMqrqvXetM4OJ2rSVVdU2Lv41ul59ZwO3Ap3xplnaUZClwPHAAsIFu15wfMsF9sV9/n/AvLL0I9OmXx9NNpyvgj8DHh9ZzSfI54Ey6HbQuqKrbW3zEd9r2HF4GvAJ4EPhoVT2XZE/gerp1aTYBJ1fV4xP/jaUXhyTzgZXAGmBbC19Mt46Tz80pyISTJEmSJEmSBsopdZIkSZIkSRooE06SJEmSJEkaKBNOkiRJkiRJGigTTpIkSZIkSRooE06SJEmSJEkaKBNOkiRpSkuyNclDSR5J8nCSC5OM+g6UZE6SU3ZD276d5I07KbOoX5kkn0hy2jjv+bO2bbQkSdKEmTHZDZAkSZpg/6qqeQBJDgRuBPYBLh2lzhzglFZ2wlTVWWMotgi4DfjtCPWvHHijJEmSBsARTpIkadqoqo3AOcD56cxJsjLJA+3PO1vRLwHHtZFRi0cp97xW5ndJbkiyNslNSV7Wzp2Q5MEka5JcneSlLf78aKMk/0iypI3CWpXkoHafDwBfaW2ZO+yen0/ymZ5rfTnJvUkeS3Jci89Ksqy16RZgVk/9hUnuad9peZLZSfZN8miSw1uZpUnOHuhfhCRJmvJMOEmSpGmlqh4H9gAOBDYC762qY4CTgG+2Yp8FVlbVvKr6+ijlhjscuLyqjgS2AJ9MsidwLXBSVb2JboT5uSPU3QtYVVVHA3cBZ1fVL4FbgYtaW/6wk683o6reDlzACyO4zgWeaW26FHgrQJIDgEuA97TvdR/w6araDJwPXJvkZGC/qrpqJ/eVJEnajgknSZI0nc0ErkqyBlgO9FtPaazlnqyqu9vx94D5dEmoJ6rqsRb/LrBghLr/pps6B3A/3bS+8bp5hPoLWluoqtXA6hY/lu573J3kIeB04DWt3ApgDfAtYCzT/iRJkrbjGk6SJGlaSXIYsJVu1NKlwAbgaLpfxD3bp9riMZarnXwezX+qaqj8VnbtPe25cdQPsKKqPrLDiW5R9SOBZ4D9gHW70BZJkjSNOcJJkiRNG0leCVwJXNaSO/sC66tqG3Aq3VQ7gKeBvXuq9is33KFJ3tGOTwF+ATwKzEnyuhY/Ffj5OJo9vC3jdVdrC0mOAt7c4quAdw21K8leSd7Qzi0G1rZ61ySZ+X/cX5IkTUMmnCRJ0lQ3qy24/QjwY+BO4Avt3OXA6UkeBo4A/tniq4GtbQHvxaOUG+5R4Lwka+lGBl1RVc8CZwDL25S8bXRJr7FaBlzUFh2fu9PSO7oCmN3a9EW66XZU1V+AjwFLk6wG7gGOaIuFnwVcWFUr6RJWl+zCfSVJ0jSWF0ZuS5IkaVclmQPcVlVHTXJTJEmSJp0jnCRJkiRJkjRQjnCSJEmSJEnSQDnCSZIkSZIkSQNlwkmSJEmSJEkDZcJJkiRJkiRJA2XCSZIkSZIkSQNlwkmSJEmSJEkD9T9Wm1FLM0D3jwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1440x864 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "threshold = 1\n",
    "\n",
    "groups = error_df.groupby('true_class')\n",
    "fig, ax = plt.subplots(figsize=(20, 12))\n",
    "\n",
    "for name, group in groups:\n",
    "    ax.plot(group.index, group.reconstruction_error, marker='o', ms=3.5, linestyle='',\n",
    "            label= \"Fraud\" if name == 1 else \"Normal\")\n",
    "ax.hlines(threshold, ax.get_xlim()[0], ax.get_xlim()[1], colors=\"r\", zorder=100, label='Threshold')\n",
    "ax.legend()\n",
    "\n",
    "plt.title(\"Reconstruction error for different classes\")\n",
    "plt.ylabel(\"Reconstruction error\")\n",
    "plt.xlabel(\"Data point index\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\n",
      "Confusion Matrix:\n",
      "True Negative = 184891\n",
      "False Negative = 326\n",
      "True Positive = 51\n",
      "False Positive = 14732\n",
      "f1 = 0.007\n",
      "ROC AUC score 0.531\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score, precision_recall_fscore_support\n",
    "\n",
    "y_pred = [1 if e > 0.5 else 0 for e in error_df.reconstruction_error.values]\n",
    "\n",
    "conf_matrix = confusion_matrix(error_df.true_class, y_pred)\n",
    "f1 = f1_score(error_df.true_class, y_pred)\n",
    "\n",
    "rtb_confusion_matrix(error_df.true_class, y_pred)\n",
    "print(\"f1 = %0.3f\" % f1)\n",
    "\n",
    "print(\"ROC AUC score %0.3f\" % roc_auc_score(error_df.true_class, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
