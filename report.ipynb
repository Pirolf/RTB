{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing a metric\n",
    "\n",
    "I first looked at class ditribution. They are binary classes, and the dataset is highly imbalanced because the true class makes up only ~0.19% of all labels.\n",
    "\n",
    "I suspected if I just run a simple logistic regression on the dataset, I could achieve 99.81% accuracy, because the model can simply cheat by classifying every sample as 0. I ran logistic regression using sklearn with default params, and the result confirmed my hypothesis - the model predicted every sample as 0.\n",
    "\n",
    "Obviously, accuracy is a bad metric for such an imbalanced dataset, especially in the RTB context, because being able to find samples that likely will result in a click-through is more important than overall correctness. If the model predicts the majority label for every sample, precision will be perfect but recall will be 0. If the model predicts the minority label for every sample, the recall will be perfect but precision will be 1. One potential metric I can use is the f1 score, but it states that precision and recall are equally important, which in reality may not be true. If capturing most of the click-throughs are more important than serving efficiency (most of the served impressions result in click-throughs, in other words, there is no infrastructure bottlenck), then recall is more important than precision. On the other hand, if serving efficiency is more important than capturing most of click-throughs, which may be true when the scale is large, then precision is more important. But in either case, f1 score can be very low even if we are happy with the model.\n",
    "\n",
    "Without making too much an assumption of what we would want in real life, ROC AUC is a better metric to use because it depicts the tradeoff between specificity and senstivity. A large ROC AUC value indicates the model is more robust, and the optimal value should approach 1.\n",
    "\n",
    "### Baseline models\n",
    "\n",
    "#### Logistic Regression\n",
    "I reran the default logistic regression model and print ROC AUC, f1 score and the confusion matrix. I use the binary labels as predicted values rather than scores, because I didn't want ROC AUC to be skewed by precision, and ultimatly I care more about the repdiced labels than probablities\n",
    "```\n",
    "================================\n",
    "Confusion Matrix:\n",
    "True Negative = 199623\n",
    "False Negative = 377\n",
    "True Positive = 0\n",
    "False Positive = 0\n",
    "================================\n",
    "f1 score = 0.000\n",
    "================================\n",
    "ROC AUC Score = 0.500\n",
    "```\n",
    "F1 score is coerced to 0 because there are no predicted positives at all. ROC AUC score is low - just as good as a random classifier. Just for fun, I added l2 regularization `penalty='l2'`, the result is the same. The imbalance is just to strong for the model to even try to predict positives.\n",
    "\n",
    "#### Neural Net\n",
    "The baseline neural net has 1 hidden layer that outputs 88 weights for all features, and an output layer with 2 outputs, one probability for each class, with a softmax activation. Categorical crossentropy is used as loss.\n",
    "\n",
    "The model is fit with 3 epochs, and a batch size of 32. The baseline neural net should be equivalent of a logisc regression, except that I'm using the SGD optimizer, so I picked a learning rate of 0.01 (it usually worked well for me when I train baseline models). I specifically set the inital weights as `Ones`, because Keras defaults the inital weights to `glorot_uniform`, which generally works really well (and too well for a baseline model).\n",
    "\n",
    "I used set split to 0.2 for validation after each epoch.\n",
    "Both training and validation error went down, and validation error is slightly smaller than training error in all epochs, indicating there is no overfitting. \n",
    "\n",
    "Unsurprisingly, the accuracy is still 99.81%, with the vast majority of samples predicted negative. ROC AUC score is still 0.5.\n",
    "\n",
    "The result:\n",
    "```\n",
    "================================\n",
    "Confusion Matrix:\n",
    "True Negative = 199619\n",
    "False Negative = 377\n",
    "True Positive = 0\n",
    "False Positive = 4\n",
    "================================\n",
    "f1 score = 0.000\n",
    "================================\n",
    "ROC AUC Score = 0.500\n",
    "```\n",
    "\n",
    "Because I set the initial weights to `Ones`, I wanted to eliminate the possibility that my weights are exploding or diminished. After checking the hidden layer's output weights, they are all between -1 and 1. So my baseline neural net is fine, even if I remove my initial weights so that they are set to `glorot_uniform`, the result is similar, there is one true positive and more false positives\n",
    "```\n",
    "================================\n",
    "Confusion Matrix:\n",
    "True Negative = 199418\n",
    "False Negative = 376\n",
    "True Positive = 1\n",
    "False Positive = 205\n",
    "================================\n",
    "f1 score = 0.003\n",
    "================================\n",
    "ROC AUC Score = 0.501\n",
    "```\n",
    "\n",
    "### Model improvements\n",
    "I think the main obstacle to achieving higher ROC AUC is the severe data imbalance. There are a few potential migations I can try:\n",
    "\n",
    "- Deepen the neural network model\n",
    "  <p>\n",
    "    If add more layers, the model will have a chance to explore nonlinearty, which might cover the positive space.\n",
    "  </p>\n",
    "- Down sampling\n",
    "  <p>\n",
    "    The negative class can be overpowering. We could find a good class ratio so that the model can be trained for positive classes without underfitting the negative class. A side benefit is the model will take less time to train.\n",
    "  </p>\n",
    "- Over sampling\n",
    "  <p>\n",
    "    Like down sampling, the goal is to make the data more balanced. If we observe underfitting for the negative class, we can try oversampling the positive class and find a good class ratio. Oversampling can be done with random selection and duplication. This could cause overfitting for the positive class, and definitely slows down training and can increase model size.\n",
    "  </p>\n",
    "- SMOTE\n",
    "  <p>\n",
    "    We can also try a hybrid approach, down sampling negative class while generating synthetic samples for the positive class eith nearest neighbours. This could help prevent positive class overfitting. But just like oversampling, it causes the model to train slower and the size to increase.\n",
    "  </p>\n",
    "  \n",
    "  \n",
    "#### Deepen the neural network model\n",
    "I added one more hidden layer with 64 outputs, without any regularization, and left the initial weights to `glorot_uniform`. Both training and validation lossses are smaller (around 0.015 v.s. 0.030 in the baseline neural net). However, there is no improvement in metrics. The vast majority of predicted clases are still negative, and ROC AUC remains 0.5. The validation error also failed to decrease while training error did. This indicates overfitting, so I added `l2(0.01)` to the first hidden layer. This time validation error decreased with training error, but there's no improvement metrics, in fact, the model predicts all negatives again. In effect, the extra layer only enabled the model to explore some nonlinearity and as a result the error declined, but it heavily benefits the negative error, as the negative class severly outweighs the positive class.\n",
    "\n",
    "I tried to tune learning rate (0.1, 0.001, 0.0001) and batch size (64, 128, 256) on different scales and tried different weight initializers (random normal, he_normal, etc). There is no improvement. Adding yet another layer didn't help either, because as expected, the reduced loss benefits the negative class much more than the positive class.\n",
    "\n",
    "Since the imbalance is preventing the model to train for positives, I set used weighted loss to balance the classes by setting `class_weight` (roughly 1:526) in the `fit` function. The losses are huge, and validation error plateaued, and the model again tried to predict all negatives. I probably overshot the class weight, so I adjusted to 1:100, and added l2(0.01). There were more false positives, but there was still overfitting, so I changed l2 to l1 in the first layer, in an attempt to reduce features. This helped. I have more true positives, but also more false positives, but the ROC AUC is 0.541!\n",
    "\n",
    "```\n",
    "================================\n",
    "Confusion Matrix:\n",
    "True Negative = 190136\n",
    "False Negative = 328\n",
    "True Positive = 49\n",
    "False Positive = 9487\n",
    "================================\n",
    "f1 score = 0.010\n",
    "================================\n",
    "ROC AUC Score = 0.541\n",
    "```\n",
    "\n",
    "There's still a little overfitting, so I added a `Dropout` layer between the two hidden layers, with rate = 0.1. However, this nudged the model back to all negatives again. I tried to tune the dropout rate and although it decreases losses when the rate is 0.05, it also prevented the mode from predicting positives, probably because most of the loss it regulated is from negatives. I also tuned the learn rate, decay, and tried out RMSprop. It doesn't seem I had a problem with decreasing loss.\n",
    "\n",
    "I figured I could keep adding layers and tuning the extra hyper parameters, but I decided to explore the sampling route, as my mode will be exponentially slower and harder to tune as trained parameters hyper parameters increase.\n",
    "\n",
    "#### Undersampling\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ytu/.local/share/virtualenvs/rtb-bjTeWfqO/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from imblearn.under_sampling import NearMiss, RandomUnderSampler\n",
    "from imblearn.combine import SMOTEENN,SMOTETomek\n",
    "from imblearn.ensemble import BalanceCascade, EasyEnsemble\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import h5py\n",
    "import keras\n",
    "from sklearn.utils import class_weight\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import *\n",
    "from keras.regularizers import *\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, f1_score, precision_recall_fscore_support\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "KFOLD_SEED = 42\n",
    "\n",
    "\n",
    "def rtb_confusion_matrix(test_labels, y_preds):\n",
    "    m = confusion_matrix(test_labels, y_preds)\n",
    "    \n",
    "    print(\"================================\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(\"True Negative = %d\" % m[0][0])\n",
    "    print(\"False Negative = %d\" % m[1][0])\n",
    "    print(\"True Positive = %d\" % m[1][1])\n",
    "    print(\"False Positive = %d\" % m[0][1])\n",
    "\n",
    "\n",
    "def rtb_f1_score(test_labels, y_preds):\n",
    "    f = f1_score(test_labels, y_preds)\n",
    "    print(\"================================\")\n",
    "    print(\"f1 score = %0.3f\" % f)\n",
    "\n",
    "\n",
    "def print_metrics(true_labels, y_preds, y_scores, is_train=True):\n",
    "    if is_train:\n",
    "        print(\"--------train---------\")\n",
    "    else:\n",
    "        print(\"--------test---------\")\n",
    "    \n",
    "    rtb_confusion_matrix(true_labels, y_preds)\n",
    "    rtb_f1_score(true_labels, y_preds)\n",
    "    print(\"================================\")\n",
    "    print(\"ROC AUC Score = %0.3f\" % roc_auc_score(true_labels, y_scores.argmax(axis=-1)))\n",
    "    \n",
    "def keras_print_metrics(true_labels, y_scores, is_train=True):\n",
    "    y_preds = y_scores.argmax(axis=-1)\n",
    "    \n",
    "    if is_train:\n",
    "        print(\"--------train---------\")\n",
    "    else:\n",
    "        print(\"--------test---------\")\n",
    "    \n",
    "    rtb_confusion_matrix(true_labels, y_preds)\n",
    "    rtb_f1_score(true_labels, y_preds)\n",
    "    print(\"================================\")\n",
    "    print(\"ROC AUC Score = %0.3f\" % roc_auc_score(true_labels, y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000000, 89)\n"
     ]
    }
   ],
   "source": [
    "input_path = '~/data/biddings.csv'\n",
    "data = pd.read_csv(input_path)\n",
    "print(data.shape)\n",
    "\n",
    "train = data[:800000]\n",
    "test = data[800000:]\n",
    "\n",
    "sample = train.sample(frac=1)\n",
    "features = sample.drop('convert', axis=1).values\n",
    "labels = sample.convert.ravel()\n",
    "categorical_labels = to_categorical(labels, 2)\n",
    "\n",
    "test_features = test.drop('convert', axis=1).values\n",
    "test_labels = test.convert.ravel()\n",
    "categorical_test_labels = to_categorical(test_labels, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear](200000, 2) (200000,)\n",
      "--------test---------\n",
      "================================\n",
      "Confusion Matrix:\n",
      "True Negative = 199623\n",
      "False Negative = 377\n",
      "True Positive = 0\n",
      "False Positive = 0\n",
      "================================\n",
      "f1 score = 0.000\n",
      "================================\n",
      "ROC AUC Score = 0.500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.local/share/virtualenvs/RTB-V2Lvgo6A/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(penalty='l2', random_state=KFOLD_SEED, verbose=2)\n",
    "\n",
    "model = lr.fit(features, labels)\n",
    "predicted_scores = model.predict_proba(test_features)\n",
    "predicted_labels = model.predict(test_features)\n",
    "print(predicted_scores.shape, predicted_labels.shape)\n",
    "\n",
    "print_metrics(test_labels, predicted_labels, predicted_scores, is_train=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_11 (Dense)             (None, 88)                7832      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 2)                 178       \n",
      "=================================================================\n",
      "Total params: 8,010\n",
      "Trainable params: 8,010\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 640000 samples, validate on 160000 samples\n",
      "Epoch 1/3\n",
      "640000/640000 [==============================] - 39s 61us/step - loss: 0.0321 - acc: 0.9964 - val_loss: 0.0300 - val_acc: 0.9970\n",
      "Epoch 2/3\n",
      "640000/640000 [==============================] - 39s 61us/step - loss: 0.0279 - acc: 0.9972 - val_loss: 0.0300 - val_acc: 0.9970\n",
      "Epoch 3/3\n",
      "640000/640000 [==============================] - 40s 62us/step - loss: 0.0279 - acc: 0.9972 - val_loss: 0.0300 - val_acc: 0.9970\n",
      "200000/200000 [==============================] - 4s 19us/step\n",
      "--------train---------\n",
      "================================\n",
      "Confusion Matrix:\n",
      "True Negative = 199418\n",
      "False Negative = 376\n",
      "True Positive = 1\n",
      "False Positive = 205\n",
      "================================\n",
      "f1 score = 0.003\n",
      "================================\n",
      "ROC AUC Score = 0.501\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(88, input_shape=(88,)))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=SGD(lr=0.01),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(features, categorical_labels,\n",
    "                    batch_size=32,\n",
    "                    epochs=3,\n",
    "                    callbacks=[keras.callbacks.EarlyStopping()],\n",
    "                    validation_split=0.2,\n",
    "                    verbose=1)\n",
    "\n",
    "y_scores = model.predict(test_features, verbose=1)\n",
    "\n",
    "keras_print_metrics(test_labels, y_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deeper neural nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_28 (Dense)             (None, 88)                7832      \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 64)                5696      \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 13,658\n",
      "Trainable params: 13,658\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 640000 samples, validate on 160000 samples\n",
      "Epoch 1/3\n",
      "640000/640000 [==============================] - 46s 72us/step - loss: 0.8766 - acc: 0.9896 - val_loss: 0.5066 - val_acc: 0.9982\n",
      "Epoch 2/3\n",
      "640000/640000 [==============================] - 45s 70us/step - loss: 0.5423 - acc: 0.9918 - val_loss: 0.5224 - val_acc: 0.9972\n",
      "200000/200000 [==============================] - 5s 23us/step\n",
      "--------train---------\n",
      "================================\n",
      "Confusion Matrix:\n",
      "True Negative = 199437\n",
      "False Negative = 375\n",
      "True Positive = 2\n",
      "False Positive = 186\n",
      "================================\n",
      "f1 score = 0.007\n",
      "================================\n",
      "ROC AUC Score = 0.502\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(88, kernel_regularizer=l1(0.01), input_shape=(88,)))\n",
    "model.add(Dense(64, kernel_regularizer=l2(0.01)))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=SGD(lr=0.01),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(features, categorical_labels,\n",
    "                    batch_size=32,\n",
    "                    class_weight={0:1, 1:100},\n",
    "                    epochs=3,\n",
    "                    callbacks=[keras.callbacks.EarlyStopping()],\n",
    "                    validation_split=0.2,\n",
    "                    verbose=1)\n",
    "\n",
    "y_scores = model.predict(test_features, verbose=1)\n",
    "\n",
    "keras_print_metrics(test_labels, y_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = model.get_layer(name='dense_10').get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Undersampling / Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]Sampler 0\n",
      "--------test---------\n",
      "================================\n",
      "Confusion Matrix:\n",
      "True Negative = 199510\n",
      "False Negative = 376\n",
      "True Positive = 1\n",
      "False Positive = 113\n",
      "================================\n",
      "f1 score = 0.004\n",
      "================================\n",
      "ROC AUC Score = 0.501\n",
      "[LibLinear]"
     ]
    }
   ],
   "source": [
    "rus = RandomUnderSampler(ratio={0: 1531*10, 1: 1531}, random_state=KFOLD_SEED)\n",
    "ros = RandomOverSampler(ratio={0: 798469, 1: 1531*10}, random_state=KFOLD_SEED)\n",
    "smote = SMOTE(n_jobs=-1, random_state=KFOLD_SEED)\n",
    "\n",
    "samplers = [rus, ros, smote]\n",
    "\n",
    "for i, sampler in enumerate(samplers):\n",
    "    X_res, y_res = sampler.fit_sample(features, labels)\n",
    "    lr = LogisticRegression(penalty='l2', random_state=KFOLD_SEED, verbose=2)\n",
    "    model = lr.fit(X_res, y_res)\n",
    "    \n",
    "    predicted_scores = model.predict_proba(test_features)\n",
    "    predicted_labels = model.predict(test_features)\n",
    "        \n",
    "    print(\"Sampler %d\" % i)\n",
    "    print_metrics(test_labels, predicted_labels, predicted_scores, is_train=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression():\n",
    "    lr = LogisticRegression(penalty='l2', random_state=KFOLD_SEED, verbose=2)\n",
    "    return lr\n",
    "    \n",
    "\n",
    "def ensembler_test(classifier_fn, ensemblers):\n",
    "    rus = RandomUnderSampler(ratio={0: 1531*10, 1: 1531}, random_state=KFOLD_SEED)\n",
    "    X_us, y_us = rus.fit_sample(features, labels)\n",
    "    \n",
    "    for i, e in enumerate(ensemblers):\n",
    "        print(\"fitting sample\")\n",
    "        X_res, y_res = e.fit_sample(X_us, y_us)\n",
    "        print(X_res.shape, y_res.shape)\n",
    "        clf = classifier_fn()\n",
    "        print(\"training\")\n",
    "        \n",
    "        for j, X_train in enumerate(X_res):\n",
    "            model = clf.fit(X_train, y_res[j])\n",
    "        \n",
    "        predicted_scores = model.predict_proba(test_features)\n",
    "        predicted_labels = model.predict(test_features)\n",
    "        \n",
    "        print(\"Ensembler %d\" % i)\n",
    "        print_metrics(test_labels, predicted_labels, predicted_scores, is_train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EasyEnsemble and decision tree are consistenly the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting sample\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.local/share/virtualenvs/RTB-V2Lvgo6A/lib/python3.5/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/root/.local/share/virtualenvs/RTB-V2Lvgo6A/lib/python3.5/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/root/.local/share/virtualenvs/RTB-V2Lvgo6A/lib/python3.5/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/root/.local/share/virtualenvs/RTB-V2Lvgo6A/lib/python3.5/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/root/.local/share/virtualenvs/RTB-V2Lvgo6A/lib/python3.5/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/root/.local/share/virtualenvs/RTB-V2Lvgo6A/lib/python3.5/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/root/.local/share/virtualenvs/RTB-V2Lvgo6A/lib/python3.5/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/root/.local/share/virtualenvs/RTB-V2Lvgo6A/lib/python3.5/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/root/.local/share/virtualenvs/RTB-V2Lvgo6A/lib/python3.5/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/root/.local/share/virtualenvs/RTB-V2Lvgo6A/lib/python3.5/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/root/.local/share/virtualenvs/RTB-V2Lvgo6A/lib/python3.5/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/root/.local/share/virtualenvs/RTB-V2Lvgo6A/lib/python3.5/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/root/.local/share/virtualenvs/RTB-V2Lvgo6A/lib/python3.5/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/root/.local/share/virtualenvs/RTB-V2Lvgo6A/lib/python3.5/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/root/.local/share/virtualenvs/RTB-V2Lvgo6A/lib/python3.5/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/root/.local/share/virtualenvs/RTB-V2Lvgo6A/lib/python3.5/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/root/.local/share/virtualenvs/RTB-V2Lvgo6A/lib/python3.5/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/root/.local/share/virtualenvs/RTB-V2Lvgo6A/lib/python3.5/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/root/.local/share/virtualenvs/RTB-V2Lvgo6A/lib/python3.5/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/root/.local/share/virtualenvs/RTB-V2Lvgo6A/lib/python3.5/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/root/.local/share/virtualenvs/RTB-V2Lvgo6A/lib/python3.5/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/root/.local/share/virtualenvs/RTB-V2Lvgo6A/lib/python3.5/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/root/.local/share/virtualenvs/RTB-V2Lvgo6A/lib/python3.5/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/root/.local/share/virtualenvs/RTB-V2Lvgo6A/lib/python3.5/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/root/.local/share/virtualenvs/RTB-V2Lvgo6A/lib/python3.5/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/root/.local/share/virtualenvs/RTB-V2Lvgo6A/lib/python3.5/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/root/.local/share/virtualenvs/RTB-V2Lvgo6A/lib/python3.5/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.local/share/virtualenvs/RTB-V2Lvgo6A/lib/python3.5/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/root/.local/share/virtualenvs/RTB-V2Lvgo6A/lib/python3.5/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/root/.local/share/virtualenvs/RTB-V2Lvgo6A/lib/python3.5/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/root/.local/share/virtualenvs/RTB-V2Lvgo6A/lib/python3.5/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/root/.local/share/virtualenvs/RTB-V2Lvgo6A/lib/python3.5/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/root/.local/share/virtualenvs/RTB-V2Lvgo6A/lib/python3.5/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/root/.local/share/virtualenvs/RTB-V2Lvgo6A/lib/python3.5/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/root/.local/share/virtualenvs/RTB-V2Lvgo6A/lib/python3.5/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-0c43f9e037ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mensemblers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbc_xgbc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mee\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbc_dt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbc_rf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mensembler_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogistic_regression\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensemblers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-cfb2a89d04cb>\u001b[0m in \u001b[0;36mensembler_test\u001b[0;34m(classifier_fn, ensemblers)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensemblers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"fitting sample\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mX_res\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_us\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_us\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_res\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_res\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/RTB-V2Lvgo6A/lib/python3.5/site-packages/imblearn/base.py\u001b[0m in \u001b[0;36mfit_sample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \"\"\"\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/RTB-V2Lvgo6A/lib/python3.5/site-packages/imblearn/base.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/RTB-V2Lvgo6A/lib/python3.5/site-packages/imblearn/ensemble/balance_cascade.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    293\u001b[0m             \u001b[0mX_subset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msafe_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubset_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0my_subset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msafe_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubset_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_subset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m             \u001b[0;31m# extract the prediction about the targeted classes only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0mpred_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mindex_under_sample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/RTB-V2Lvgo6A/lib/python3.5/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_predict\u001b[0;34m(estimator, X, y, groups, cv, n_jobs, verbose, fit_params, pre_dispatch, method)\u001b[0m\n\u001b[1;32m    678\u001b[0m     prediction_blocks = parallel(delayed(_fit_and_predict)(\n\u001b[1;32m    679\u001b[0m         clone(estimator), X, y, train, test, verbose, fit_params, method)\n\u001b[0;32m--> 680\u001b[0;31m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m     \u001b[0;31m# Concatenate the predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/RTB-V2Lvgo6A/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/RTB-V2Lvgo6A/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/RTB-V2Lvgo6A/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/RTB-V2Lvgo6A/lib/python3.5/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/RTB-V2Lvgo6A/lib/python3.5/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/RTB-V2Lvgo6A/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/RTB-V2Lvgo6A/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/RTB-V2Lvgo6A/lib/python3.5/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_predict\u001b[0;34m(estimator, X, y, train, test, verbose, fit_params, method)\u001b[0m\n\u001b[1;32m    751\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 753\u001b[0;31m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    754\u001b[0m     \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/RTB-V2Lvgo6A/lib/python3.5/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model)\u001b[0m\n\u001b[1;32m    504\u001b[0m                               \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 506\u001b[0;31m                               verbose_eval=verbose, xgb_model=None)\n\u001b[0m\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb_options\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"objective\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/RTB-V2Lvgo6A/lib/python3.5/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates)\u001b[0m\n\u001b[1;32m    202\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/RTB-V2Lvgo6A/lib/python3.5/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/RTB-V2Lvgo6A/lib/python3.5/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m    896\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),\n\u001b[0;32m--> 898\u001b[0;31m                                                     dtrain.handle))\n\u001b[0m\u001b[1;32m    899\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ee = EasyEnsemble(random_state=KFOLD_SEED)\n",
    "bc = BalanceCascade(random_state=KFOLD_SEED)\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state=KFOLD_SEED)\n",
    "bc_dt = BalanceCascade(estimator=dt, random_state=KFOLD_SEED)\n",
    "\n",
    "rf = RandomForestClassifier(n_jobs=-1, random_state=KFOLD_SEED, verbose=1)\n",
    "bc_rf = BalanceCascade(estimator=rf, random_state=KFOLD_SEED)\n",
    "\n",
    "xgbc = XGBClassifier(n_jobs=-1, scale_pos_weight=500)\n",
    "bc_xgbc = BalanceCascade(estimator=xgbc, random_state=KFOLD_SEED)\n",
    "\n",
    "\n",
    "ensemblers = [bc_xgbc, ee, bc, bc_dt, bc_rf]\n",
    "ensembler_test(logistic_regression, ensemblers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to tune EasyEnsemble by adjusting subsets. It does not affect f1-score or ROC AUC socre\n",
    "```\n",
    "ee = EasyEnsemble(n_subsets = 100, random_state=KFOLD_SEED)\n",
    "ensembler_test(logistic_regression, [ee])\n",
    "\n",
    "ee = EasyEnsemble(n_subsets = 4, random_state=KFOLD_SEED)\n",
    "ensembler_test(logistic_regression, [ee])\n",
    "```\n",
    "Both result in:\n",
    "```\n",
    "================================\n",
    "Confusion Matrix:\n",
    "True Negative = 127470\n",
    "False Negative = 132\n",
    "True Positive = 245\n",
    "False Positive = 72153\n",
    "================================\n",
    "f1 score = 0.007\n",
    "================================\n",
    "ROC AUC Score = 0.644\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tune max subset for BalanceCascade with DecisionTreeClassifier\n",
    "\n",
    "10 is the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting sample\n",
      "(17, 3062, 88) (17, 3062)\n",
      "training\n",
      "[LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear]Ensembler 0\n",
      "--------test---------\n",
      "================================\n",
      "Confusion Matrix:\n",
      "True Negative = 134778\n",
      "False Negative = 150\n",
      "True Positive = 227\n",
      "False Positive = 64845\n",
      "================================\n",
      "f1 score = 0.007\n",
      "================================\n",
      "ROC AUC Score = 0.639\n",
      "fitting sample\n",
      "(10, 3062, 88) (10, 3062)\n",
      "training\n",
      "[LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear]Ensembler 1\n",
      "--------test---------\n",
      "================================\n",
      "Confusion Matrix:\n",
      "True Negative = 131774\n",
      "False Negative = 133\n",
      "True Positive = 244\n",
      "False Positive = 67849\n",
      "================================\n",
      "f1 score = 0.007\n",
      "================================\n",
      "ROC AUC Score = 0.654\n",
      "fitting sample\n",
      "(5, 3062, 88) (5, 3062)\n",
      "training\n",
      "[LibLinear][LibLinear][LibLinear][LibLinear][LibLinear]Ensembler 2\n",
      "--------test---------\n",
      "================================\n",
      "Confusion Matrix:\n",
      "True Negative = 128482\n",
      "False Negative = 138\n",
      "True Positive = 239\n",
      "False Positive = 71141\n",
      "================================\n",
      "f1 score = 0.007\n",
      "================================\n",
      "ROC AUC Score = 0.639\n"
     ]
    }
   ],
   "source": [
    "dt_20 = DecisionTreeClassifier(random_state=KFOLD_SEED)\n",
    "bc_dt_20 = BalanceCascade(estimator=dt_20, n_max_subset=20, random_state=KFOLD_SEED)\n",
    "\n",
    "dt_10 = DecisionTreeClassifier(random_state=KFOLD_SEED)\n",
    "bc_dt_10 = BalanceCascade(estimator=dt_10, n_max_subset=10, random_state=KFOLD_SEED)\n",
    "\n",
    "dt_5 = DecisionTreeClassifier(random_state=KFOLD_SEED)\n",
    "bc_dt_5 = BalanceCascade(estimator=dt_5, n_max_subset=5, random_state=KFOLD_SEED)\n",
    "\n",
    "ensemblers = [bc_dt_20, bc_dt_10, bc_dt_5]\n",
    "ensembler_test(logistic_regression, ensemblers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tune max features, overall it doesn't affect ROC AUC much, but 17 features is slightly higher than others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting sample\n",
      "(10, 3062, 88) (10, 3062)\n",
      "training\n",
      "[LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear]Ensembler 0\n",
      "--------test---------\n",
      "================================\n",
      "Confusion Matrix:\n",
      "True Negative = 131814\n",
      "False Negative = 132\n",
      "True Positive = 245\n",
      "False Positive = 67809\n",
      "================================\n",
      "f1 score = 0.007\n",
      "================================\n",
      "ROC AUC Score = 0.655\n",
      "fitting sample\n",
      "(10, 3062, 88) (10, 3062)\n",
      "training\n",
      "[LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear]Ensembler 1\n",
      "--------test---------\n",
      "================================\n",
      "Confusion Matrix:\n",
      "True Negative = 133049\n",
      "False Negative = 142\n",
      "True Positive = 235\n",
      "False Positive = 66574\n",
      "================================\n",
      "f1 score = 0.007\n",
      "================================\n",
      "ROC AUC Score = 0.645\n",
      "fitting sample\n",
      "(10, 3062, 88) (10, 3062)\n",
      "training\n",
      "[LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear]Ensembler 2\n",
      "--------test---------\n",
      "================================\n",
      "Confusion Matrix:\n",
      "True Negative = 131934\n",
      "False Negative = 126\n",
      "True Positive = 251\n",
      "False Positive = 67689\n",
      "================================\n",
      "f1 score = 0.007\n",
      "================================\n",
      "ROC AUC Score = 0.663\n",
      "fitting sample\n",
      "(10, 3062, 88) (10, 3062)\n",
      "training\n",
      "[LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear]Ensembler 3\n",
      "--------test---------\n",
      "================================\n",
      "Confusion Matrix:\n",
      "True Negative = 132297\n",
      "False Negative = 138\n",
      "True Positive = 239\n",
      "False Positive = 67326\n",
      "================================\n",
      "f1 score = 0.007\n",
      "================================\n",
      "ROC AUC Score = 0.648\n"
     ]
    }
   ],
   "source": [
    "# 70 features\n",
    "dt_08 = DecisionTreeClassifier(max_features=0.8, random_state=KFOLD_SEED)\n",
    "bc_dt_08 = BalanceCascade(estimator=dt_08, n_max_subset=10, random_state=KFOLD_SEED)\n",
    "\n",
    "# 35 features\n",
    "dt_04 = DecisionTreeClassifier(max_features=0.4, random_state=KFOLD_SEED)\n",
    "bc_dt_04 = BalanceCascade(estimator=dt_04, n_max_subset=10, random_state=KFOLD_SEED)\n",
    "\n",
    "# 17 features\n",
    "dt_02 = DecisionTreeClassifier(max_features=0.2, random_state=KFOLD_SEED)\n",
    "bc_dt_02 = BalanceCascade(estimator=dt_02, n_max_subset=10, random_state=KFOLD_SEED)\n",
    "\n",
    "# Auto is sqrt(n_features) ~= 9\n",
    "dt_auto = DecisionTreeClassifier(max_features='auto', random_state=KFOLD_SEED)\n",
    "bc_dt_auto = BalanceCascade(estimator=dt_auto, n_max_subset=10, random_state=KFOLD_SEED)\n",
    "\n",
    "ensemblers = [bc_dt_08, bc_dt_04, bc_dt_02, bc_dt_auto]\n",
    "ensembler_test(logistic_regression, ensemblers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No need to tune decision tree's class weight, since ensembler already ensures both classes have equal samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Min samples at leaves do not seem to matter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting sample\n",
      "(10, 3062, 88) (10, 3062)\n",
      "training\n",
      "[LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear]Ensembler 0\n",
      "--------test---------\n",
      "================================\n",
      "Confusion Matrix:\n",
      "True Negative = 133422\n",
      "False Negative = 154\n",
      "True Positive = 223\n",
      "False Positive = 66201\n",
      "================================\n",
      "f1 score = 0.007\n",
      "================================\n",
      "ROC AUC Score = 0.630\n",
      "fitting sample\n",
      "(10, 3062, 88) (10, 3062)\n",
      "training\n",
      "[LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear]Ensembler 1\n",
      "--------test---------\n",
      "================================\n",
      "Confusion Matrix:\n",
      "True Negative = 130890\n",
      "False Negative = 152\n",
      "True Positive = 225\n",
      "False Positive = 68733\n",
      "================================\n",
      "f1 score = 0.006\n",
      "================================\n",
      "ROC AUC Score = 0.626\n",
      "fitting sample\n",
      "(10, 3062, 88) (10, 3062)\n",
      "training\n",
      "[LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear]Ensembler 2\n",
      "--------test---------\n",
      "================================\n",
      "Confusion Matrix:\n",
      "True Negative = 131893\n",
      "False Negative = 133\n",
      "True Positive = 244\n",
      "False Positive = 67730\n",
      "================================\n",
      "f1 score = 0.007\n",
      "================================\n",
      "ROC AUC Score = 0.654\n",
      "fitting sample\n",
      "(10, 3062, 88) (10, 3062)\n",
      "training\n",
      "[LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear]Ensembler 3\n",
      "--------test---------\n",
      "================================\n",
      "Confusion Matrix:\n",
      "True Negative = 131859\n",
      "False Negative = 154\n",
      "True Positive = 223\n",
      "False Positive = 67764\n",
      "================================\n",
      "f1 score = 0.007\n",
      "================================\n",
      "ROC AUC Score = 0.626\n"
     ]
    }
   ],
   "source": [
    "dt_min_samples_50 = DecisionTreeClassifier(min_samples_leaf=50, max_features=0.2, random_state=KFOLD_SEED)\n",
    "bc_dt_min_samples_50 = BalanceCascade(estimator=dt_min_samples_50, n_max_subset=10, random_state=KFOLD_SEED)\n",
    "\n",
    "dt_min_samples_20 = DecisionTreeClassifier(min_samples_leaf=20, max_features=0.2, random_state=KFOLD_SEED)\n",
    "bc_dt_min_samples_20 = BalanceCascade(estimator=dt_min_samples_20, n_max_subset=10, random_state=KFOLD_SEED)\n",
    "\n",
    "dt_min_samples_10 = DecisionTreeClassifier(min_samples_leaf=10, max_features=0.2, random_state=KFOLD_SEED)\n",
    "bc_dt_min_samples_10 = BalanceCascade(estimator=dt_min_samples_10, n_max_subset=10, random_state=KFOLD_SEED)\n",
    "\n",
    "dt_min_samples_5 = DecisionTreeClassifier(min_samples_leaf=5, max_features=0.2, random_state=KFOLD_SEED)\n",
    "bc_dt_min_samples_5 = BalanceCascade(estimator=dt_min_samples_5, n_max_subset=10, random_state=KFOLD_SEED)\n",
    "\n",
    "ensemblers = [bc_dt_min_samples_50, bc_dt_min_samples_20, bc_dt_min_samples_10, bc_dt_min_samples_5]\n",
    "ensembler_test(logistic_regression, ensemblers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class weight has to be balanced!\n",
    "If negative class is heavier, both TPR and FNR decrease, but TPR decrease causes more harm to ROC AUC.\n",
    "If positive class is heavier, the observation is the opposite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting sample\n",
      "(10, 3062, 88) (10, 3062)\n",
      "training\n",
      "Ensembler 0\n",
      "--------test---------\n",
      "================================\n",
      "Confusion Matrix:\n",
      "True Negative = 187970\n",
      "False Negative = 288\n",
      "True Positive = 89\n",
      "False Positive = 11653\n",
      "================================\n",
      "f1 score = 0.015\n",
      "================================\n",
      "ROC AUC Score = 0.589\n",
      "fitting sample\n",
      "(10, 3062, 88) (10, 3062)\n",
      "training\n",
      "Ensembler 0\n",
      "--------test---------\n",
      "================================\n",
      "Confusion Matrix:\n",
      "True Negative = 131934\n",
      "False Negative = 126\n",
      "True Positive = 251\n",
      "False Positive = 67689\n",
      "================================\n",
      "f1 score = 0.007\n",
      "================================\n",
      "ROC AUC Score = 0.663\n",
      "fitting sample\n",
      "(10, 3062, 88) (10, 3062)\n",
      "training\n",
      "Ensembler 0\n",
      "--------test---------\n",
      "================================\n",
      "Confusion Matrix:\n",
      "True Negative = 38409\n",
      "False Negative = 25\n",
      "True Positive = 352\n",
      "False Positive = 161214\n",
      "================================\n",
      "f1 score = 0.004\n",
      "================================\n",
      "ROC AUC Score = 0.563\n",
      "fitting sample\n",
      "(10, 3062, 88) (10, 3062)\n",
      "training\n",
      "Ensembler 0\n",
      "--------test---------\n",
      "================================\n",
      "Confusion Matrix:\n",
      "True Negative = 13244\n",
      "False Negative = 6\n",
      "True Positive = 371\n",
      "False Positive = 186379\n",
      "================================\n",
      "f1 score = 0.004\n",
      "================================\n",
      "ROC AUC Score = 0.525\n"
     ]
    }
   ],
   "source": [
    "def create_lr_proxy(class_weight='balanced'):\n",
    "    def create_lr():\n",
    "        return LogisticRegression(penalty='l2', class_weight=class_weight)\n",
    "    return create_lr\n",
    "\n",
    "dt = DecisionTreeClassifier(max_features=0.2, random_state=KFOLD_SEED)\n",
    "bc = BalanceCascade(estimator=dt, n_max_subset=10, random_state=KFOLD_SEED)\n",
    "\n",
    "ensembler_test(create_lr_proxy({0: 2, 1: 1}), [bc])\n",
    "ensembler_test(create_lr_proxy(), [bc])\n",
    "ensembler_test(create_lr_proxy({0: 1, 1: 2}), [bc])\n",
    "ensembler_test(create_lr_proxy({0: 1, 1: 4}), [bc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try L1 regularization, C=1.0 is just right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting sample\n",
      "(10, 3062, 88) (10, 3062)\n",
      "training\n",
      "Ensembler 0\n",
      "--------test---------\n",
      "================================\n",
      "Confusion Matrix:\n",
      "True Negative = 131967\n",
      "False Negative = 125\n",
      "True Positive = 252\n",
      "False Positive = 67656\n",
      "================================\n",
      "f1 score = 0.007\n",
      "================================\n",
      "ROC AUC Score = 0.665\n",
      "fitting sample\n",
      "(10, 3062, 88) (10, 3062)\n",
      "training\n",
      "Ensembler 0\n",
      "--------test---------\n",
      "================================\n",
      "Confusion Matrix:\n",
      "True Negative = 131859\n",
      "False Negative = 124\n",
      "True Positive = 253\n",
      "False Positive = 67764\n",
      "================================\n",
      "f1 score = 0.007\n",
      "================================\n",
      "ROC AUC Score = 0.666\n",
      "fitting sample\n",
      "(10, 3062, 88) (10, 3062)\n",
      "training\n",
      "Ensembler 0\n",
      "--------test---------\n",
      "================================\n",
      "Confusion Matrix:\n",
      "True Negative = 131884\n",
      "False Negative = 124\n",
      "True Positive = 253\n",
      "False Positive = 67739\n",
      "================================\n",
      "f1 score = 0.007\n",
      "================================\n",
      "ROC AUC Score = 0.666\n",
      "fitting sample\n",
      "(10, 3062, 88) (10, 3062)\n",
      "training\n",
      "Ensembler 0\n",
      "--------test---------\n",
      "================================\n",
      "Confusion Matrix:\n",
      "True Negative = 131745\n",
      "False Negative = 126\n",
      "True Positive = 251\n",
      "False Positive = 67878\n",
      "================================\n",
      "f1 score = 0.007\n",
      "================================\n",
      "ROC AUC Score = 0.663\n",
      "fitting sample\n",
      "(10, 3062, 88) (10, 3062)\n",
      "training\n",
      "Ensembler 0\n",
      "--------test---------\n",
      "================================\n",
      "Confusion Matrix:\n",
      "True Negative = 131609\n",
      "False Negative = 127\n",
      "True Positive = 250\n",
      "False Positive = 68014\n",
      "================================\n",
      "f1 score = 0.007\n",
      "================================\n",
      "ROC AUC Score = 0.661\n"
     ]
    }
   ],
   "source": [
    "def create_lr_proxy(C=1.0):\n",
    "    def create_lr():\n",
    "        return LogisticRegression(penalty='l1', C=C, random_state=KFOLD_SEED)\n",
    "    return create_lr\n",
    "\n",
    "dt = DecisionTreeClassifier(max_features=0.2, random_state=KFOLD_SEED)\n",
    "bc = BalanceCascade(estimator=dt, n_max_subset=10, random_state=KFOLD_SEED)\n",
    "\n",
    "ensembler_test(create_lr_proxy(2.0), [bc])\n",
    "ensembler_test(create_lr_proxy(1.0), [bc])\n",
    "ensembler_test(create_lr_proxy(0.8), [bc])\n",
    "ensembler_test(create_lr_proxy(0.5), [bc])\n",
    "ensembler_test(create_lr_proxy(0.2), [bc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
